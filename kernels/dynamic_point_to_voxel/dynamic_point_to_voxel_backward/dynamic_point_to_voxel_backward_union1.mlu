/*************************************************************************
 * Copyright (C) [2023] by Cambricon, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#include "dynamic_point_to_voxel_backward.h"

#include "core/logging.h"
#include "kernels/debug.h"
#include "kernels/kernel.h"
#include "kernels/utils/common.h"

__nram__ int8_t nram_buffer[MAX_NRAM_SIZE];

template <typename T>
__mlu_func__ void loadAsync(T *feats_nram, T *voxel_feats_nram,
                            int *feats_index_nram, int *voxel_from_nram,
                            int *map_curr_ipu, const int *map_global,
                            const int *dim_c_idx, const T *feats,
                            const T *voxel_feats, const int *voxel_from,
                            int &n_global, int &n_curr_ipu,
                            const int n_deal_num, const int N, const int C) {
  const int invalid_index = -1;
  const int size_feats = C * sizeof(T);
  const int size_feats_idx = C * sizeof(int);
  n_curr_ipu = 0;
  for (; n_global < N && n_curr_ipu < n_deal_num; ++n_global) {
    // calculate offset
    int gdram_voxel_feat_offset;
    const int gdram_feat_offset = n_global * C;
    const int nram_offset = n_curr_ipu * C;

    const int point_to = map_global[n_global];
    if (taskId == point_to % taskDim) {
      if (point_to == invalid_index) {
        continue;
      }
      gdram_voxel_feat_offset = point_to * C;
      // load feats
      // feats_nram = [feats[0],feats[1],...,feats[n_curr_ipu-1]]
      __memcpy_async(feats_nram + nram_offset, feats + gdram_feat_offset,
                     size_feats, GDRAM2NRAM);
      // load voxel_feats
      // voxel_feats_nram = [voxel_feats[0],voxel_feats[0],voxel_feats[1],...]
      //   when map = [0,0,1...]
      __memcpy_async(voxel_feats_nram + nram_offset,
                     voxel_feats + gdram_voxel_feat_offset, size_feats,
                     GDRAM2NRAM);

      // load voxel2point
      __memcpy_async(voxel_from_nram + nram_offset,
                     voxel_from + gdram_voxel_feat_offset, size_feats_idx,
                     GDRAM2NRAM);

      // set feat-points index
      __bang_write_value(feats_index_nram + nram_offset, C, n_global * C);

      // point2voxel_map removed invalid data
      map_curr_ipu[n_curr_ipu] = point_to;
      ++n_curr_ipu;
    }
  }
  if (n_curr_ipu > 0) {
    // update feat-points index
    __bang_cycle_add(feats_index_nram, feats_index_nram, dim_c_idx,
                     n_curr_ipu * C, C);
  }
}

template <typename T>
__mlu_func__ void compute(T *feats_nram, T *voxel_feats_nram,
                          int *feats_index_nram, int *voxel_from_nram,
                          const int n_curr_ipu, const int N, const int C) {
  if (n_curr_ipu > 0) {
    // feats[i] == voxel_feats[i] ? mask[i] = 1 : mask[i] = 0
    const int deal_num = n_curr_ipu * C;
    __bang_eq(feats_nram, voxel_feats_nram, feats_nram, deal_num);
    __bang_float2int32_tz((int *)feats_nram, feats_nram, deal_num, 0);

    // recover feats_index (local->global)
    // recover !mask to N*C
    __bang_not((int *)voxel_feats_nram, (int *)feats_nram, deal_num);
    __bang_mul((int *)feats_nram, (int *)feats_nram, feats_index_nram,
               deal_num);
    __bang_mul_scalar((int *)voxel_feats_nram, (int *)voxel_feats_nram, N * C,
                      deal_num);

    // mix mask and !mask, and choose the min index
    __bang_add(feats_index_nram, (int *)voxel_feats_nram, (int *)feats_nram,
               deal_num);
    __bang_minequal(voxel_from_nram, voxel_from_nram, feats_index_nram,
                    deal_num);
  }
}

__mlu_func__ void storeAsync(int *voxel_from, const int *voxel_from_nram,
                             const int *map_curr_ipu, bool *voxel_count_flag,
                             int *feats_index_nram, const int n_curr_ipu,
                             const int N, const int C) {
  for (int i = 0; i < n_curr_ipu; i++) {
#if __BANG_ARCH__ >= 592
    // better performance for mlu590
    __bang_atomic_reduce_min(voxel_from + map_curr_ipu[i] * C,
                             voxel_from_nram + i * C, C);
#else
    const int offset_local = map_curr_ipu[i];
    if (taskId == offset_local % taskDim) {
      if (!voxel_count_flag[offset_local]) {
        __memcpy(voxel_from + offset_local * C, voxel_from_nram + i * C,
                 C * sizeof(int), NRAM2GDRAM);
        voxel_count_flag[offset_local] = true;
      } else {
        __memcpy(feats_index_nram, voxel_from + offset_local * C,
                 C * sizeof(int), GDRAM2NRAM);
        __bang_minequal(feats_index_nram, feats_index_nram,
                        voxel_from_nram + i * C, C);
        __memcpy(voxel_from + offset_local * C, feats_index_nram,
                 C * sizeof(int), NRAM2GDRAM);
      }
    }
#endif
  }
}

template <typename T>
__mlu_global__ void MLUKernelMaxReduceTracebackScatterIdx(
    const T *feats, const T *voxel_feats, int *voxel_from,
    const int *point2voxel_map, const int *voxel_num, const int N,
    const int C) {
  const int M = *voxel_num;
  if (__is_mpu() || M == 0) {
    return;
  }

  /*
   * NRAM partition
   *  |==================|============================|
   *  | Semantics        | Size                       |
   *  |==================|============================|
   *  | feats            |    [n_deal_num, C], float  |
   *  | voxel_feats      |    [n_deal_num, C], float  |
   *  | index_mask       |    [n_deal_num, C], int    |
   *  | voxel_from       |    [n_deal_num, C], int    |
   *  | map_curr_ipu     |    [n_deal_num], int       |
   *  | map_global       |    [N], int                |
   *  | dim_c_idx        |    [C], int                |
   *  | voxel_count_flag |    [M], bool               |
   *  |==================|============================|
   */
  const int n_deal_num =
      (MAX_NRAM_SIZE - N * sizeof(int) - M - C * sizeof(int)) /
      (2 * C * sizeof(T) + 2 * C * sizeof(int) + sizeof(int));
  const int feats_num = n_deal_num * C;

  T *feats_nram = (T *)nram_buffer;
  T *voxel_feats_nram = feats_nram + feats_num;
  int *feats_index_nram = (int *)(voxel_feats_nram + feats_num);
  int *voxel_from_nram = feats_index_nram + feats_num;
  int *map_global = voxel_from_nram + feats_num;
  int *map_curr_ipu = map_global + N;
  int *dim_c_idx = map_curr_ipu + n_deal_num;
  bool *voxel_count_flag = (bool *)(dim_c_idx + C);

  // load point2voxel_map & init voxel_count_flag
  __memcpy(map_global, point2voxel_map, N * sizeof(int), GDRAM2NRAM);
  __memset_nram(voxel_count_flag, M, (int8_t) false);

  // init dim_c_idx: 0,1,2,...,C-1
  for (int i = 0; i < C; i++) {
    dim_c_idx[i] = i;
  }

  for (int n_global = 0, n_curr_ipu = 0; n_global < N;) {
    loadAsync(feats_nram, voxel_feats_nram, feats_index_nram, voxel_from_nram,
              map_curr_ipu, map_global, dim_c_idx, feats, voxel_feats,
              voxel_from, n_global, n_curr_ipu, n_deal_num, N, C);
    __sync();
    compute(feats_nram, voxel_feats_nram, feats_index_nram, voxel_from_nram,
            n_curr_ipu, N, C);
    __sync();
    storeAsync(voxel_from, voxel_from_nram, map_curr_ipu, voxel_count_flag,
               feats_index_nram, n_curr_ipu, N, C);
    __sync();
  }
}

mluOpStatus_t MLUOP_WIN_API KernelDynamicPointToVoxelBackward(
    cnrtDim3_t k_dim, cnrtFunctionType_t k_type, cnrtQueue_t queue,
    const void *feats, const void *voxel_feats, void *voxel_from,
    const void *point2voxel_map, const void *voxel_num, const int N,
    const int C) {
  KERNEL_CHECK(MLUKernelMaxReduceTracebackScatterIdx<<<k_dim, k_type, queue>>>(
      (const float *)feats, (const float *)voxel_feats, (int *)voxel_from,
      (const int *)point2voxel_map, (const int *)voxel_num, N, C));
  return MLUOP_STATUS_SUCCESS;
}
