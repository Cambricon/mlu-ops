/*************************************************************************
 * Copyright (C) [2022] by Cambricon, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#include "voxelization.h"

#include <cmath>
#include <climits>

#include "core/logging.h"
#include "kernels/kernel.h"
#include "kernels/utils/common.h"

__nram__ int8_t nram_buffer[MAX_NRAM_SIZE];

template <typename T>
__mlu_func__ bool containNanInf(const T pos1, const T pos2) {
  if (std::isnan(pos1) || std::isnan(pos2) || std::isinf(pos1) ||
      std::isinf(pos2)) {
    return true;
  }
  return false;
}

__mlu_func__ void computeDynamicVoxelize(
    int8_t *points_x, int8_t *points_y, int8_t *points_z, int8_t *auxiliary_a,
    int8_t *auxiliary_b, int8_t *auxiliary_c, const float coors_x_min,
    const float coors_y_min, const float coors_z_min, const float voxel_x,
    const float voxel_y, const float voxel_z, const int32_t grid_x,
    const int32_t grid_y, const int32_t grid_z, const int32_t deal_num) {
  // x - coors_x_min
  __bang_sub_scalar((float *)points_x, (float *)points_x, coors_x_min,
                    deal_num);
  // y - coors_y_min
  __bang_sub_scalar((float *)points_y, (float *)points_y, coors_y_min,
                    deal_num);
  // z - coors_z_min
  __bang_sub_scalar((float *)points_z, (float *)points_z, coors_z_min,
                    deal_num);
  // (x - coors_x_min) / voxel_x
  __bang_mul_scalar((float *)points_x, (float *)points_x, 1.0 / voxel_x,
                    deal_num);
  // (y - coors_y_min) / voxel_y
  __bang_mul_scalar((float *)points_y, (float *)points_y, 1.0 / voxel_y,
                    deal_num);
  // (z - coors_z_min) / voxel_z
  __bang_mul_scalar((float *)points_z, (float *)points_z, 1.0 / voxel_z,
                    deal_num);
  // c_x = floor((x - coors_x_min) / voxel_x)
  __bang_floor((float *)auxiliary_a, (float *)points_x, deal_num);
  __bang_float2int32((int32_t *)points_x, (float *)auxiliary_a, deal_num, 0);
  // c_y = floor((y - coors_y_min) / voxel_y)
  __bang_floor((float *)auxiliary_a, (float *)points_y, deal_num);
  __bang_float2int32((int32_t *)points_y, (float *)auxiliary_a, deal_num, 0);
  // c_z = floor((z - coors_z_min) / voxel_z)
  __bang_floor((float *)auxiliary_a, (float *)points_z, deal_num);
  __bang_float2int32((int32_t *)points_z, (float *)auxiliary_a, deal_num, 0);
  // c_x >= 0
  __bang_ge_scalar((int32_t *)auxiliary_b, (int32_t *)points_x, (int32_t)0,
                   deal_num);
  // c_x < grid_x
  __bang_lt_scalar((int32_t *)auxiliary_c, (int32_t *)points_x, grid_x,
                   deal_num);
  // 0 <= c_x < grid_x
  __bang_mul((int32_t *)auxiliary_a, (int32_t *)auxiliary_b,
             (int32_t *)auxiliary_c, deal_num);
  // c_y >= 0
  __bang_ge_scalar((int32_t *)auxiliary_b, (int32_t *)points_y, (int32_t)0,
                   deal_num);
  // c_y < grid_y
  __bang_lt_scalar((int32_t *)auxiliary_c, (int32_t *)points_y, grid_y,
                   deal_num);
  // 0 <= c_y < grid_y
  __bang_mul((int32_t *)auxiliary_b, (int32_t *)auxiliary_b,
             (int32_t *)auxiliary_c, deal_num);
  // c_x >= 0 && c_x < grid_x && c_y >= 0 && c_y < grid_y
  __bang_mul((int32_t *)auxiliary_a, (int32_t *)auxiliary_a,
             (int32_t *)auxiliary_b, deal_num);
  // c_z >= 0
  __bang_ge_scalar((int32_t *)auxiliary_b, (int32_t *)points_z, (int32_t)0,
                   deal_num);
  // c_z < grid_z
  __bang_lt_scalar((int32_t *)auxiliary_c, (int32_t *)points_z, grid_z,
                   deal_num);
  // 0 <= c_z < grid_z
  __bang_mul((int32_t *)auxiliary_b, (int32_t *)auxiliary_b,
             (int32_t *)auxiliary_c, deal_num);
  // 0 <= c_x < grid_x && 0 <= c_y < grid_y && 0 <= c_z < grid_z
  __bang_mul((int32_t *)auxiliary_a, (int32_t *)auxiliary_a,
             (int32_t *)auxiliary_b, deal_num);
  __bang_not((int32_t *)auxiliary_c, (int32_t *)auxiliary_a, deal_num);

  __bang_mul((int32_t *)points_x, (int32_t *)points_x, (int32_t *)auxiliary_a,
             deal_num);
  __bang_mul_scalar((int32_t *)auxiliary_b, (int32_t *)auxiliary_c,
                    (int32_t)(-1), deal_num);
  __bang_add((int32_t *)points_x, (int32_t *)points_x, (int32_t *)auxiliary_b,
             deal_num);
  __bang_mul((int32_t *)points_y, (int32_t *)points_y, (int32_t *)auxiliary_a,
             deal_num);
  __bang_add((int32_t *)points_y, (int32_t *)points_y, (int32_t *)auxiliary_b,
             deal_num);
  __bang_mul((int32_t *)points_z, (int32_t *)points_z, (int32_t *)auxiliary_a,
             deal_num);
  __bang_add((int32_t *)points_z, (int32_t *)points_z, (int32_t *)auxiliary_b,
             deal_num);
}

__mlu_func__ void computePoint2Voxel(int8_t *coors_x, int8_t *coors_y,
                                     int8_t *coors_z, const int32_t c_x,
                                     const int32_t c_y, const int32_t c_z,
                                     const int32_t max_points, int32_t *num,
                                     int32_t *first_point,
                                     const int32_t deal_idx,
                                     const int32_t deal_num) {
  // Time complexity: o(n^2)
  // Find points with the same coordinates as the current point.
  // duplicate points indexs set as the index of points
  // that appear for the first time
  __bang_eq_scalar((int32_t *)coors_x, (int32_t *)coors_x, c_x, deal_num);
  __bang_eq_scalar((int32_t *)coors_y, (int32_t *)coors_y, c_y, deal_num);
  __bang_eq_scalar((int32_t *)coors_z, (int32_t *)coors_z, c_z, deal_num);
  __bang_mul((int32_t *)coors_x, (int32_t *)coors_x, (int32_t *)coors_y,
             deal_num);
  __bang_mul((int32_t *)coors_x, (int32_t *)coors_x, (int32_t *)coors_z,
             deal_num);
  if (*num == 0) {
    *num = (int32_t)__bang_count((float *)coors_x, deal_num);
    if (*num > 0) {
      *first_point =
          (int32_t)__bang_findfirst1((float *)coors_x, deal_num) + deal_idx;
    }
  } else {
    *num += (int32_t)__bang_count((float *)coors_x, deal_num);
  }
}

__mlu_global__ void mluDynamicVoxelize(const float *points,
                                       const float *voxel_size_gdram,
                                       const float *coors_range_gdram,
                                       int32_t *coors, const int32_t num_points,
                                       const int32_t num_features) {
  if (__is_mpu()) {
    return;
  }

  const int32_t points_rem = num_points % taskDim;
  const int32_t points_per_core =
      taskId < points_rem ? num_points / taskDim + 1 : num_points / taskDim;
  const int32_t points_start = taskId < points_rem
                                   ? taskId * points_per_core
                                   : taskId * points_per_core + points_rem;

  float *voxel_size = (float *)nram_buffer;
  float *coors_range = (float *)(nram_buffer + 3 * sizeof(float));
  __memcpy_async(voxel_size, voxel_size_gdram, 3 * sizeof(float), GDRAM2NRAM);
  __memcpy_async(coors_range, coors_range_gdram, 6 * sizeof(float), GDRAM2NRAM);
  __sync();

  const float voxel_x = voxel_size[0];
  const float voxel_y = voxel_size[1];
  const float voxel_z = voxel_size[2];
  const float coors_x_min = coors_range[0];
  const float coors_y_min = coors_range[1];
  const float coors_z_min = coors_range[2];
  const float coors_x_max = coors_range[3];
  const float coors_y_max = coors_range[4];
  const float coors_z_max = coors_range[5];

  bool contain_nan_inf = false;
  contain_nan_inf = containNanInf(coors_x_max, coors_x_min);
  const int32_t grid_x =
      contain_nan_inf ? INT_MIN : round((coors_x_max - coors_x_min) / voxel_x);
  contain_nan_inf = containNanInf(coors_y_max, coors_y_min);
  const int32_t grid_y =
      contain_nan_inf ? INT_MIN : round((coors_y_max - coors_y_min) / voxel_y);
  contain_nan_inf = containNanInf(coors_z_max, coors_z_min);
  const int32_t grid_z =
      contain_nan_inf ? INT_MIN : round((coors_z_max - coors_z_min) / voxel_z);

  const int32_t split_num = 9;
  const int32_t deal_num =
      FLOOR_ALIGN(MAX_NRAM_SIZE / split_num / sizeof(float), NFU_ALIGN_SIZE);
  const int32_t repeat = points_per_core / deal_num;
  const int32_t rem = points_per_core % deal_num;
  const int32_t ping_pong_gap = 3 * deal_num * sizeof(float);

  int8_t *points_x = nram_buffer;
  int8_t *points_y = points_x + deal_num * sizeof(float);
  int8_t *points_z = points_y + deal_num * sizeof(float);
  int8_t *auxiliary_a = points_x + 2 * ping_pong_gap;
  int8_t *auxiliary_b = auxiliary_a + deal_num * sizeof(float);
  int8_t *auxiliary_c = auxiliary_b + deal_num * sizeof(float);

  int32_t *coors_z_start = coors + points_start;
  int32_t *coors_y_start = coors + num_points + points_start;
  int32_t *coors_x_start = coors + num_points * 2 + points_start;

  if (repeat > 0) {
    __memcpy_async(points_x, points + points_start * num_features,
                   sizeof(float), GDRAM2NRAM, sizeof(float),
                   num_features * sizeof(float), deal_num - 1);
    __memcpy_async(points_y, points + points_start * num_features + 1,
                   sizeof(float), GDRAM2NRAM, sizeof(float),
                   num_features * sizeof(float), deal_num - 1);
    __memcpy_async(points_z, points + points_start * num_features + 2,
                   sizeof(float), GDRAM2NRAM, sizeof(float),
                   num_features * sizeof(float), deal_num - 1);
    __sync();
  }
  if (repeat > 1) {
    __memcpy_async(points_x + ping_pong_gap,
                   points + (points_start + deal_num) * num_features,
                   sizeof(float), GDRAM2NRAM, sizeof(float),
                   num_features * sizeof(float), deal_num - 1);
    __memcpy_async(points_y + ping_pong_gap,
                   points + (points_start + deal_num) * num_features + 1,
                   sizeof(float), GDRAM2NRAM, sizeof(float),
                   num_features * sizeof(float), deal_num - 1);
    __memcpy_async(points_z + ping_pong_gap,
                   points + (points_start + deal_num) * num_features + 2,
                   sizeof(float), GDRAM2NRAM, sizeof(float),
                   num_features * sizeof(float), deal_num - 1);
    computeDynamicVoxelize(points_x, points_y, points_z, auxiliary_a,
                           auxiliary_b, auxiliary_c, coors_x_min, coors_y_min,
                           coors_z_min, voxel_x, voxel_y, voxel_z, grid_x,
                           grid_y, grid_z, deal_num);
    __sync();
  }

  for (int32_t i = 0; i < repeat - 2; ++i) {
    __memcpy_async(coors_x_start + i * deal_num,
                   points_x + (i % 2) * ping_pong_gap,
                   deal_num * sizeof(int32_t), NRAM2GDRAM);
    __memcpy_async(coors_y_start + i * deal_num,
                   points_y + (i % 2) * ping_pong_gap,
                   deal_num * sizeof(int32_t), NRAM2GDRAM);
    __memcpy_async(coors_z_start + i * deal_num,
                   points_z + (i % 2) * ping_pong_gap,
                   deal_num * sizeof(int32_t), NRAM2GDRAM);

    __memcpy_async(points_x + (i % 2) * ping_pong_gap,
                   points + (points_start + (i + 2) * deal_num) * num_features,
                   sizeof(float), GDRAM2NRAM, sizeof(float),
                   num_features * sizeof(float), deal_num - 1);
    __memcpy_async(
        points_y + (i % 2) * ping_pong_gap,
        points + (points_start + (i + 2) * deal_num) * num_features + 1,
        sizeof(float), GDRAM2NRAM, sizeof(float), num_features * sizeof(float),
        deal_num - 1);
    __memcpy_async(
        points_z + (i % 2) * ping_pong_gap,
        points + (points_start + (i + 2) * deal_num) * num_features + 2,
        sizeof(float), GDRAM2NRAM, sizeof(float), num_features * sizeof(float),
        deal_num - 1);
    computeDynamicVoxelize(points_x + ((i + 1) % 2) * ping_pong_gap,
                           points_y + ((i + 1) % 2) * ping_pong_gap,
                           points_z + ((i + 1) % 2) * ping_pong_gap,
                           auxiliary_a, auxiliary_b, auxiliary_c, coors_x_min,
                           coors_y_min, coors_z_min, voxel_x, voxel_y, voxel_z,
                           grid_x, grid_y, grid_z, deal_num);
    __sync();
  }

  if (repeat >= 2) {
    __memcpy_async(coors_x_start + (repeat - 2) * deal_num,
                   points_x + (repeat % 2) * ping_pong_gap,
                   deal_num * sizeof(int32_t), NRAM2GDRAM);
    __memcpy_async(coors_y_start + (repeat - 2) * deal_num,
                   points_y + (repeat % 2) * ping_pong_gap,
                   deal_num * sizeof(int32_t), NRAM2GDRAM);
    __memcpy_async(coors_z_start + (repeat - 2) * deal_num,
                   points_z + (repeat % 2) * ping_pong_gap,
                   deal_num * sizeof(int32_t), NRAM2GDRAM);
  }
  if (rem > 0) {
    __memcpy_async(points_x + (repeat % 2) * ping_pong_gap,
                   points + (points_start + repeat * deal_num) * num_features,
                   sizeof(float), GDRAM2NRAM, sizeof(float),
                   num_features * sizeof(float), rem - 1);
    __memcpy_async(
        points_y + (repeat % 2) * ping_pong_gap,
        points + (points_start + repeat * deal_num) * num_features + 1,
        sizeof(float), GDRAM2NRAM, sizeof(float), num_features * sizeof(float),
        rem - 1);
    __memcpy_async(
        points_z + (repeat % 2) * ping_pong_gap,
        points + (points_start + repeat * deal_num) * num_features + 2,
        sizeof(float), GDRAM2NRAM, sizeof(float), num_features * sizeof(float),
        rem - 1);
  }
  if (repeat > 0) {
    computeDynamicVoxelize(points_x + ((repeat - 1) % 2) * ping_pong_gap,
                           points_y + ((repeat - 1) % 2) * ping_pong_gap,
                           points_z + ((repeat - 1) % 2) * ping_pong_gap,
                           auxiliary_a, auxiliary_b, auxiliary_c, coors_x_min,
                           coors_y_min, coors_z_min, voxel_x, voxel_y, voxel_z,
                           grid_x, grid_y, grid_z, deal_num);
  }
  __sync();

  if (repeat > 0) {
    __memcpy_async(coors_x_start + (repeat - 1) * deal_num,
                   points_x + ((repeat - 1) % 2) * ping_pong_gap,
                   deal_num * sizeof(int32_t), NRAM2GDRAM);
    __memcpy_async(coors_y_start + (repeat - 1) * deal_num,
                   points_y + ((repeat - 1) % 2) * ping_pong_gap,
                   deal_num * sizeof(int32_t), NRAM2GDRAM);
    __memcpy_async(coors_z_start + (repeat - 1) * deal_num,
                   points_z + ((repeat - 1) % 2) * ping_pong_gap,
                   deal_num * sizeof(int32_t), NRAM2GDRAM);
  }
  if (rem > 0) {
    computeDynamicVoxelize(points_x + (repeat % 2) * ping_pong_gap,
                           points_y + (repeat % 2) * ping_pong_gap,
                           points_z + (repeat % 2) * ping_pong_gap, auxiliary_a,
                           auxiliary_b, auxiliary_c, coors_x_min, coors_y_min,
                           coors_z_min, voxel_x, voxel_y, voxel_z, grid_x,
                           grid_y, grid_z, rem);
    __sync();
    __memcpy_async(coors_x_start + repeat * deal_num,
                   points_x + (repeat % 2) * ping_pong_gap,
                   rem * sizeof(int32_t), NRAM2GDRAM);
    __memcpy_async(coors_y_start + repeat * deal_num,
                   points_y + (repeat % 2) * ping_pong_gap,
                   rem * sizeof(int32_t), NRAM2GDRAM);
    __memcpy_async(coors_z_start + repeat * deal_num,
                   points_z + (repeat % 2) * ping_pong_gap,
                   rem * sizeof(int32_t), NRAM2GDRAM);
  }
}

__mlu_global__ void mluPoint2Voxel(int32_t *coors, int32_t *point_to_pointidx,
                                   int32_t *point_to_voxelidx,
                                   const int32_t num_points,
                                   const int32_t max_points) {
  if (__is_mpu()) {
    return;
  }

  const int32_t split_num = 6;
  const int32_t deal_num =
      FLOOR_ALIGN(MAX_NRAM_SIZE / split_num / sizeof(int32_t), NFU_ALIGN_SIZE);
  const int32_t ping_pong_gap = 3 * deal_num * sizeof(int32_t);

  int8_t *coors_x = nram_buffer;
  int8_t *coors_y = coors_x + deal_num * sizeof(int32_t);
  int8_t *coors_z = coors_y + deal_num * sizeof(int32_t);

  int32_t *coors_z_start = coors;
  int32_t *coors_y_start = coors + num_points;
  int32_t *coors_x_start = coors + num_points * 2;

  for (int32_t point_idx = taskId; point_idx < num_points;
       point_idx += taskDim) {
    if (coors_x_start[point_idx] == -1) {
      point_to_pointidx[point_idx] = -1;
      point_to_voxelidx[point_idx] = -1;
      continue;
    }

    int32_t c_x = coors_x_start[point_idx];
    int32_t c_y = coors_y_start[point_idx];
    int32_t c_z = coors_z_start[point_idx];

    int32_t deal_total_num = point_idx;
    int32_t repeat = deal_total_num / deal_num;
    int32_t rem = deal_total_num % deal_num;
    int32_t num = 0;
    int32_t first_point = -1;

    if (repeat > 0) {
      __memcpy_async(coors_x, coors_x_start, deal_num * sizeof(int32_t),
                     GDRAM2NRAM);
      __memcpy_async(coors_y, coors_y_start, deal_num * sizeof(int32_t),
                     GDRAM2NRAM);
      __memcpy_async(coors_z, coors_z_start, deal_num * sizeof(int32_t),
                     GDRAM2NRAM);
      __sync();
    }

    for (int32_t i = 0; i < repeat - 1; ++i) {
      __memcpy_async(coors_x + ((i + 1) % 2) * ping_pong_gap,
                     coors_x_start + (i + 1) * deal_num,
                     deal_num * sizeof(int32_t), GDRAM2NRAM);
      __memcpy_async(coors_y + ((i + 1) % 2) * ping_pong_gap,
                     coors_y_start + (i + 1) * deal_num,
                     deal_num * sizeof(int32_t), GDRAM2NRAM);
      __memcpy_async(coors_z + ((i + 1) % 2) * ping_pong_gap,
                     coors_z_start + (i + 1) * deal_num,
                     deal_num * sizeof(int32_t), GDRAM2NRAM);
      computePoint2Voxel(
          coors_x + (i % 2) * ping_pong_gap, coors_y + (i % 2) * ping_pong_gap,
          coors_z + (i % 2) * ping_pong_gap, c_x, c_y, c_z, max_points, &num,
          &first_point, i * deal_num, deal_num);
      __sync();
    }

    if (rem > 0) {
      __memcpy_async(coors_x + (repeat % 2) * ping_pong_gap,
                     coors_x_start + repeat * deal_num, rem * sizeof(int32_t),
                     GDRAM2NRAM);
      __memcpy_async(coors_y + (repeat % 2) * ping_pong_gap,
                     coors_y_start + repeat * deal_num, rem * sizeof(int32_t),
                     GDRAM2NRAM);
      __memcpy_async(coors_z + (repeat % 2) * ping_pong_gap,
                     coors_z_start + repeat * deal_num, rem * sizeof(int32_t),
                     GDRAM2NRAM);
    }
    if (repeat > 0) {
      computePoint2Voxel(coors_x + ((repeat - 1) % 2) * ping_pong_gap,
                         coors_y + ((repeat - 1) % 2) * ping_pong_gap,
                         coors_z + ((repeat - 1) % 2) * ping_pong_gap, c_x, c_y,
                         c_z, max_points, &num, &first_point,
                         (repeat - 1) * deal_num, deal_num);
    }
    __sync();

    if (rem > 0) {
      computePoint2Voxel(coors_x + (repeat % 2) * ping_pong_gap,
                         coors_y + (repeat % 2) * ping_pong_gap,
                         coors_z + (repeat % 2) * ping_pong_gap, c_x, c_y, c_z,
                         max_points, &num, &first_point, repeat * deal_num,
                         rem);
      __sync();
    }

    if (num == 0) {
      point_to_pointidx[point_idx] = point_idx;
    } else if (num > 0) {
      point_to_pointidx[point_idx] = first_point;
    }

    if (num < max_points) {
      point_to_voxelidx[point_idx] = num;
    } else {
      point_to_voxelidx[point_idx] = -1;
    }
  }
}

__mlu_global__ void mluCalcPointsPerVoxel(
    int32_t *point_to_pointidx, int32_t *point_to_voxelidx,
    int32_t *coor_to_voxelidx, int32_t *num_points_per_voxel,
    int32_t *voxel_num, const int32_t max_voxels, const int32_t num_points) {
#if __BANG_ARCH__ >= 592
  if (coreId == 0) {
    /**|********************************************************|
     * |                    NRAM SPACE DIVIDE                   |
     * |----------------------|---------------|-----------------|
     * |       space          |     reuse     |      size       |
     * |----------------------|---------------|-----------------|
     * | nram_p2p_idx         |               | max_nram_count  |
     * | nram_p2v_idx         |  gather_mask  | max_nram_count  |
     * | nram_scatter_output  | gather_output | max_nram_count  |
     * | nram_scatter_mask    |               | max_nram_count  |
     * | nram_scatter_offset  |               | max_nram_count  |
     * | nram_mask_bitindex   |               | max_nram_count  |
     * | nram_base_offset     |               | max_nram_count  |
     * | nram_temp_zeros      |               | max_nram_count  |
     * | nram_temp_mask       |               | max_nram_count  |
     * |--------------------------------------------------------|
     */
    int32_t voxel_num_temp = 0;
    const int32_t split_num = 9;
    const int32_t max_nram_count = FLOOR_ALIGN(
        MAX_NRAM_SIZE / split_num / sizeof(int32_t), NFU_ALIGN_SIZE);
    int32_t repeat = num_points / max_nram_count;
    int32_t rem = num_points % max_nram_count;
    int32_t *nram_p2p_idx = (int32_t *)nram_buffer;
    int32_t *nram_p2v_idx = (int32_t *)nram_p2p_idx + max_nram_count;
    int32_t *nram_scatter_output = (int32_t *)nram_p2v_idx + max_nram_count;
    int32_t *nram_scatter_mask =
        (int32_t *)nram_scatter_output + max_nram_count;
    int32_t *nram_mask_bitindex = (int32_t *)nram_scatter_mask + max_nram_count;
    int32_t *nram_scatter_offset =
        (int32_t *)nram_mask_bitindex + max_nram_count;
    int32_t *nram_base_offset = (int32_t *)nram_scatter_offset + max_nram_count;
    int32_t *nram_temp_zeros = (int32_t *)nram_base_offset + max_nram_count;
    int32_t *nram_temp_mask = (int32_t *)nram_temp_zeros + max_nram_count;

    // generate 0~deal_num indices.
    __mluop_get_stage_indices_tfuse(nram_base_offset, max_nram_count);
    __bang_write_zero(nram_temp_zeros, max_nram_count);
    for (int32_t i = 0; i <= repeat; i++) {
      if (i == repeat && rem == 0) {
        break;
      }
      int32_t points_offset = i * max_nram_count;
      int32_t deal_num = i == repeat ? rem : max_nram_count;
      int32_t deal_num_align8 = PAD_UP(deal_num, 8);

      // step1: load p2p/p2v data.
      __memcpy_async(nram_p2p_idx, point_to_pointidx + points_offset,
                     deal_num * sizeof(int32_t), GDRAM2NRAM);
      __memcpy(nram_p2v_idx, point_to_voxelidx + points_offset,
               deal_num * sizeof(int32_t), GDRAM2NRAM);

      // step2:ã€€process the first point in voxel
      __bang_eq_scalar(nram_temp_mask, nram_p2v_idx, 0, deal_num);
      __bang_write_value(nram_scatter_output, deal_num, -1);
      __bang_mul(nram_scatter_offset, nram_temp_mask, nram_base_offset,
                 deal_num);

      // compute current loop voxel_num and voxel_id
      int32_t cur_loop_num_voxels =
          __bang_filter((float *)nram_scatter_offset, (float *)nram_base_offset,
                        (float *)nram_temp_mask, deal_num);
      __bang_write_value(nram_scatter_mask, deal_num, 1);
      int32_t reserve_voxels = cur_loop_num_voxels;
      int32_t vir_total_num_voxels = voxel_num_temp + cur_loop_num_voxels;
      if (vir_total_num_voxels >= max_voxels) {
        reserve_voxels = max_voxels - voxel_num_temp;
        __bang_write_value(nram_scatter_mask + reserve_voxels, deal_num, 0);
      }

      if (voxel_num_temp <= max_voxels && reserve_voxels > 0) {
        // compute scatter bitmask
        __bang_gt_bitindex((float *)nram_mask_bitindex,
                           (float *)nram_scatter_mask, (float *)nram_temp_zeros,
                           deal_num_align8);
        // compute scatter offset
        __bang_mul_scalar(nram_scatter_offset, nram_scatter_offset,
                          sizeof(int32_t), deal_num);

        // compute scatter src: voxel_idx
        __bang_add_scalar(nram_temp_mask, nram_base_offset, voxel_num_temp,
                          deal_num);
        __scatter(nram_scatter_output, nram_temp_mask,
                  (unsigned int *)nram_scatter_offset, nram_mask_bitindex,
                  sizeof(int32_t), NRAM2NRAM, sizeof(int32_t), reserve_voxels);
        __memcpy(num_points_per_voxel + voxel_num_temp, nram_scatter_mask,
                 reserve_voxels * sizeof(int32_t), NRAM2GDRAM);
        voxel_num_temp += reserve_voxels;
      }
      // store voxel first points.
      __memcpy(coor_to_voxelidx + points_offset, nram_scatter_output,
               deal_num * sizeof(int32_t), NRAM2GDRAM);

      // step3: process other points in voxels.
      float *gather_mask = (float *)nram_p2v_idx;
      int32_t *gather_output = (int32_t *)nram_scatter_output;
      __bang_gt(gather_mask, (float *)nram_p2v_idx, (float *)nram_temp_zeros,
                deal_num);
      const int32_t count = __bang_filter(
          (float *)nram_p2p_idx, (float *)nram_p2p_idx, gather_mask, deal_num);
      if (count > 0) {
        __bang_mul_scalar(nram_p2p_idx, nram_p2p_idx, sizeof(int32_t), count);
        // get repeated point real point_id
        __gather(gather_output, coor_to_voxelidx, (unsigned int *)nram_p2p_idx,
                 sizeof(int32_t), GDRAM2NRAM, sizeof(int32_t), count);
        __bang_eq_scalar(nram_scatter_mask, gather_output, -1, count);
        __bang_not(nram_scatter_mask, nram_scatter_mask, count);
        __bang_gt_bitindex((float *)nram_mask_bitindex,
                           (float *)nram_scatter_mask, (float *)nram_temp_zeros,
                           PAD_UP(count, 8));
        __bang_add_scalar(nram_temp_mask, nram_base_offset, points_offset,
                          deal_num);
        // get repeated point real voxel_id
        __bang_filter((float *)nram_temp_mask, (float *)nram_temp_mask,
                      gather_mask, deal_num);
        __bang_mul_scalar(nram_temp_mask, nram_temp_mask, sizeof(int32_t),
                          deal_num);
        __scatter(coor_to_voxelidx, gather_output,
                  (unsigned int *)nram_temp_mask, nram_mask_bitindex,
                  sizeof(int32_t), NRAM2GDRAM, sizeof(int32_t), count);

        // step4: compute num_points_per_voxel
        for (int32_t i = 0; i < count; i++) {
          int32_t voxel_idx = gather_output[i];
          if (voxel_idx != -1) {
            num_points_per_voxel[voxel_idx] += 1;
          }
        }
      }
    }
    *voxel_num = voxel_num_temp;
  }
#else
  if (coreId == 0) {
    int32_t split_num = 2;
    bool nram_can_contain_all_c2v_points = false;
    if (num_points <
        FLOOR_ALIGN(MAX_NRAM_SIZE / 3 / sizeof(int32_t), NFU_ALIGN_SIZE)) {
      nram_can_contain_all_c2v_points = true;
      split_num = 3;
    }
    int32_t voxel_num_temp = 0;
    const int32_t max_nram_count = FLOOR_ALIGN(
        MAX_NRAM_SIZE / split_num / sizeof(int32_t), NFU_ALIGN_SIZE);
    int32_t repeat = num_points / max_nram_count;
    int32_t rem = num_points % max_nram_count;
    int32_t *p2p_idx = (int32_t *)nram_buffer;
    int32_t *p2v_idx = (int32_t *)p2p_idx + max_nram_count;
    int32_t *c2v_idx = p2v_idx;
    if (nram_can_contain_all_c2v_points) {
      c2v_idx = p2v_idx + max_nram_count;
      int32_t deal_num = rem;
      __memcpy_async(p2p_idx, point_to_pointidx, deal_num * sizeof(int32_t),
                     GDRAM2NRAM);
      __memcpy_async(p2v_idx, point_to_voxelidx, deal_num * sizeof(int32_t),
                     GDRAM2NRAM);
      __memcpy(c2v_idx, coor_to_voxelidx, deal_num * sizeof(int32_t),
               GDRAM2NRAM);
      __bang_write_value(c2v_idx, deal_num, -1);
      for (int32_t point_idx = 0; point_idx < deal_num; ++point_idx) {
        int32_t point_pos_in_voxel = p2v_idx[point_idx];
        if (point_pos_in_voxel == -1) {
          continue;
        } else if (point_pos_in_voxel == 0) {
          int32_t voxel_idx = voxel_num_temp;
          if (voxel_num_temp >= max_voxels) {
            continue;
          }
          voxel_num_temp += 1;
          c2v_idx[point_idx] = voxel_idx;
          num_points_per_voxel[voxel_idx] = 1;
        } else {
          int32_t point_idx_temp = p2p_idx[point_idx];
          int32_t voxel_idx = c2v_idx[point_idx_temp];
          if (voxel_idx != -1) {
            c2v_idx[point_idx] = voxel_idx;
            num_points_per_voxel[voxel_idx] += 1;
          }
        }
      }
      __memcpy(coor_to_voxelidx, c2v_idx, deal_num * sizeof(int32_t),
               NRAM2GDRAM);
    } else {
      for (int32_t i = 0; i <= repeat; i++) {
        if (i == repeat && rem == 0) {
          break;
        }
        int32_t points_offset = i * max_nram_count;
        int32_t deal_num = i == repeat ? rem : max_nram_count;
        __memcpy_async(p2p_idx, point_to_pointidx + points_offset,
                       deal_num * sizeof(int32_t), GDRAM2NRAM);
        __memcpy(p2v_idx, point_to_voxelidx + points_offset,
                 deal_num * sizeof(int32_t), GDRAM2NRAM);
        for (int32_t point_idx = 0; point_idx < deal_num; ++point_idx) {
          int32_t point_pos_in_voxel = p2v_idx[point_idx];
          coor_to_voxelidx[point_idx + points_offset] = -1;
          if (point_pos_in_voxel == -1) {
            continue;
          } else if (point_pos_in_voxel == 0) {
            int32_t voxel_idx = voxel_num_temp;
            if (voxel_num_temp >= max_voxels) {
              continue;
            }
            voxel_num_temp += 1;
            coor_to_voxelidx[point_idx + points_offset] = voxel_idx;
            num_points_per_voxel[voxel_idx] = 1;
          } else {
            int32_t point_idx_temp = p2p_idx[point_idx];
            int32_t voxel_idx = coor_to_voxelidx[point_idx_temp];
            if (voxel_idx != -1) {
              coor_to_voxelidx[point_idx + points_offset] = voxel_idx;
              num_points_per_voxel[voxel_idx] += 1;
            }
          }
        }
      }
    }
    *voxel_num = voxel_num_temp;
  }
#endif
}

__mlu_global__ void mluAssignVoxelsCoors(
    const float *points, int32_t *temp_coors, int32_t *point_to_voxelidx,
    int32_t *coor_to_voxelidx, float *voxels, int32_t *coors,
    const int32_t max_points, const int32_t num_points,
    const int32_t num_features) {
  if (__is_mpu()) {
    return;
  }
  int32_t points_per_core = num_points / taskDim;
  int32_t points_rem = num_points % taskDim;
  points_per_core += int32_t(points_rem > taskId);
  const int points_core_offset =
      taskId * points_per_core + (points_rem > taskId ? 0 : points_rem);

  const int32_t split_num = 2;
  const int32_t max_nram_count =
      FLOOR_ALIGN(MAX_NRAM_SIZE / split_num / sizeof(int32_t), NFU_ALIGN_SIZE);
  int32_t *p2v_nram = (int32_t *)nram_buffer;
  int32_t *c2v_nram = p2v_nram + max_nram_count;
  const int32_t repeat = points_per_core / max_nram_count;
  const int32_t rem = points_per_core % max_nram_count;

  for (int32_t c_index = 0; c_index <= repeat; c_index++) {
    if (c_index == repeat && rem == 0) {
      break;
    }
    const int32_t deal_num = c_index == repeat ? rem : max_nram_count;
    const int32_t point_nram_start =
        points_core_offset + c_index * max_nram_count;

    __memcpy_async(p2v_nram, point_to_voxelidx + point_nram_start,
                   deal_num * sizeof(int32_t), GDRAM2NRAM);
    __memcpy(c2v_nram, coor_to_voxelidx + point_nram_start,
             deal_num * sizeof(int32_t), GDRAM2NRAM);

    for (int32_t point_idx = 0; point_idx < deal_num; ++point_idx) {
      int32_t num = p2v_nram[point_idx];
      int32_t voxel_idx = c2v_nram[point_idx];
      if (num > -1 && voxel_idx > -1) {
        float *voxels_offset =
            voxels + voxel_idx * max_points * num_features + num * num_features;
        const float *points_offset =
            points + (point_idx + point_nram_start) * num_features;
        __memcpy_async(voxels_offset, points_offset,
                       num_features * sizeof(float), GDRAM2GDRAM);
        if (num == 0) {
          int32_t *coors_offset = coors + voxel_idx * 3;
          __memcpy_async(coors_offset,
                         temp_coors + (point_idx + point_nram_start),
                         sizeof(int32_t), GDRAM2GDRAM, sizeof(int32_t),
                         num_points * sizeof(int32_t), 2);
        }
      }
    }
  }
  __sync();
}

mluOpStatus_t MLUOP_WIN_API KernelDynamicVoxelize(
    cnrtDim3_t k_dim, cnrtFunctionType_t k_type, cnrtQueue_t queue,
    const void *points, const void *voxel_size, const void *coors_range,
    void *coors, const int32_t num_points, const int32_t num_features) {
  KERNEL_CHECK(mluDynamicVoxelize<<<k_dim, k_type, queue>>>(
      (float *)points, (float *)voxel_size, (float *)coors_range,
      (int32_t *)coors, num_points, num_features));
  return MLUOP_STATUS_SUCCESS;
}

mluOpStatus_t MLUOP_WIN_API KernelPoint2Voxel(
    cnrtDim3_t k_dim, cnrtFunctionType_t k_type, cnrtQueue_t queue, void *coors,
    void *point_to_pointidx, void *point_to_voxelidx, const int32_t num_points,
    const int32_t max_points) {
  KERNEL_CHECK(mluPoint2Voxel<<<k_dim, k_type, queue>>>(
      (int32_t *)coors, (int32_t *)point_to_pointidx,
      (int32_t *)point_to_voxelidx, num_points, max_points));
  return MLUOP_STATUS_SUCCESS;
}

mluOpStatus_t MLUOP_WIN_API KernelCalcPointsPerVoxel(
    cnrtDim3_t k_dim, cnrtFunctionType_t k_type, cnrtQueue_t queue,
    void *point_to_pointidx, void *point_to_voxelidx, void *coor_to_voxelidx,
    void *num_points_per_voxel, void *voxel_num, const int32_t max_voxels,
    const int32_t num_points) {
  KERNEL_CHECK(mluCalcPointsPerVoxel<<<k_dim, k_type, queue>>>(
      (int32_t *)point_to_pointidx, (int32_t *)point_to_voxelidx,
      (int32_t *)coor_to_voxelidx, (int32_t *)num_points_per_voxel,
      (int32_t *)voxel_num, max_voxels, num_points));
  return MLUOP_STATUS_SUCCESS;
}

mluOpStatus_t MLUOP_WIN_API KernelAssignVoxelsCoors(
    cnrtDim3_t k_dim, cnrtFunctionType_t k_type, cnrtQueue_t queue,
    const void *points, void *temp_coors, void *point_to_voxelidx,
    void *coor_to_voxelidx, void *voxels, void *coors, const int32_t max_points,
    const int32_t num_points, const int32_t num_features) {
  KERNEL_CHECK(mluAssignVoxelsCoors<<<k_dim, k_type, queue>>>(
      (float *)points, (int32_t *)temp_coors, (int32_t *)point_to_voxelidx,
      (int32_t *)coor_to_voxelidx, (float *)voxels, (int32_t *)coors,
      max_points, num_points, num_features));
  return MLUOP_STATUS_SUCCESS;
}
