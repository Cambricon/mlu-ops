/*************************************************************************
 * Copyright (C) 2022 by Cambricon, Inc. All rights reserved.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#include "roi_crop_forward.h"
  
#include <string>
#include <iostream>
  
#include "core/context.h"
#include "core/logging.h"
#include "core/runtime/device.h"
#include "core/tensor.h"
#include "core/type.h"
#include "kernels/binary_op/binary_op_host.h"
#include "kernels/kernel.h"

#include "mlu_op.h"

 static void policyFunc(const mluOpHandle_t &handle,
                        int bin_num,
                        cnrtDim3_t *k_dim,
                        cnrtFunctionType_t *k_type){
  //unsigned int cluster_num = mluop::runtime::getClusterLimitCapability(handle);
  unsigned int core_in_cluster = handle->core_num_per_cluster;
  *k_type = CNRT_FUNC_TYPE_UNION1; 
  k_dim->x = core_in_cluster;
  //unsigned int use_cluster = (bin_num + core_in_cluster -1) / core_in_cluster;
  //k_dim->y = use_cluster > cluster_num ? cluster_num : use_cluster;
  k_dim->y = 1;
  k_dim->z = 1;

}

mluOpStatus_t MLUOP_WIN_API mluOpRoiCropForward(mluOpHandle_t handle,
                                                const mluOpTensorDescriptor_t input_desc,
                                                const void *input,
                                                const mluOpTensorDescriptor_t grid_desc,
                                                const void *grid,
                                                const mluOpTensorDescriptor_t output_desc,
                                                void *output) {
  //check params
  const std::string op_name = "[mluOpRoiCropForward]";

  // check descriptor
  PARAM_CHECK(op_name, handle != NULL);
  PARAM_CHECK(op_name, input_desc != NULL);
  PARAM_CHECK(op_name, grid_desc != NULL);
  PARAM_CHECK(op_name, output_desc != NULL);

  // check dtype equal
  PARAM_CHECK_EQ(op_name, input_desc->dtype, grid_desc->dtype);
  PARAM_CHECK_EQ(op_name, input_desc->dtype, output_desc->dtype);

  // check dim and layout
  PARAM_CHECK_EQ(op_name, input_desc->dim, 4);
  PARAM_CHECK(op_name, input_desc->layout == MLUOP_LAYOUT_NHWC);
  PARAM_CHECK_EQ(op_name, grid_desc->dim, 4);
  PARAM_CHECK(op_name, grid_desc->layout == MLUOP_LAYOUT_NHWC);
  PARAM_CHECK_EQ(op_name, output_desc->dim, 4);
  PARAM_CHECK(op_name, output_desc->layout == MLUOP_LAYOUT_NHWC);
  if (output_desc->dims[0] != grid_desc->dims[0]||output_desc->dims[1] != grid_desc->dims[1]||output_desc->dims[2] != grid_desc->dims[2]) {
    LOG(ERROR) << op_name << ":Check failed: output_desc->dims[0]/[1]/[2] should be equal to grid_desc->dims[0]/[1]/[2].";
    return MLUOP_STATUS_BAD_PARAM;
  }
  if (output_desc->dims[3] != input_desc->dims[3]) {
    LOG(ERROR) << op_name << ":Check failed: output_desc->dims[3] should be equal to input_desc->dims[3].";
    return MLUOP_STATUS_BAD_PARAM;
  }
  if (grid_desc->dims[0]!=2)
  {
    LOG(ERROR) << op_name << ":Check failed: grid_desc->dims[0] should be equal to 2.";
    return MLUOP_STATUS_BAD_PARAM;
  }
  
    // check 0 element
  if ((mluOpGetTensorElementNum(input_desc) == 0) ||
      (mluOpGetTensorElementNum(grid_desc) == 0) ||
      (mluOpGetTensorElementNum(output_desc) == 0)) {
    VLOG(5) << op_name << " skip zero element tensor.";
    return MLUOP_STATUS_SUCCESS;
  }
  // check device pointer
  PARAM_CHECK(op_name, input != NULL);
  PARAM_CHECK(op_name, grid != NULL);
  PARAM_CHECK(op_name, output != NULL);

  uint32_t batch = input_desc->dims[0];
  uint32_t height = input_desc->dims[1];
  uint32_t width = input_desc->dims[2];
  uint32_t channels = input_desc->dims[3];

  uint32_t grid_n = grid_desc->dims[0];
  
  uint32_t output_h = output_desc->dims[1];
  uint32_t output_w = output_desc->dims[2];

  uint32_t bin_num = grid_n * output_h * output_w;

  cnrtDim3_t k_dim;
  cnrtFunctionType_t k_type;
  policyFunc(handle, bin_num, &k_dim, &k_type);

  mluOpDataType_t data_type = input_desc->dtype;
 

  VLOG(5) << "[mluOpRoiCropForward] launch kernel policyFUnc[" << k_dim.x << ", " << k_dim.y << ", " << k_dim.z << "]";
  KERNEL_CHECK((MLUKernelRoiCropForward<<<k_dim, k_type, handle->queue>>>(input, batch, height, width, channels, grid, grid_n, output, output_h, output_w, data_type)));
  
  return MLUOP_STATUS_SUCCESS;
}