/*************************************************************************
 * Copyright (C) [2023] by Cambricon, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#include "mutual_information_forward.h"

#include "kernels/kernel.h"
#include "kernels/utils/common.h"

#define MIN_LOG_DIFF_FLOAT -15.9423847198486328125f
#define MIN_POS_NAN 0x7f800001

__nram__ char nram_buffer[MAX_NRAM_SIZE];

__mlu_func__ void computeLog(float *nram_dst, float *nram_src,
                             const int32_t deal_num) {
  int x2d = 0x3f317217;
  float rlog2e = *(float *)&x2d;
  __bang_log(nram_dst, nram_src, deal_num);
  __bang_mul_scalar(nram_dst, nram_src, rlog2e, deal_num);
}

__mlu_func__ void logAddVector(float *dst, float *src1, float *src2,
                               float *max_value, float *mask, float *temp,
                               int data_num) {
  __bang_nan_minimum(dst, src1, src2, data_num);
  __bang_maximum(max_value, src1, src2, data_num);

  // if src1 is nan, then max_value = src1
  int nan_mask_i = 0x7fffffff;
  float nan_mask_f = *(float *)&nan_mask_i;
  __bang_band_scalar(mask, src1, nan_mask_f, data_num);
  __bang_ge_scalar((int *)mask, (int *)mask, MIN_POS_NAN, data_num);
  __bang_mul_scalar((int *)mask, (int *)mask, -1, data_num);
  __bang_band((char *)temp, (char *)src1, (char *)mask,
              data_num * sizeof(float));
  __bang_add(max_value, max_value, temp, data_num);

  // compute log sum exp
  __bang_sub(dst, dst, max_value, data_num);
  __bang_ge_scalar(mask, dst, MIN_LOG_DIFF_FLOAT, data_num);
  __mluop_exp(dst, dst, nullptr, 0, data_num);
  __bang_add_scalar(dst, dst, 1.f, data_num);
  computeLog(dst, dst, data_num);
  __bang_add(dst, dst, max_value, data_num);

  // if min_value - max_value < MIN_LOG_DIFF_FLOAT, return the larger one
  __bang_float2int32_tz((int *)mask, mask, data_num, 0);
  __bang_mul_scalar((int *)mask, (int *)mask, -1, data_num);
  __bang_band((char *)dst, (char *)dst, (char *)mask, data_num * sizeof(float));
  __bang_add_scalar((int *)mask, (int *)mask, 1, data_num);
  __bang_mul_scalar((int *)mask, (int *)mask, -1, data_num);
  __bang_band((char *)max_value, (char *)max_value, (char *)mask,
              data_num * sizeof(float));
  __bang_add(dst, dst, max_value, data_num);
}

__mlu_func__ void pipelineLoad(float *nram_px, float *nram_py,
                               const int T, const int s_len, const int t_len,
                               const int max_len, const int min_len,
                               const int s_begin, const int t_begin,
                               const int t_end, const int diagonal_num,
                               const int i, float *ping) {
  int data_num = i < max_len ? __mluop_min(i + 1, min_len) : diagonal_num - i;
  int s = i - 1 < t_len ? s_begin : i - t_len + s_begin;
  int t = i - 1 < t_len ? i + t_begin - 1 : t_end;
  int px_num = i < t_len ? data_num - 1 : data_num;
  if (px_num > 0) {
    __memcpy_async(ping + min_len + 1, nram_px + s * (T + 1) + t, sizeof(float),
                   NRAM2NRAM, sizeof(float), T * sizeof(float), px_num - 1);
  }

  int py_num = i < s_len ? data_num - 1 : data_num;
  if (i >= t_len) {
    s += 1;
    t -= 1;
  }

  if (py_num > 0) {
    __memcpy_async(ping, nram_py + s * T + t, sizeof(float), NRAM2NRAM,
                   sizeof(float), (T - 1) * sizeof(float), py_num - 1);
  }
}

__mlu_func__ void pipelineCompute(const int s_len, const int t_len,
                                  const int max_len, const int min_len,
                                  const int diagonal_num, const int i,
                                  float *max_value, float *mask, float *temp,
                                  float *pong_p, float *ping) {
  float *ping_py = ping;
  float *ping_px = ping_py + min_len + 1;
  float *ping_p = ping_px + min_len;

  int data_num = i < max_len ? __mluop_min(i + 1, min_len) : diagonal_num - i;

  int px_num = i < t_len ? data_num - 1 : data_num;
  if (px_num > 0) {
    __bang_add(ping_px, ping_px, pong_p, px_num);
  }

  int py_num = i < s_len ? data_num - 1 : data_num;
  if (i >= t_len) {
    pong_p += 1;
  }
  if (py_num > 0) {
    __bang_add(ping_py, ping_py, pong_p, py_num);
  }

  ping_px = i < t_len ? ping_px - 1 : ping_px;
  logAddVector(ping_p, ping_px, ping_py, max_value, mask, temp, data_num);

  __bang_write_value(ping, 2 * min_len + 1, -INFINITY);
}

__mlu_func__ void pipelineStore(float *nram_p, const int T, const int t_len,
                                const int max_len,
                                const int min_len, const int s_begin,
                                const int t_begin, const int t_end,
                                const int diagonal_num, const int i,
                                float *ping_p) {
  int data_num = i < max_len ? __mluop_min(i + 1, min_len) : diagonal_num - i;
  int s = i < t_len ? s_begin : i - t_len + 1 + s_begin;
  int t = i < t_len ? i + t_begin : t_end;
  __memcpy_async(nram_p + s * (T + 1) + t, ping_p, sizeof(float), NRAM2NRAM,
                 T * sizeof(float), sizeof(float), data_num - 1);
}

__mlu_func__ void computeMutualInformation(const int b, const int S,
                                           const int T, const bool has_boundary,
                                           const int s_begin, const int s_end,
                                           const int t_begin, const int t_end,
                                           const float *px, const float *py,
                                           float *p, float *ans) {
  /* *********************nram space split********************** */
  /* |--------------------------COMMON-------------------------| */
  /* |  px   |  py   |     p     | max_val |  mask   |  temp   | */
  /* |S*(T+1)|(S+1)*T|(S+1)*(T+1)| min_len | min_len | min_len | */
  /* |------------PING------------|------------PONG------------| */
  /* | cur_py|-inf|cur_px | cur_p | cur_py|-inf|cur_px | cur_p | */
  /* |min_len| 1  |min_len|min_len|min_len| 1  |min_len|min_len| */
  const int px_one_batch_size = S * (T + 1);
  const int py_one_batch_size = (S + 1) * T;
  const int p_one_batch_size = (S + 1) * (T + 1);

  float *nram_px = (float *)nram_buffer;
  float *nram_py = nram_px + px_one_batch_size;
  float *nram_p = nram_py + py_one_batch_size;

  if (S > 0) {
    __memcpy(nram_px, px + b * px_one_batch_size,
             px_one_batch_size * sizeof(float), GDRAM2NRAM);
  }

  if (T > 0) {
    __memcpy(nram_py, py + b * py_one_batch_size,
             py_one_batch_size * sizeof(float), GDRAM2NRAM);
  }

  if (has_boundary) {
    __memcpy(nram_p, p + b * p_one_batch_size, p_one_batch_size * sizeof(float),
             GDRAM2NRAM);
  }

  const int s_len = s_end - s_begin + 1;
  const int t_len = t_end - t_begin + 1;
  const int max_len = __mluop_max(s_len, t_len);
  const int min_len = __mluop_min(s_len, t_len);
  const int ping_pong_gap = 3 * min_len + 1;

  float *nram_max_value = nram_p + p_one_batch_size;
  float *nram_mask = nram_max_value + min_len;
  float *nram_temp = nram_mask + min_len;

  float *ping = nram_temp + min_len;
  float *ping_p = ping + 2 * min_len + 1;

  __bang_write_value(ping, ping_pong_gap * 2, -INFINITY);

  nram_p[s_begin * (T + 1) + t_begin] = (float)0;
  ping_p[ping_pong_gap] = (float)0;

  __sync();

  int repeat = s_len + t_len - 2;
  for (int i = 0; i < repeat + 2; ++i) {
    if (i < repeat) {
      pipelineLoad(nram_px, nram_py, T, s_len, t_len, max_len, min_len, s_begin,
                   t_begin, t_end, repeat + 1, i + 1,
                   ping + (i % 2) * ping_pong_gap);
    }

    if (i > 0 && i <= repeat) {
      pipelineCompute(s_len, t_len, max_len, min_len, repeat + 1, i,
                      nram_max_value, nram_mask, nram_temp,
                      ping_p + (i % 2) * ping_pong_gap,
                      ping + ((i - 1) % 2) * ping_pong_gap);
    }

    if (i > 1) {
      pipelineStore(nram_p, T, t_len, max_len, min_len, s_begin, t_begin, t_end,
                    repeat + 1, i - 1, ping_p + (i % 2) * ping_pong_gap);
    }
    __sync();
  }

  __memcpy(ans + b, nram_p + s_end * (T + 1) + t_end, sizeof(float),
           NRAM2GDRAM);
  __memcpy(p + b * p_one_batch_size, nram_p, p_one_batch_size * sizeof(float),
           NRAM2GDRAM);
}

__mlu_global__ void mluBlockMutualInformationForward(
    const int B, const int S, const int T, const float *px, const float *py,
    const bool has_boundary, const int64_t *opt_boundary, float *p,
    float *ans) {
  const int num_per_core = B / taskDim;
  const int num_rem = B % taskDim;
  const int num_cur_core = num_per_core + (taskId < num_rem);
  const int b_offset = taskId * num_cur_core + (taskId >= num_rem) * num_rem;

  int s_begin = 0;
  int t_begin = 0;
  int s_end = S;
  int t_end = T;
  if (has_boundary) {
    int64_t *boundary = (int64_t *)nram_buffer;
    for (int b = b_offset; b < b_offset + num_cur_core; ++b) {
      __memcpy(boundary, opt_boundary + 4 * b, 4 * sizeof(int64_t), GDRAM2NRAM);
      s_begin = boundary[0];
      t_begin = boundary[1];
      s_end = boundary[2];
      t_end = boundary[3];

      if (s_begin > s_end || t_begin > t_end) {
        ans[b] = 0;
        continue;
      }
      computeMutualInformation(b, S, T, has_boundary, s_begin, s_end, t_begin,
                               t_end, px, py, p, ans);
    }
  } else {
    for (int b = b_offset; b < b_offset + num_cur_core; ++b) {
      computeMutualInformation(b, S, T, has_boundary, s_begin, s_end, t_begin,
                               t_end, px, py, p, ans);
    }
  }
}

void MLUOP_WIN_API kernelMutualInformationForward(
  cnrtDim3_t k_dim, cnrtFunctionType_t k_type, cnrtQueue_t queue,
  const int B, const int S, const int T, const void *px, const void *py,
  const bool has_boundary, const void *opt_boundary, void *p, void *ans) {
  mluBlockMutualInformationForward<<<k_dim, k_type, queue>>>(
      B, S, T, (float *)px, (float *)py, has_boundary, (int64_t *)opt_boundary,
      (float *)p, (float *)ans);
}
