#include "cholesky.h"
#include <cmath> 


__nram__ uint8_t nram_buffer[MAX_NRAM_SIZE];

__mlu_func__
float recur_add(float* input, int length)
{
    if(length == 1)
    {
        return input[0];
    }
    else
    {
        int half_length;
        half_length = length / 2;
        float sum1 = recur_add(input, half_length);
        float sum2 = recur_add(input + half_length, length - half_length);
        input[0] = sum1+sum2;
        return sum1 + sum2;
    }

}

__mlu_func__
float kahansum(float* input, int length)
{
    float sum = 0.0;
    float c = 0.0;
    for(int i = 0; i < length; i++)
    {
        float y = input[i] - c;
        float t = sum + y;
        c = (t - sum) - y;
        sum = t;
    }
    input[0] = sum;
    return sum;
}
    
__mlu_func__ 
void sgemm_fixwidth_device(int m, int k,
        float* A0, const int lda,
        float *sC, float  *sB)
{
    int id = taskId % 4;
    
    int span = POTF_NB;
    

    __nram__ float rC[M * POTF_NB/TASK_NUM ];
    __nram__ float rA[M * POTF_NB/TASK_NUM ];
    __nram__ float rp[M * POTF_NB/TASK_NUM ];

    __nram__ float rB[POTF_NB * POTF_NB];

    __nram__ float temp_result[POTF_NB * POTF_NB];
    temp_result[0] = 0.0;


    if(id*span<m)
        __memcpy(rp,OFFSET_ROW(A0,span*id,0),POTF_NB*sizeof(float),GDRAM2NRAM,POTF_NB*sizeof(float),lda*sizeof(float),span-1);
    __memset_nram(rC,POTF_NB*span,(float)ZERO);
    __sync_cluster();
    


    if(id == 0)
    {
        __memcpy(sB,rp,POTF_NB*POTF_NB*sizeof(float),NRAM2SRAM);
    }


    __sync_cluster();


    for(int iter = 0; iter < k; iter+= POTF_NB)
    {
        __bang_move(rA,rp,POTF_NB*span*sizeof(float));
        __memcpy(rB,sB,POTF_NB*POTF_NB*sizeof(float),SRAM2NRAM);
        
        __sync_cluster();
        if(id*span<m)
            __memcpy_async(rp,OFFSET_ROW(A0,span*id,iter+POTF_NB),POTF_NB*sizeof(float),GDRAM2NRAM,POTF_NB*sizeof(float),lda*sizeof(float),span-1);
        

        for(int i = 0; i < span; i++)
        {
            for(int j = 0; j < POTF_NB; j++)
            {

                for(int h = 0; h < POTF_NB; h++)
                {
                    temp_result[h] = rA[i*POTF_NB+h] * rB[j*POTF_NB+h];
                }

                rC[i*POTF_NB+j] +=kahansum(temp_result,POTF_NB);
            }
        }
       
        __sync_cluster();
        if(id == 0)
            __memcpy(sB,rp,POTF_NB*POTF_NB*sizeof(float),NRAM2SRAM);
        __sync_cluster();
    }

    


    __bang_sub(rp,rp,rC,POTF_NB * span);

    if(id*span<m)
        __memcpy(sC+coreId*span*POTF_NB,rp,POTF_NB* span *sizeof(float),NRAM2SRAM);
    __sync_cluster();
}  

static __mlu_func__ void spotf2_sminout_fixsize_device(int m, float *A, int lda)
{
    
    int id = coreId % 4;
    int span = POTF_NB;
    float* diag = (float*)nram_buffer;
    float* nram_src = diag + span * span;
    
    __memcpy(diag,A,span*span*sizeof(float),SRAM2NRAM);
    __memcpy(nram_src,A + id *span*POTF_NB,span*span*sizeof(float),SRAM2NRAM);
   
    float factor;
    for(int iter = 0; iter < POTF_NB; iter++)
    {
        
        
        
        if(iter>0)
        {
            float* temp_result = nram_src+span*span;
            float* temp_result2 = temp_result + span * span;
            float* temp_a = nram_src;
            float* temp_b = diag+iter*span;
            float* local_result = temp_result;
            float* local_diag = temp_result2;
         
            for(int i = 0; i < span; i++)
            {
                __bang_mul(local_result, temp_a, temp_b, iter);
                __bang_mul(local_diag, diag+i*span, temp_b,iter);
                local_result = local_result + span;
                temp_a = temp_a + span;
                local_diag = local_diag + span;
            }
          
            if(iter>1)
            {
                local_result = temp_result;
                local_diag = temp_result2;
                for(int i = 0; i < span; i++)
                {
                    kahansum(local_result,iter);
                    kahansum(local_diag,iter);
                    local_result = local_result + span;
                    local_diag = local_diag + span;
                }
            }
            for(int i = 0; i < span; i++)
            {
                nram_src[i*span+iter] -= temp_result[i*span];
                diag[i*span+iter] -= temp_result2[i*span];
            }
            
        }

        if(factor<0)
        {
            if(id == 0)
            {
                printf("factor:%.3f\n",factor);
                printf("iter:%d\n",iter);
            }

        }
        factor = diag[iter*POTF_NB+iter];
        __nram__ float temp[1];
        __bang_rsqrt(temp,diag+iter*POTF_NB+iter,1);
        factor = temp[0];

 
        for(int i = 0; i < span; i++)
        {
           
            nram_src[i*POTF_NB+iter] *= factor;
            diag[i*POTF_NB+iter] *= factor;
            
            
        }
        __sync();

      
        
        

    }
    __sync_cluster();
   
    
    if(id*span<m)
        __memcpy(A + id *span * POTF_NB,nram_src,span*span*sizeof(float),NRAM2SRAM);
    __sync_cluster();
   
}

__mlu_func__ 
void sgemm_anywidth_device(int m, int k,
        float* A0, const int lda,
        float *sC, float  *sB)
{
    int id = taskId % 4;
    

    int remain = m - id * POTF_NB;
    bool if_execute = remain > 0;
    int span =  (remain > POTF_NB||remain <= 0) ? POTF_NB : remain;

    float *rA = (float*)nram_buffer + id * NB * NB * 4;

    float *rB = rA + NB * NB;

    float *rC = rB + NB * NB;

    float* rp = rC + NB * NB;

    int span_b = POTF_NB > m ? m : POTF_NB;



    __memset_nram(rC,span_b*span,(float)ZERO);

    if(if_execute)
    {
        if(k>0)
        {
            __memcpy(rA,A0+id*POTF_NB*lda,k*sizeof(float),SRAM2NRAM,NB*sizeof(float),lda*sizeof(float),span-1);
        }
        __memcpy(rp,sC+id*POTF_NB*lda,span_b*sizeof(float),SRAM2NRAM,span_b*sizeof(float),lda*sizeof(float),span-1);

    }
        
    if(k>0)
    {
       
        __memcpy(rB,A0,k*sizeof(float),SRAM2NRAM,NB*sizeof(float),lda*sizeof(float),span_b-1);
        
        
    }

 
    

    __sync_cluster();

    for(int i = 0; i < span; i++)
    {
        for(int j = 0; j < span_b; j++)
        {
            for(int h = 0; h < k; h++)
            {
                rC[i*span_b+j] += rA[i*NB+h] * rB[j*NB+h];
            }
        }
    }

    __bang_sub(rp,rp,rC,span_b * span);

    __sync_cluster();

    if(id==0)
    {
        for(int i = 0; i < span; i++)
        {
            __memcpy(sC+(i*lda),rp+i*span_b,(i+1)*sizeof(float),NRAM2SRAM);
        } 
       
    }
    else if(if_execute)
    {
        __memcpy(sC+(id*POTF_NB*lda),rp,span_b*sizeof(float),NRAM2SRAM,lda*sizeof(float),span_b*sizeof(float),span-1);
    }
    





}


static __mlu_func__ void spotf2_sminout_anysize_device(int m, float *A, int lda)
{
    float factor;
    int id = coreId % 4;
    int finish = id * POTF_NB;
    int remain = m - finish;
    bool if_execute = remain > 0;
    int span =  remain > POTF_NB ? POTF_NB : remain;
    int iter_num = m > POTF_NB ? POTF_NB : m;
    for(int iter = 0; iter < iter_num; iter++)
    {
        factor=sqrt(A[iter*lda+iter]);
        factor = 1.0/factor;
        __sync_cluster();
        for(int i = 0; i < span; i++)
        {
            if(if_execute)
                A[i*lda+iter+id*POTF_NB*lda] *= factor;
            
        }
        __sync_cluster();

        if(if_execute)
        {
            for(int i = iter + 1; i < iter_num; i++)
            {
                for(int j = finish; j < finish + span; j++)
                {
                    if(j < i)
                        continue;
                    A[j * lda + i ] -= A[i*lda+iter] * A[j * lda + iter];
                }
            }
        }
        
        __sync_cluster();

    }
    
}

__mlu_func__ void spotf2_smlpout_fixwidth_device(const int m, float *A0, float *A, int lda, const int localstep, const int gbstep)
{
    int id = taskId % 4;
    __mlu_shared__  float shared_data[SHARED_MEM_SIZE];
    float* sdata_A = shared_data;
    float* sdata_B = shared_data + m *POTF_NB/TASK_NUM * 4;

    

    sgemm_fixwidth_device(m, localstep, A0, lda, sdata_A, sdata_B);


    __sync_cluster();

   

    spotf2_sminout_fixsize_device(m, sdata_A, POTF_NB);

    __sync_cluster();

    int span = POTF_NB;

    if(id==0)
    {
        for(int i = 0; i < span; i++)
        {

             __memcpy(A+(i*lda),sdata_A+i*POTF_NB,(i+1)*sizeof(float),SRAM2LDRAM);
        }
       
    }
    else if(id*span < m)
    {
        __memcpy(A+(id*POTF_NB*lda),sdata_A+coreId*POTF_NB*POTF_NB,POTF_NB*sizeof(float),SRAM2LDRAM,lda*sizeof(float),POTF_NB*sizeof(float),span-1);
    }

    __sync_cluster();




}

__mlu_func__ void spotf2_smlpout_anywidth_device(const int m, float *A0, float *A, int lda, const int localstep, const int gbstep)
{

    sgemm_anywidth_device(m, localstep, A0, lda, A, nullptr);


    
    spotf2_sminout_anysize_device(m, A, lda);

    __sync_cluster();




}


__mlu_global__ void spotf2_smlpin_anywidth_kernel(int batch, int stride, bool trans, int m, float *dA, int lda, int localstep, int gbstep)
{
    int id = taskId;

    float* orignA = dA;

        int batch_id = id / 4;
        if(batch_id >= batch)
            return;
        dA = orignA + batch_id * stride;

    __mlu_shared__  float shared_data[NB * NB];

    if(m%4==0)
    {
        for(int i = 0; i < m; i += POTF_NB)
        {
            spotf2_smlpout_fixwidth_device(m-i,OFFSET_ROW(dA, localstep + i,0), OFFSET_ROW(dA, localstep + i, localstep + i), lda, localstep+i, gbstep);
        }
    }
    else
    {
        
        if(id == 0)
        {
            __memcpy(shared_data,dA,m*sizeof(float),GDRAM2SRAM,NB*sizeof(float),lda*sizeof(float),m-1);

        }
        __sync_cluster();

        for(int i = 0; i < m; i += POTF_NB)
        {
            spotf2_smlpout_anywidth_device(m-i,shared_data+i*NB, shared_data+i*NB+i, NB, localstep+i, gbstep);
        }

        __sync_cluster();

        if(id == 0)
        {
            __memcpy(dA,shared_data,m*sizeof(float),SRAM2GDRAM,lda*sizeof(float),NB*sizeof(float),m-1);
        }
        __sync_cluster();
    } 
    

}

__mlu_func__ 
void small_sgemm_batch(int m, int k,
        float* A0, const int lda,int width, 
        float* dst, float* nram_remain)
{
    int ldk = k;
    int ldm = m;
    float* src1 = nram_remain;
    float* src2 = src1 + ldk * ldm;
    float* dst2 = src2 + width * ldk;
    
    float* dA = A0 + k;
    __memcpy_async(dst, dA, width*sizeof(float),GDRAM2NRAM,width*sizeof(float),lda*sizeof(float),m-1);

    if(k == 0)
    {
        __sync();
        return;
    }
    
    __memset_nram(src1,ldm*ldk,(float)ZERO);

    __memcpy_async(src1, A0, k*sizeof(float),GDRAM2NRAM,ldk*sizeof(float),lda*sizeof(float),m-1);

    __memset_nram(dst2,ldm*width,(float)ZERO);

    __sync();

    __memcpy(src2, src1, ldk*width*sizeof(float),NRAM2NRAM);

    for(int i = 0; i < m; i++)
    {
        for(int j = 0; j < width; j++)
        {
            for(int h = 0; h < k; h++)
            {
                dst2[i*width+j] += src1[i*ldk+h] * src2[j*ldk+h];
            }
        }
    }

    __bang_sub(dst,dst,dst2,width * m);

    __sync();
}

__mlu_func__ void small_sminout_batch(int m, int width, float *dst, float *nram_remain, int lda)
{
    float factor;
    float* diag = dst;

    for(int iter = 0; iter < width; iter++)
    {
        factor=sqrt(diag[iter*width+iter]);
        factor = 1.0/factor;
        for(int i = 0; i < m; i ++)
        {
            dst[i*width+iter] *= factor;
        }
        __sync();
        for(int i = iter + 1; i < width; i++)
        {
            for(int j = 0; j < m; j++)
            {
                dst[j * width + i ] -= dst[i*width+iter] * dst[j * width + iter];
                
            }
        }
        __sync();


        
        

    }
    __sync();

    
}

__mlu_func__ 
void smlpout_batch(const int m, float *A0, float *A, int lda, const int localstep, int width)
{
    float* dst = (float*)nram_buffer;
    float* nram_remain = dst + m * m;

    
    small_sgemm_batch(m, localstep, A0, lda, width, dst, nram_remain);

    __sync();
    
    small_sminout_batch(m, width, dst, nram_remain, width);

    __sync();

    for(int i = 0; i < width; i++)
    {
        __memcpy(A+(i*lda),dst+i*width,(i+1)*sizeof(float),NRAM2GDRAM);
    }

    if(m > width)
    {
        __memcpy(A+(width*lda),dst+width*width,width*sizeof(float),NRAM2GDRAM,lda*sizeof(float),width*sizeof(float),m-width-1);
    }

    __sync();


}

__mlu_global__ void spotf2_batch_kernel(int batch, int stride, int m, float *dA, int lda)
{
    int id = taskId;
    int batch_id = id;
    if(batch_id >= batch)
        return;
    float* orignA = dA;
    dA = orignA + batch_id * stride;
    int width = POTF_NB;
    int span = width;

    for(int i = 0; i < m; i += width)
    {
        span = std::min(width, m - i);
        smlpout_batch(m-i,dA+i*lda,dA+i*lda+i,lda,i,span);
    }

}

mluOpStatus_t mlu_spotf2_lpin(int batch, int stride, bool trans,bool uplo, int n, int ldda, float* dA, int gbstep, cnrtQueue_t queue)
{
    cnrtDim3_t dim;
    cnrtFunctionType_t func_type = CNRT_FUNC_TYPE_BLOCK;
    dim.y = 1;
    dim.z = 1;
    if(batch > 1)
    {
        dim.x = batch;
        KERNEL_CHECK(
            spotf2_batch_kernel<<<dim, func_type, queue>>>(batch, stride, n, dA, ldda));
    }
    else
    {
        int carry_batch = batch;
        if(batch == 1)
        {
            func_type = CNRT_FUNC_TYPE_UNION1;
        }
        else if(batch == 2)
        {
            func_type = CNRT_FUNC_TYPE_UNION2;
        }
        else if(batch <= 4)
        {
            func_type = CNRT_FUNC_TYPE_UNION4;
            carry_batch = 4;
        }
        else 
        {
            func_type = CNRT_FUNC_TYPE_UNION8;
            carry_batch = batch < 8 ? 8 : batch;
        }
        dim.x = carry_batch * 4;
        KERNEL_CHECK(
          spotf2_smlpin_anywidth_kernel<<<dim, func_type, queue>>>(batch, stride, trans, n, dA, ldda, 0,gbstep));
    
    }

    return MLUOP_STATUS_SUCCESS;
}

__mlu_entry__ void mlu_strsm_rectile_batch_kernel(
    int batch, int stride,
    int m,int n, bool trans,
    float *dA, int32_t lda,
    float *dB, int32_t ldb)
{
    int id = taskId;
    int batch_id = id;
    if(batch_id >= batch)
        return;
    float* orignA = dA;
    float* orignB = dB;
    dA = orignA + batch_id * stride;
    dB = orignB + batch_id * stride;
    
    int span = n;
    int start = 0;

    __nram__ float sA[8*POTF_NB];
    __nram__ float rB[4*POTF_NB * 8*POTF_NB];
    __nram__ float rC[4*POTF_NB * 8*POTF_NB];
    __nram__ float rBp[4*POTF_NB];
    __nram__ float rA[8*POTF_NB];
    int calc_length = (8 * POTF_NB) > m ? m : (8 * POTF_NB);
    __memset_nram(rB,POTF_NB*calc_length,(float)ZERO);
    __memset_nram(sA,calc_length*calc_length,(float)ZERO);


    float temp_b = 0, factor = 0;
    


    __memcpy_async(sA,dA,sizeof(float),GDRAM2NRAM);

    __memcpy(rBp,OFFSET_B_ROW(dB,start,0),sizeof(float),GDRAM2NRAM,sizeof(float), ldb * sizeof(float), span - 1);
    __sync();

    if(trans)
    {
        __memcpy_async(rA,sA,(1)*sizeof(float),NRAM2NRAM);
        __memcpy_async(rB,rBp,sizeof(float),NRAM2NRAM,calc_length * sizeof(float), sizeof(float), span - 1);
        __sync();

        __memcpy_async(sA,OFFSET_ROW(dA,1,0),2*sizeof(float),GDRAM2NRAM);
        __memcpy_async(rBp,OFFSET_B_ROW(dB,start,1),sizeof(float),GDRAM2NRAM,sizeof(float), ldb * sizeof(float), span - 1);
        factor = 1.0 / rA[0];
        for(int i = 0; i < span; i++)
        {
            rB[i*calc_length] *=  factor;
        }

        __sync();
        
        for(int iter = 1; iter < m - 1; iter++)
        {
            __memcpy_async(rA,sA,(iter+1)*sizeof(float),NRAM2NRAM);
            __memcpy_async(rB+iter,rBp,sizeof(float),NRAM2NRAM,calc_length * sizeof(float), sizeof(float), span - 1);
            __sync();

            __memcpy_async(sA,OFFSET_ROW(dA,iter+1,0),(iter+2)*sizeof(float),GDRAM2NRAM);
            __memcpy_async(rBp,OFFSET_B_ROW(dB,start,iter+1),sizeof(float),GDRAM2NRAM,sizeof(float), ldb * sizeof(float), span - 1);
            factor = 1.0 / rA[iter];
            for(int i = 0; i < span; i++)
            {
                __bang_mul(rC+i*calc_length,rA,rB+i*calc_length,iter);
                temp_b = 0;
                for(int j = 0; j < iter; j++)
                {
                    temp_b += rC[i*calc_length+j];
                }
                temp_b = rB[i*calc_length+iter] - temp_b;
                rB[i*calc_length+iter] = temp_b * factor;
            }

            __sync();
        }

        __memcpy_async(rA,sA,(m)*sizeof(float),NRAM2NRAM);
        __memcpy_async(rB+m-1,rBp,sizeof(float),NRAM2NRAM,calc_length * sizeof(float), sizeof(float), span - 1);
        __sync();
        factor = 1.0 / rA[m-1];
        for(int i = 0; i < span; i++)
        {
            __bang_mul(rC+i*calc_length,rA,rB+i*calc_length,m-1);

            temp_b = 0;
            for(int j = 0; j < m-1; j++)
            {
                temp_b += rC[i*calc_length+j];
            }
            temp_b = rB[i*calc_length+m-1] - temp_b;

            rB[i*calc_length+m-1] = temp_b * factor;
        }
        __sync();


        __memcpy(OFFSET_B_ROW(dB,start,0),rB,calc_length*sizeof(float),NRAM2GDRAM,ldb * sizeof(float), calc_length * sizeof(float), span - 1);
        __sync();

    }

}


__mlu_entry__ void mlu_strsm_rectile_kernel(
    int batch, int stride,
    int m,int n, bool trans,
    float *dA, int32_t lda,
    float *dB, int32_t ldb)
{
    int id = taskId;
    int batch_id = id / 4;
    if(batch_id >= batch)
        return;
    id = id % 4;
    float* orignA = dA;
    float* orignB = dB;
    dA = orignA + batch_id * stride;
    dB = orignB + batch_id * stride;
    

    int span = n / 4;
    int start = id * span;
    if(id == 3)
    {
        span = n - 3 * span;
    }

    bool if_execute = span > 0;
    __mlu_shared__ float sA[8*POTF_NB];
    __nram__ float rB[4*POTF_NB * 8*POTF_NB];
    __nram__ float rC[4*POTF_NB * 8*POTF_NB];
    __nram__ float rBp[4*POTF_NB];
    __nram__ float rA[8*POTF_NB];
    int calc_length = (8 * POTF_NB) > m ? m : (8 * POTF_NB);
    __memset_nram(rB,POTF_NB*calc_length,(float)ZERO);


    float temp_b = 0, factor = 0;
    float sum = 0.0;
    float c = 0.0;
    float t = 0.0;
    sum = 0.0;
    c = 0.0;
    t =0.0;
    temp_b = 0;
    factor = 0;
    

    if(id == 0)
    {
        __memcpy(sA,dA,sizeof(float),LDRAM2SRAM);
    }
    if(if_execute)
        __memcpy(rBp,OFFSET_B_ROW(dB,start,0),sizeof(float),LDRAM2NRAM,sizeof(float), ldb * sizeof(float), span - 1);
    __sync_cluster();

        
    if(trans)
    {
        __memcpy_async(rA,sA,(1)*sizeof(float),SRAM2NRAM);
        if(if_execute) 
        __memcpy_async(rB,rBp,sizeof(float),NRAM2NRAM,calc_length * sizeof(float), sizeof(float), span - 1);
        __sync_cluster();
        if(id == 0)
        {
            __memcpy_async(sA,OFFSET_ROW(dA,1,0),2*sizeof(float),LDRAM2SRAM);
        }
        if(if_execute)
        __memcpy_async(rBp,OFFSET_B_ROW(dB,start,1),sizeof(float),LDRAM2NRAM,sizeof(float), ldb * sizeof(float), span - 1);
        factor = 1.0 / rA[0];
        for(int i = 0; i < span; i++)
        {
            rB[i*calc_length] *=  factor;
        }

        __sync_cluster();
        
        for(int iter = 1; iter < m - 1; iter++)
        {
            __memcpy_async(rA,sA,(iter+1)*sizeof(float),SRAM2NRAM);
            if(if_execute)
            __memcpy_async(rB+iter,rBp,sizeof(float),NRAM2NRAM,calc_length * sizeof(float), sizeof(float), span - 1);
            __sync_cluster();
            if(id == 0)
            {
                __memcpy_async(sA,OFFSET_ROW(dA,iter+1,0),(iter+2)*sizeof(float),LDRAM2SRAM);
            }
            if(if_execute)
            __memcpy_async(rBp,OFFSET_B_ROW(dB,start,iter+1),sizeof(float),LDRAM2NRAM,sizeof(float), ldb * sizeof(float), span - 1);
            factor = 1.0 / rA[iter];
            for(int i = 0; i < span; i++)
            {
                __bang_mul(rC+i*calc_length,rA,rB+i*calc_length,iter);
                temp_b = 0;
                sum = 0.0;
                c = 0.0;
                t = 0.0;

                for(int j = 0; j < iter; j++)
                {
                    temp_b = rC[i*calc_length+j] - c;   
                    t = sum + temp_b;     
                    c = (t - sum) - temp_b;  
                    sum = t;   
                }
                temp_b = sum;
                temp_b = rB[i*calc_length+iter] - temp_b;
                rB[i*calc_length+iter] = temp_b * factor;
            }

            __sync_cluster();
        }

        __memcpy_async(rA,sA,(m)*sizeof(float),SRAM2NRAM);
        if(if_execute)
        __memcpy_async(rB+m-1,rBp,sizeof(float),NRAM2NRAM,calc_length * sizeof(float), sizeof(float), span - 1);
        __sync_cluster();
        factor = 1.0 / rA[m-1];
        for(int i = 0; i < span; i++)
        {
            __bang_mul(rC+i*calc_length,rA,rB+i*calc_length,m-1);

            sum = 0.0;
            c = 0.0;
            t = 0.0;
            temp_b = 0;
            for(int j = 0; j < m-1; j++)
            {
                temp_b = rC[i*calc_length+j] - c;    
                t = sum + temp_b;      
                c = (t - sum) - temp_b;   
                sum = t;   
            }
            temp_b = sum;
            temp_b = rB[i*calc_length+m-1] - temp_b;
            rB[i*calc_length+m-1] = temp_b * factor;
        }
        __sync_cluster();


        if(if_execute)
        {
            __memcpy(OFFSET_B_ROW(dB,start,0),rB,calc_length*sizeof(float),NRAM2LDRAM,ldb * sizeof(float), calc_length * sizeof(float), span - 1);
        }
        __sync_cluster();

    }

}

mluOpStatus_t strsm_rectile(int batch, int stride, bool upper, bool trans, int m, int n, float *d_a, int lda, float *d_b, int lddb, cnrtQueue_t queue)
{
    cnrtDim3_t dim;
    
    cnrtFunctionType_t func_type = CNRT_FUNC_TYPE_BLOCK;

    dim.y = 1;
    dim.z = 1;

    if(batch>16)
    {
        dim.x = batch;
        KERNEL_CHECK(
            mlu_strsm_rectile_batch_kernel<<<dim, func_type, queue>>>(batch,stride,m,n,trans,d_a,lda,d_b,lddb));
    }
    else
    {
        int carry_batch = batch;
        if(batch == 1)
        {
            func_type = CNRT_FUNC_TYPE_UNION1;
        }
        else if(batch == 2)
        {
            func_type = CNRT_FUNC_TYPE_UNION2;
        }
        else if(batch <= 4)
        {
            func_type = CNRT_FUNC_TYPE_UNION4;
            carry_batch = 4;
        }
        else 
        {
            func_type = CNRT_FUNC_TYPE_UNION8;
            carry_batch = batch < 8 ? 8 : batch;
        }
        dim.x = carry_batch * 4;

        if(!upper && trans)
        {
            KERNEL_CHECK(
                mlu_strsm_rectile_kernel<<<dim, func_type, queue>>>(batch,stride,m,n,trans,d_a,lda,d_b,lddb));
        }
    }

    
    return MLUOP_STATUS_SUCCESS;
}

__mlu_global__
void add_c_batch(int batch, int stride, float beta, float *d_c, float* src,int ldc, int ldsrc, int m, int n)
{
    
    int id = taskId;
    int batch_id = id;
    if(batch_id >= batch)
        return;
    float* orignC = d_c;
    float* orignSrc = src;
    d_c = orignC + batch_id * stride;
    src = orignSrc + batch_id * m*n;
    

    if (beta == 0.0f)
    {

            
        __memcpy(d_c,src,n*sizeof(float),GDRAM2GDRAM,ldc*sizeof(float),ldsrc*sizeof(float),m-1);
        return;
    }

    float* a_sram = (float*)nram_buffer + m * n;

    __memcpy(nram_buffer,d_c,n*sizeof(float),LDRAM2NRAM,n*sizeof(float),ldc*sizeof(float),m-1);
    __memcpy(a_sram,src,n*m*sizeof(float),LDRAM2NRAM);

  __sync();


  int32_t data_num = m*n;
  const float *a_offset = a_sram;
  const float *b_offset = (float*)nram_buffer;

  float *a_nram = (float *)a_offset;
  float *b_nram = (float *)b_offset;

  __bang_add(b_nram, a_nram, b_nram, data_num);

  __memcpy(d_c,b_nram,n*sizeof(float),NRAM2LDRAM,ldc*sizeof(float),n*sizeof(float),m-1);


  __sync();

}

__mlu_global__
void add_c(int batch, int stride, float beta, float *d_c, float* src,int ldc, int ldsrc, int m, int n)
{
    
    int id = taskId;
    int ipu_per_cluster = 4;
    int batch_id = id / ipu_per_cluster;
    if(batch_id >= batch)
        return;
    id = taskId % ipu_per_cluster;
    float* orignC = d_c;
    float* orignSrc = src;
    d_c = orignC + batch_id * stride;
    src = orignSrc + batch_id * m*n;
    


    __mlu_shared__ uint8_t sram_buffer[MAX_SRAM_SIZE];
    if (beta == 0.0f)
    {
        if(id == 0)
        {
            __memcpy(sram_buffer,src,n*sizeof(float),GDRAM2SRAM,n*sizeof(float),ldsrc*sizeof(float),m-1);
            
        }
        __sync_cluster();
        if(id == 0)
        {
            __memcpy(d_c,sram_buffer,n*sizeof(float),SRAM2LDRAM,ldc*sizeof(float),n*sizeof(float),m-1);
        }
        __sync_cluster();
        return;
    }

    float* a_sram = (float*)sram_buffer + 3* m * n;

    if (id == 0) {
      __memcpy(sram_buffer,d_c,n*sizeof(float),GDRAM2SRAM,n*sizeof(float),ldc*sizeof(float),m-1);
      __memcpy(a_sram,src,n*m*sizeof(float),GDRAM2SRAM);
    }

  __sync_cluster();


  int32_t data_num = m*n;
  int32_t data_per_core = data_num / ipu_per_cluster;
  int32_t data_last_core = data_per_core + data_num % ipu_per_cluster;
  const float *a_offset = a_sram + id * data_per_core;
  const float *b_offset = (float*)sram_buffer + id * data_per_core;
  float *output_offset = (float*)sram_buffer + id * data_per_core;

  if (id == ipu_per_cluster - 1) {
    data_per_core = data_last_core;
  }

  int32_t align_num = NFU_ALIGN_SIZE / sizeof(float);

  int32_t data_nram_num =
    MAX_NRAM_SIZE / sizeof(float) / 2 / align_num * align_num;
  float *a_nram = (float *)nram_buffer;
  float *b_nram = (float *)a_nram + data_nram_num;
  int32_t loop_num = data_per_core / data_nram_num;
  int32_t rem_nram_num = data_per_core % data_nram_num;

  for (int32_t i = 0; i < loop_num; i++) {
    __memcpy(a_nram, a_offset + i * data_nram_num,
             data_nram_num * sizeof(float), SRAM2NRAM);
    __memcpy(b_nram, b_offset + i * data_nram_num,
             data_nram_num * sizeof(float), SRAM2NRAM);
    __bang_add(a_nram, a_nram, b_nram, data_nram_num);
    __memcpy(output_offset + i * data_nram_num, a_nram,
             data_nram_num * sizeof(float), NRAM2SRAM);
  }
  if (rem_nram_num != 0) {
    int32_t rem_align_num =
      (rem_nram_num + align_num - 1) / align_num * align_num;
    __memcpy(a_nram, a_offset + loop_num * data_nram_num,
             rem_nram_num * sizeof(float), SRAM2NRAM);
    __memcpy(b_nram, b_offset + loop_num * data_nram_num,
             rem_nram_num * sizeof(float), SRAM2NRAM);
    __bang_add(a_nram, a_nram, b_nram, rem_align_num);
    __memcpy(output_offset + loop_num * data_nram_num, a_nram,
             rem_nram_num * sizeof(float), NRAM2SRAM);
  }
  __sync_cluster();

  if (id == 0) {
      __memcpy(d_c,sram_buffer,n*sizeof(float),SRAM2GDRAM,ldc*sizeof(float),n*sizeof(float),m-1);

  }

  __sync_cluster();

}


mluOpStatus_t sgemm(int batch, bool trans_a, bool trans_b, int m, int n, int k, float alpha, float beta, float* d_a,int lda, int stride_a,  float* d_b, int ldb, int stride_b, float* d_c, int ldc, int stride_c, mluOpHandle_t handle)
{
    if(k==0)
        return MLUOP_STATUS_SUCCESS;

    int32_t batch_size_arr[1] = {batch};
    int64_t stride_a_arr[1] = {stride_a};
    int64_t stride_b_arr[1] = {stride_b};
    int64_t stride_c_arr[1] = {stride_c};

    std::string api_name = "Cholesky";
   

    cnrtQueue_t queue;
    mluOpGetQueue(handle,&queue);

    cnnlStrideBatchMatMulAlgo_t algo;
    CALL_CNNL(cnnlStrideBatchMatMulAlgoCreate(&algo));

    cnnlStrideBatchMatMulHeuristicResult_t heuristic_result;
    CALL_CNNL(cnnlCreateStrideBatchMatMulHeuristicResult(&heuristic_result));

    

    cnnlStrideBatchMatMulDescriptor_t stride_bmm_desc;
    CALL_CNNL(cnnlStrideBatchMatMulDescCreate(&stride_bmm_desc));
    int32_t allow_tf32 = 0, max_batch_dim = 1;
    CALL_CNNL(cnnlSetStrideBatchMatMulDescAttr(stride_bmm_desc, CNNL_STRIDE_BMM_ALLOW_TF32,
                                                &(allow_tf32), sizeof(int32_t)));
    CALL_CNNL(cnnlSetStrideBatchMatMulDescAttr(stride_bmm_desc, CNNL_STRIDE_BMM_MAX_BATCH_DIM,
                                                &(max_batch_dim), sizeof(int32_t)));

    
    mluOpTensorDescriptor_t matmul_a_desc, matmul_b_desc, matmul_c_desc;

    

    CHECK_RETURN(api_name, mluOpCreateTensorDescriptor(&matmul_a_desc));
    CHECK_RETURN(api_name, mluOpCreateTensorDescriptor(&matmul_b_desc));;
    CHECK_RETURN(api_name, mluOpCreateTensorDescriptor(&matmul_c_desc));

    int32_t matmul_a_shape[2] = {batch, stride_a};
    int32_t matmul_b_shape[2] = {batch, stride_b};
    int32_t matmul_c_shape[2] = {batch, stride_c};

    CHECK_RETURN(api_name, mluOpSetTensorDescriptor(
                               matmul_a_desc, MLUOP_LAYOUT_ARRAY,
                               MLUOP_DTYPE_FLOAT, 2, matmul_a_shape));
    CHECK_RETURN(api_name, mluOpSetTensorDescriptor(
                               matmul_b_desc, MLUOP_LAYOUT_ARRAY,
                               MLUOP_DTYPE_FLOAT, 2, matmul_b_shape));
    CHECK_RETURN(api_name, mluOpSetTensorDescriptor(
                               matmul_c_desc, MLUOP_LAYOUT_ARRAY,
                               MLUOP_DTYPE_FLOAT, 2, matmul_c_shape));

    

    DEFINE_CREATE_AND_SET_CNNL_HANDLE(handle, cnnl_handle);
    DEFINE_CREATE_AND_SET_CNNL_TENSOR_DESCRIPTOR(matmul_a_desc, cnnl_a_desc);
    DEFINE_CREATE_AND_SET_CNNL_TENSOR_DESCRIPTOR(matmul_b_desc, cnnl_b_desc);
    DEFINE_CREATE_AND_SET_CNNL_TENSOR_DESCRIPTOR(matmul_c_desc, cnnl_c_desc);
    DEFINE_CREATE_AND_SET_CNNL_TENSOR_DESCRIPTOR(matmul_c_desc, cnnl_d_desc);


    int requested_algo_count = 1, return_algo_count = 0;
    float *workspace;
    size_t workspace_size;


    cnnlGetStrideBatchMatMulAlgoHeuristic(
        cnnl_handle, stride_bmm_desc, cnnl_a_desc, cnnl_b_desc, cnnl_c_desc, cnnl_d_desc, trans_a, trans_b, false,
        &(alpha), &(beta), m, n, k, lda, ldb, ldc, batch_size_arr, stride_a_arr, stride_b_arr,
        stride_c_arr, nullptr, requested_algo_count, &heuristic_result, &return_algo_count);
    
    cnnlGetStrideBatchMatMulHeuristicResult(heuristic_result, &algo, &workspace_size);


    if(workspace_size > 0)
    {
        CNRT_CHECK(cnrtMalloc((void **)&workspace, workspace_size));
    }
    else
    {
        CNRT_CHECK(cnrtMalloc((void **)&workspace, m*n*sizeof(float)));
    }

    
    


    CALL_CNNL(cnnlStrideBatchMatMul_v2(
    cnnl_handle, stride_bmm_desc, algo, trans_a, trans_b, false, m, n, k, batch_size_arr, &(alpha), 
    cnnl_a_desc, d_a, lda, stride_a_arr,
    cnnl_b_desc, d_b, ldb, stride_b_arr, &(beta), cnnl_c_desc, d_c, ldc,
    stride_c_arr, cnnl_d_desc, d_c, workspace, workspace_size)); 

    return MLUOP_STATUS_SUCCESS;
}

__mlu_global__
void batch_inverse_kernel(int batch, float *d_input, int ld_input, int stride_input, float* d_output, int ld_output, int stride_output, int m)
{
    int id = taskId;
    int batch_id = id;
    if(batch_id >= batch)
        return;

    float* orign_input = d_input;
    float* orign_output = d_output;
    d_input = orign_input + batch_id * stride_input;
    d_output = orign_output + batch_id * stride_output;


    float* nram_offset = (float*)nram_buffer;
    float* nram_src0 = nram_offset;
    float* nram_src1 = nram_src0 + m * m;
    float* nram_src2 = nram_src1 + m * m;
    float* mul_result = nram_src2 + m;
    float* nram_dst = nram_src2 + m * m;
    float* diag_start = nram_dst;
    int height = m, span = m;

    __memset_nram(nram_offset, 4 * m * m, (float)ZERO);

    __memcpy(nram_dst,d_input,m*sizeof(float),GDRAM2NRAM,m*sizeof(float),ld_input*sizeof(float),m-1);
    
    float result = 0.0;
    for(int i = 0; i < m; i++)
    {
        int off = i * m + i;
        result = nram_dst[off];
        result = 1.0 / result;
        nram_src1[i*height+i] = result;
        nram_dst[i*span + i] = result;
        diag_start[off] = result;
    }

    for(int i = 1; i < height; i++)
    {
        __memcpy(nram_src2,diag_start+i*m,i*sizeof(float),NRAM2NRAM);
        int num = std::min(i, span);
        float diag_element = diag_start[i*m+i];
        for(int j = 0; j < num; j++)
        {
            float temp = 0.0;
            __bang_mul(mul_result,nram_src2,nram_src1+j*height,i);
            for(int k = 0; k< i; k++)
            {
                temp += mul_result[k];
            }
            temp = temp * -1.0 * diag_element;
            nram_dst[i*span+j] = temp;
            nram_src1[j*height+i] = temp;
        } 
        __sync();
        
    }

    __memcpy(d_output,nram_dst,m*sizeof(float),NRAM2GDRAM,ld_output*sizeof(float), m*sizeof(float),m-1);
    

}

__mlu_global__
void inverse_kernel(int batch, float *d_input, int ld_input, int stride_input, float* d_output, int ld_output, int stride_output, int m)
{
    int id = taskId;
    int batch_id = id / 4;
    if(batch_id >= batch)
        return;
    id = taskId % 4;
    float* orignInput = d_input;
    float* orignOutput = d_output;
    d_input = orignInput + batch_id * stride_input;
    d_output = orignOutput + batch_id * stride_output;
    __mlu_shared__ uint8_t sram_buffer[MAX_SRAM_SIZE];
    

    if (id == 0) {
      __memcpy(sram_buffer,d_input,m*sizeof(float),GDRAM2SRAM,m*sizeof(float),ld_input*sizeof(float),m-1);
    }
    __sync_cluster();

    
    int span = m/taskDim;
    int start = id * span;
    if (id == 3)
    {
        span = m - 3 * span;
    }
    float* nram_offset = (float*)nram_buffer + id * 3 * m * m;
    float* nram_src1 = nram_offset;
    float* nram_src2 = nram_src1 + m * m;
    float* mul_result = nram_src2 + m;
    float* nram_dst = nram_src2 + m * m;
    float* diag_start = ((float*)sram_buffer) + m * start + start;
    int height = m - start;

    __memset_nram(nram_offset, 3 * m * m, (float)ZERO);

    float result = 0.0;
    for(int i = 0; i < span; i++)
    {
        int off = i * m + i;
        result = diag_start[off];
        result = 1.0 / result;
        nram_src1[i*height+i] = result;
        nram_dst[i*span + i] = result;
        diag_start[off] = result;

    }
    __sync_cluster();


    for(int i = 1; i < height; i++)
    {
        __memcpy(nram_src2,diag_start+i*m,i*sizeof(float),SRAM2NRAM);
        int num = std::min(i, span);
        float diag_element = diag_start[i*m+i];
        for(int j = 0; j < num; j++)
        {
            float temp = 0.0;

            __bang_mul(mul_result,nram_src2,nram_src1+j*height,i);
            for(int k = 0; k< i; k++)
            {
                temp += mul_result[k];
            }
            temp = temp * -1.0 * diag_element;
            nram_dst[i*span+j] = temp;
            nram_src1[j*height+i] = temp;
        } 
        __sync();
        
    }

    __sync_cluster();

    if (span > 0)
        __memcpy(diag_start,nram_dst,span*sizeof(float),NRAM2SRAM,m*sizeof(float),span*sizeof(float),height-1);

    __sync_cluster();

    if (id == 0) {
      __memcpy(d_output,sram_buffer,m*sizeof(float),SRAM2GDRAM,ld_output*sizeof(float), m*sizeof(float),m-1);
    }
    
    

}

__mlu_global__ void set_zero(int batch, int stride, bool upper, int m, float* d_c, int lddc)
{
    int id = taskId;
    int batch_id = id / 4;
    if(batch_id >= batch)
        return;
    float* orignC = d_c;
    d_c = orignC + batch_id * stride;
    id = taskId % 4;
    int span = m/4;
    int pre = id * span;
    float* start_c = d_c + pre * lddc + pre;
    float* temp_c = start_c;
    if (id == 3)
    {
        span = m - 3 * span;
    
    }
    for(int i = 0; i  < span - 1; i++)
    {
        temp_c = start_c + i * lddc + i;
        int num = m - pre - i;
        __ldramset(temp_c+1, num - 1, 0);
    }
    if (id != 3&&span > 0)
    {
        temp_c = start_c + (span - 1) * lddc + span - 1;
        int num = m - pre - span + 1;
        __ldramset(temp_c+1, num - 1, 0);
    
    }
}

mluOpStatus_t strsm(int batch, int stride, bool upper, bool trans, int m, int n, float* d_a, int lda, float* d_b, int ldb, mluOpHandle_t handle)
{
    if(n==0)
        return MLUOP_STATUS_SUCCESS;
    mluOpTensorDescriptor_t matmul_a_desc, matmul_b_desc, info_desc;
    std::string api_name = "Cholesky";

    cnrtQueue_t queue;
    mluOpGetQueue(handle,&queue);

    int32_t *info;
    CNRT_CHECK(cnrtMalloc((void **)&info, batch*sizeof(int32_t)));

    CHECK_RETURN(api_name, mluOpCreateTensorDescriptor(&matmul_a_desc));
    CHECK_RETURN(api_name, mluOpCreateTensorDescriptor(&matmul_b_desc));
    CHECK_RETURN(api_name, mluOpCreateTensorDescriptor(&info_desc));
    int32_t matmul_a_shape[2] = {batch, m*m};
    int32_t matmul_b_shape[2] = {batch, stride};
    int32_t info_shape[1] = {1};

    CHECK_RETURN(api_name, mluOpSetTensorDescriptor(
                               matmul_a_desc, MLUOP_LAYOUT_ARRAY,
                               MLUOP_DTYPE_FLOAT, 2, matmul_a_shape));
    CHECK_RETURN(api_name, mluOpSetTensorDescriptor(
                               matmul_b_desc, MLUOP_LAYOUT_ARRAY,
                               MLUOP_DTYPE_FLOAT, 2, matmul_b_shape));
    CHECK_RETURN(api_name, mluOpSetTensorDescriptor(
                               info_desc, MLUOP_LAYOUT_ARRAY,
                               MLUOP_DTYPE_INT32, 1, info_shape));

    DEFINE_CREATE_AND_SET_CNNL_HANDLE(handle, cnnl_handle);
    DEFINE_CREATE_AND_SET_CNNL_TENSOR_DESCRIPTOR(matmul_a_desc, cnnl_a_desc);
    DEFINE_CREATE_AND_SET_CNNL_TENSOR_DESCRIPTOR(matmul_b_desc, cnnl_b_desc);
    DEFINE_CREATE_AND_SET_CNNL_TENSOR_DESCRIPTOR(info_desc, cnnl_info_desc);

    float* workspace;
    CNRT_CHECK(cnrtMalloc((void **)&workspace, batch*m*m*sizeof(float)));
    CNRT_CHECK(cnrtMemset(workspace, 0.0, batch*m*m*sizeof(float)));

    float* h_i;
    h_i = (float*)malloc(m*m*sizeof(float));
    

    int m1 = m/2;
    int m2 = m - m1;

    float* workspace1 = workspace;
    float* workspace2 = workspace1 + m1*m+m1;

    cnrtDim3_t dim;
    dim.y = 1;
    dim.z = 1;
    cnrtFunctionType_t func_type = CNRT_FUNC_TYPE_BLOCK;
    if(batch > 1)
    {
        dim.x = batch;
        KERNEL_CHECK(batch_inverse_kernel<<<dim, func_type, queue>>>(batch, d_a,lda,stride, workspace1,m,m*m,m1));
        KERNEL_CHECK(batch_inverse_kernel<<<dim, func_type, queue>>>(batch, d_a+m1*lda+m1,lda,stride, workspace2,m,m*m,m2));
    }
    else
    {
        int carry_batch = batch;
        if(batch == 1)
        {
            func_type = CNRT_FUNC_TYPE_UNION1;
        }
        else if(batch == 2)
        {
            func_type = CNRT_FUNC_TYPE_UNION2;
        }
        else if(batch <= 4)
        {
            func_type = CNRT_FUNC_TYPE_UNION4;
            carry_batch = 4;
        }
        else 
        {
            func_type = CNRT_FUNC_TYPE_UNION8;
            carry_batch = batch < 8 ? 8 : batch;
        }
        dim.x = carry_batch * 4;


        KERNEL_CHECK(inverse_kernel<<<dim, func_type, queue>>>(batch, d_a,lda,stride, workspace1,m,m*m,m1));
        KERNEL_CHECK(inverse_kernel<<<dim, func_type, queue>>>(batch, d_a+m1*lda+m1,lda,stride, workspace2,m,m*m,m2));
   
    }
    sgemm(batch, false,false,m2,m1,m1,1.0f,0.0f,d_a+m1*lda,lda,stride,workspace1,m,m*m,workspace1+m1*m,m,m*m,handle);
    sgemm(batch, false,false,m2,m2,m1,-1.0f,0.0f,workspace2,m,m*m,workspace1+m1*m,m,m*m,workspace1+m1*m,m,m*m,handle);
    cnrtQueueSync(queue);
    cnnlStrideBatchMatMul(cnnl_handle, false, true, n,m, m, batch, 1.0, cnnl_b_desc, d_b, ldb, stride, cnnl_a_desc, workspace, m, m*m, 0.0f, cnnl_b_desc, d_b, ldb, stride);

    return MLUOP_STATUS_SUCCESS;
}

mluOpStatus_t set_half_zero(int batch,int stride,float* d_a, int lda, int m, mluOpHandle_t handle)
{
    cnrtQueue_t queue;
    mluOpGetQueue(handle,&queue);
    cnrtDim3_t dim;
    cnrtFunctionType_t func_type = CNRT_FUNC_TYPE_UNION1;
    dim.x = 4 * batch;
    dim.y = 1;
    dim.z = 1;
    KERNEL_CHECK(set_zero<<<dim, func_type, queue>>>(batch, stride, false, m, d_a,lda));
    return MLUOP_STATUS_SUCCESS;
}


mluOpStatus_t ssyrk(int batch, int stride, bool upper, bool trans,int n, int k, float* d_a, int ldda, float* d_c, int lddc, mluOpHandle_t handle)
{
    if(k==0)
        return MLUOP_STATUS_SUCCESS;

    sgemm(batch, false,true,n,n,k,-1.0f,1.0f,d_a,ldda,stride,d_a,ldda,stride,d_c,lddc,stride,handle);
    cnrtQueue_t queue;
    mluOpGetQueue(handle,&queue);
    cnrtDim3_t dim;
    cnrtFunctionType_t func_type = CNRT_FUNC_TYPE_UNION1;
    int carry_batch = batch;
    if(batch == 1)
    {
        func_type = CNRT_FUNC_TYPE_UNION1;
    }
    else if(batch == 2)
    {
        func_type = CNRT_FUNC_TYPE_UNION2;
    }
    else if(batch <= 4)
    {
        func_type = CNRT_FUNC_TYPE_UNION4;
        carry_batch = 4;
    }
    else 
    {
        func_type = CNRT_FUNC_TYPE_UNION8;
        carry_batch = batch < 8 ? 8 : batch;
    }
    dim.x = carry_batch * 4;
    dim.y = 1;
    dim.z = 1;
    KERNEL_CHECK(set_zero<<<dim, func_type, queue>>>(batch, stride, upper, n, d_c,lddc));
    
    
    return MLUOP_STATUS_SUCCESS;
}

mluOpStatus_t mlu_spotrf_rectile(int batch, int stride, bool trans, bool uplo, int n, int recnb, float* d_A, int lda, int gbstep, mluOpHandle_t handle)
{
    cnrtQueue_t queue;
    mluOpGetQueue(handle,&queue);
    if(n==0)
        return MLUOP_STATUS_SUCCESS;
    
    if(n <=recnb)
    {
        mlu_spotf2_lpin(batch, stride, trans, uplo,n,lda,d_A,gbstep,queue);
    }
    else
    {
        int n1 = n/2;
        int n2 = n-n1;
        mlu_spotrf_rectile(batch,stride,trans,uplo,n1,recnb,OFFSET_ROW(d_A,0,0),lda,gbstep, handle);
        strsm_rectile(batch, stride, uplo,trans,n1,n2,OFFSET_ROW(d_A,0,0),lda,OFFSET_ROW(d_A,n1,0),lda,queue);
        ssyrk(batch,stride,uplo,trans,n2,n1,d_A+n1*lda,lda,OFFSET_ROW(d_A,n1,n1),lda,handle);
        mlu_spotrf_rectile(batch,stride,trans,uplo,n2,recnb,OFFSET_ROW(d_A,n1,n1),lda,gbstep+n1,handle);

        
    return MLUOP_STATUS_SUCCESS;
}

// m * n
mluOpStatus_t transpose(int batch, int m, int n, float* d_input,float* d_output, mluOpHandle_t handle,mluOpDataType_t type, float* workspace)
{
    if(m==0)
        return MLUOP_STATUS_SUCCESS;
    cnrtQueue_t queue;
    mluOpGetQueue(handle,&queue);

    mluOpTensorDescriptor_t trans_input_desc, trans_output_desc;
    std::string api_name = "Cholesky";
    const int input_dim = 3;

    CHECK_RETURN(api_name, mluOpCreateTensorDescriptor(&trans_input_desc));
    CHECK_RETURN(api_name, mluOpCreateTensorDescriptor(&trans_output_desc));

    int32_t transpose_input_shape[3] = {batch, m, n};
    int32_t transpose_output_shape[3] = {batch, n, m};

    CHECK_RETURN(api_name, mluOpSetTensorDescriptor(
                               trans_input_desc, MLUOP_LAYOUT_ARRAY,
                               type, 3, transpose_input_shape));

    CHECK_RETURN(api_name, mluOpSetTensorDescriptor(
                               trans_output_desc, MLUOP_LAYOUT_ARRAY,
                               type, 3, transpose_output_shape));

    int permute[3] = {0, 2, 1};

    DEFINE_CREATE_AND_SET_CNNL_HANDLE(handle, cnnl_handle);
    DEFINE_CREATE_AND_SET_CNNL_TENSOR_DESCRIPTOR(trans_input_desc, cnnl_in_desc);
    DEFINE_CREATE_AND_SET_CNNL_TENSOR_DESCRIPTOR(trans_output_desc, cnnl_out_desc);

    cnnlTransposeDescriptor_t cnnl_trans_desc = NULL;

    CALL_CNNL(cnnlCreateTransposeDescriptor(&cnnl_trans_desc));

    CALL_CNNL(cnnlSetTransposeDescriptor(cnnl_trans_desc, input_dim, permute));

    size_t size=0;
    

    CALL_CNNL(cnnlGetTransposeWorkspaceSize(cnnl_handle, cnnl_in_desc, cnnl_trans_desc, &size));


    if(size > 0ul)
    {

        printf("transpose2 need size: %zu\n",size);
    }

    CALL_CNNL(cnnlTranspose_v2(cnnl_handle, cnnl_trans_desc, cnnl_in_desc,
                             d_input, cnnl_out_desc, d_output,
                             workspace, size));
    return MLUOP_STATUS_SUCCESS;

}

