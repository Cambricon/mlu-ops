/*************************************************************************
 * Copyright (C) [2023] by Cambricon, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#include "mutual_information_forward.h"

#include "core/logging.h"
#include "kernels/kernel.h"
#include "kernels/utils/common.h"
#include "kernels/mutual_information/mutual_information_forward/mutual_information_forward_utils.h"

__mlu_func__ bool calPartitionJobScope(
    bool has_boundary, const int64_t *opt_boundary, const int B, const int S,
    const int T, const int step_i, const int job_num_on_step,
    const int s_block_num, const int t_block_num, const int s_block_size,
    const int t_block_size, int &batch_idx, int &batch_s_begin,
    int &batch_t_begin, int &batch_s_end, int &batch_t_end, int &cur_s_begin,
    int &cur_t_begin, int &cur_s_end, int &cur_t_end, int &cur_s_size,
    int &cur_t_size, bool &need_compute_ans, float *ans) {
  int job_num_on_batch = job_num_on_step / B;  // Each batch job num
  batch_idx = taskId / job_num_on_batch;       // Current job on which batch
  int block_id_in_batch =
      taskId - batch_idx * job_num_on_batch;  // Current job id in batch

  // taskDim is not always job num, because of task dim x limit
  if (batch_idx >= B) {
    return true;
  }

  // Compute s and t block id in batch
  int s_block_id, t_block_id;
  s_block_id = __mluop_max(0, step_i - (t_block_num - 1)) + block_id_in_batch;
  t_block_id = __mluop_min(step_i, t_block_num - 1) - block_id_in_batch;

  // Compute current job id scope
  cur_s_begin = s_block_id * s_block_size;
  cur_t_begin = t_block_id * t_block_size;
  cur_s_end = (s_block_id + 1) * s_block_size - 1;
  cur_t_end = (t_block_id + 1) * t_block_size - 1;

  // Deal with boundary and decide current job if need to compute
  if (has_boundary) {
    int64_t *boundary = (int64_t *)nram_buffer;
    __memcpy(boundary, opt_boundary + 4 * batch_idx, 4 * sizeof(int64_t),
             GDRAM2NRAM);
    batch_s_begin = boundary[0];
    batch_t_begin = boundary[1];
    batch_s_end = boundary[2];
    batch_t_end = boundary[3];
    // invalid boundary, first launch step set ans to 0
    if (step_i == 0 &&
        (batch_s_begin > batch_s_end || batch_t_begin > batch_t_end)) {
      ans[batch_idx] = 0;
      return true;
    }
  }

  // Compare current job scope with batch scope, if empty job, return
  if (cur_s_begin > batch_s_end || cur_t_begin > batch_t_end ||
      cur_s_end < batch_s_begin || cur_t_end < batch_t_begin) {
    return true;
  }

  // Reset s and t begin and end to valid boundary
  if (cur_s_begin < batch_s_begin) {
    cur_s_begin = batch_s_begin;
  }
  if (cur_t_begin < batch_t_begin) {
    cur_t_begin = batch_t_begin;
  }
  if (cur_s_end > batch_s_end) {
    cur_s_end = batch_s_end;
  }
  if (cur_t_end > batch_t_end) {
    cur_t_end = batch_t_end;
  }

  cur_s_size = cur_s_end - cur_s_begin + 1;
  cur_t_size = cur_t_end - cur_t_begin + 1;

  // At last compute step, need compute ans
  if (cur_s_end == batch_s_end && cur_t_end == batch_t_end) {
    need_compute_ans = true;
  } else {
    need_compute_ans = false;
  }

  return false;
}

__mlu_func__ void loadInitP(const float *gdram_px, const float *gdram_py,
                            const float *gdram_p, float *nram_px,
                            float *nram_py, float *nram_p, const int S,
                            const int T, const int batch_s_begin,
                            const int batch_t_begin, const int cur_s_begin,
                            const int cur_t_begin, const int cur_s_size,
                            const int cur_t_size) {
  // Compare current s_begin and batch_s_begin to decide load px or write -inf
  if (cur_s_begin > batch_s_begin) {
    // Load px(s-1, t)
    __memcpy_async(
        nram_px, gdram_px + (cur_s_begin - 1) * (T + 1) + cur_t_begin,
        cur_t_size * sizeof(float), GDRAM2NRAM, cur_t_size * sizeof(float),
        (T + 1) * sizeof(float), cur_s_size - 1);
    // Load p(s-1, t), one row
    __memcpy_async(nram_p + 1,
                   gdram_p + (cur_s_begin - 1) * (T + 1) + cur_t_begin,
                   cur_t_size * sizeof(float), GDRAM2NRAM, 0, 0, 0);
  } else {  // cur_s_begin == batch_s_begin, skip first row, and write -inf
    if (cur_s_size > 1) {
      __memcpy_async(
          nram_px + cur_t_size, gdram_px + cur_s_begin * (T + 1) + cur_t_begin,
          cur_t_size * sizeof(float), GDRAM2NRAM, cur_t_size * sizeof(float),
          (T + 1) * sizeof(float), cur_s_size - 2);
    }
    __nramset_async(nram_px, cur_t_size, (float)(-INFINITY), 0, 0);
    // p(s-1, t) first row write -inf
    __nramset_async(nram_p + 1, cur_t_size, (float)(-INFINITY), 0, 0);
  }

  // Compare current t_begin and batch_t_begin to decide load py or write -inf
  if (cur_t_begin > batch_t_begin) {
    // Load py(s, t-1)
    __memcpy_async(nram_py, gdram_py + cur_s_begin * T + cur_t_begin - 1,
                   cur_t_size * sizeof(float), GDRAM2NRAM,
                   cur_t_size * sizeof(float), T * sizeof(float),
                   cur_s_size - 1);
    // Load p(s, t-1)
    __memcpy_async(nram_p + cur_t_size + 1,
                   gdram_p + cur_s_begin * (T + 1) + cur_t_begin - 1,
                   sizeof(float), GDRAM2NRAM, (cur_t_size + 1) * sizeof(float),
                   (T + 1) * sizeof(float), cur_s_size - 1);
  } else {  // cur_t_begin == batch_t_begin, skip first column, and write -inf
    if (cur_t_size > 1) {
      __memcpy_async(nram_py + 1, gdram_py + cur_s_begin * T + cur_t_begin,
                     (cur_t_size - 1) * sizeof(float), GDRAM2NRAM,
                     cur_t_size * sizeof(float), T * sizeof(float),
                     cur_s_size - 1);
    }
    __nramset_async(nram_py, 1, (float)(-INFINITY), cur_t_size * sizeof(float),
                    cur_s_size - 1);
    // p(s, t-1) first column write -inf
    __nramset_async(nram_p + cur_t_size + 1, 1, (float)(-INFINITY),
                    (cur_t_size + 1) * sizeof(float), cur_s_size - 1);
  }

  // sync for memcpy async
  __sync();
}

__mlu_func__ void computePByDiagonal(
    float *nram_px, float *nram_py, float *nram_p, float *nram_cur_px,
    float *nram_cur_py, float *nram_cur_p, float *max_val, float *mask,
    float *temp, const int batch_s_begin, const int batch_t_begin,
    const int cur_s_begin, const int cur_t_begin, const int cur_s_size,
    const int cur_t_size) {
  // Compute P by diagonal
  const int repeat = cur_s_size + cur_t_size - 1;
  const int max_s_t = __mluop_max(cur_s_size, cur_t_size);
  const int min_s_t = __mluop_min(cur_s_size, cur_t_size);

  for (int i = 0; i < repeat; ++i) {
    // Initialize p(batch_s_begin, batch_t_begin) to 0
    if (cur_s_begin == batch_s_begin && cur_t_begin == batch_t_begin &&
        i == 0) {
      nram_p[cur_t_size + 2] = 0.0;
      continue;
    }

    int data_num = i < max_s_t ? __mluop_min(i + 1, min_s_t)
                               : cur_s_size + cur_t_size - i - 1;

    // px, py use same s, t index on nram,
    // different -1 offset of row and column is considered when load
    int first_s = __mluop_max(0, i - (cur_t_size - 1));
    int first_t = __mluop_min(i, cur_t_size - 1);

    // Move p(s-1, t), p(s, t-1), px(s-1, t), py(s, t-1)
    __memcpy(nram_cur_p, nram_p + first_s * (cur_t_size + 1) + first_t + 1,
             sizeof(float), NRAM2NRAM, sizeof(float),
             cur_t_size * sizeof(float), data_num);
    __memcpy(nram_cur_px, nram_px + first_s * cur_t_size + first_t,
             sizeof(float), NRAM2NRAM, sizeof(float),
             (cur_t_size - 1) * sizeof(float), data_num - 1);
    __memcpy(nram_cur_py, nram_py + first_s * cur_t_size + first_t,
             sizeof(float), NRAM2NRAM, sizeof(float),
             (cur_t_size - 1) * sizeof(float), data_num - 1);

    // Compute current p
    __bang_add(nram_cur_px, nram_cur_px, nram_cur_p, data_num);
    __bang_add(nram_cur_py, nram_cur_py, nram_cur_p + 1, data_num);
    logAddVector(nram_cur_p, nram_cur_px, nram_cur_py, max_val, mask, temp,
                 data_num);

    // Move p back
    __memcpy(nram_p + (first_s + 1) * (cur_t_size + 1) + first_t + 1,
             nram_cur_p, sizeof(float), NRAM2NRAM, cur_t_size * sizeof(float),
             sizeof(float), data_num - 1);
  }
}

__mlu_global__ void mluBlockDefaultMutualInformationForward(
    const int B, const int S, const int T, const int step_i,
    const int job_num_on_step, const int s_block_num, const int t_block_num,
    const int s_block_size, const int t_block_size, const float *px,
    const float *py, const bool has_boundary, const int64_t *opt_boundary,
    float *p, float *ans) {
  /************************* NRAM SPACE *******************************/
  /*|----------------------------------------------------------------|*/
  /*| px, py |     p     |max_val,mask,temp|cur_px |cur_py |  cur_p  |*/
  /*| 2*S*T  |(S+1)*(T+1)|   3 * min_len   |min_len|min_len|min_len+1|*/
  /*|----------------------------------------------------------------|*/

  // NOTE: s and t block size has already + 1 on S and T
  int min_s_t_block_size = __mluop_min(s_block_size, t_block_size);
  float *nram_px = (float *)nram_buffer;
  float *nram_py = nram_px + s_block_size * t_block_size;
  float *nram_p = nram_py + s_block_size * t_block_size;
  float *nram_max_val = nram_p + (s_block_size + 1) * (t_block_size + 1);
  float *nram_mask = nram_max_val + min_s_t_block_size;
  float *nram_temp = nram_mask + min_s_t_block_size;
  float *nram_cur_px = nram_temp + min_s_t_block_size;
  float *nram_cur_py = nram_cur_px + min_s_t_block_size;
  float *nram_cur_p = nram_cur_py + min_s_t_block_size;

  int batch_idx;
  int batch_s_begin = 0;
  int batch_t_begin = 0;
  int batch_s_end = S;
  int batch_t_end = T;
  int cur_s_begin, cur_t_begin, cur_s_end, cur_t_end, cur_s_size, cur_t_size;
  bool need_compute_ans;

  // According to has_boundary, calculate current job scope
  bool need_return = calPartitionJobScope(
      has_boundary, opt_boundary, B, S, T, step_i, job_num_on_step, s_block_num,
      t_block_num, s_block_size, t_block_size, batch_idx, batch_s_begin,
      batch_t_begin, batch_s_end, batch_t_end, cur_s_begin, cur_t_begin,
      cur_s_end, cur_t_end, cur_s_size, cur_t_size, need_compute_ans, ans);
  // Because taskDimX could change to taskDimY, so not all jobs need to compute
  if (need_return) {
    return;
  }

  // sync for boundary loadGE on NRAM
  __sync();
  const int px_one_batch_num = S * (T + 1);
  const int py_one_batch_num = (S + 1) * T;
  const int p_one_batch_num = (S + 1) * (T + 1);

  const float *gdram_px = px + batch_idx * px_one_batch_num;
  const float *gdram_py = py + batch_idx * py_one_batch_num;
  float *gdram_p = p + batch_idx * p_one_batch_num;

  // LoadInitP, load px, py, other block p, or write -inf at first row/column
  loadInitP(gdram_px, gdram_py, gdram_p, nram_px, nram_py, nram_p, S, T,
            batch_s_begin, batch_t_begin, cur_s_begin, cur_t_begin, cur_s_size,
            cur_t_size);

  // ComputeP by diagonal
  // p[b,s,t] = log_add(p[b,s-1,t] + px[b,s-1,t], p[b,s,t-1] + py[b,s,t-1])
  computePByDiagonal(nram_px, nram_py, nram_p, nram_cur_px, nram_cur_py,
                     nram_cur_p, nram_max_val, nram_mask, nram_temp,
                     batch_s_begin, batch_t_begin, cur_s_begin, cur_t_begin,
                     cur_s_size, cur_t_size);

  // StoreP
  __memcpy(gdram_p + cur_s_begin * (T + 1) + cur_t_begin,
           nram_p + cur_t_size + 2, cur_t_size * sizeof(float), NRAM2GDRAM,
           (T + 1) * sizeof(float), (cur_t_size + 1) * sizeof(float),
           cur_s_size - 1);

  // If last compute step, need store p[s_end, t_end] to ans
  if (need_compute_ans) {
    ans[batch_idx] = nram_p[cur_s_size * (cur_t_size + 1) + cur_t_size];
  }
}

mluOpStatus_t MLUOP_WIN_API kernelDefaultMutualInformationForward(
    cnrtDim3_t k_dim, cnrtFunctionType_t k_type, cnrtQueue_t queue, const int B,
    const int S, const int T, const int step_i, const int job_num_on_step,
    const int s_block_num, const int t_block_num, const int s_block_size,
    const int t_block_size, const void *px, const void *py,
    const bool has_boundary, const void *opt_boundary, void *p, void *ans) {
  KERNEL_CHECK(
      mluBlockDefaultMutualInformationForward<<<k_dim, k_type, queue>>>(
          B, S, T, step_i, job_num_on_step, s_block_num, t_block_num,
          s_block_size, t_block_size, (float *)px, (float *)py, has_boundary,
          (int64_t *)opt_boundary, (float *)p, (float *)ans));
  return MLUOP_STATUS_SUCCESS;
}
