/*************************************************************************
 * Copyright (C) [2019-2022] by Cambricon, Inc.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/

#include "kernels/poly_nms/pnms.h"

#include <iostream>
#include <string>

#include <cnrt.h>
#include <mlu_op.h>
#include "core/context.h"
#include "core/logging.h"
#include "core/runtime/device.h"
#include "core/tensor.h"
#include "core/type.h"
#include "core/tool.h"

int selectUnionType(uint32_t use_job, int box_num_per_core) {
  const int min_box_per_core = 256;
  const int core_num_per_cluster = 4;
  // the box_num_per_core should be at least 256, otherwise the real IO
  // bandwidth would be very low
  while (box_num_per_core < min_box_per_core &&
         use_job >= core_num_per_cluster) {
    box_num_per_core *= 2;
    use_job /= 2;
  }
  return use_job;
}

static mluOpStatus_t policyFunc(mluOpHandle_t handle, cnrtDim3_t *k_dim,
                                cnrtFunctionType_t *k_type,
                                const int input_boxes_num) {
  uint32_t job_limit = mluop::runtime::getJobLimitCapability(handle);
  uint32_t core_dim = handle->core_num_per_cluster;
  uint32_t core_number = job_limit;
  int box_num_per_core = (input_boxes_num + core_number - 1) / core_number;

  // initiate k_type as Union1
  k_dim->x = core_dim;
  k_dim->y = 1;
  k_dim->z = 1;
  *k_type = CNRT_FUNC_TYPE_UNION1;
  switch (job_limit) {
    case CN_KERNEL_CLASS_BLOCK:
    case CN_KERNEL_CLASS_UNION:
    case CN_KERNEL_CLASS_UNION2:
    case CN_KERNEL_CLASS_UNION4:
    case CN_KERNEL_CLASS_UNION8:
    case CN_KERNEL_CLASS_UNION16: {
      int use_job = selectUnionType(job_limit, box_num_per_core);
      if (use_job < 4) {
        k_dim->x = 1;
        *k_type = CNRT_FUNC_TYPE_BLOCK;
      } else {
        k_dim->x = core_dim;
        *k_type = CNRT_FUNC_TYPE_UNION1;
      }
    }; break;
    default:
      LOG(WARNING) << "[cnnlPolyNms]: got unsupported job limit number."
                   << " Use default CN_KERNEL_CLASS_UNION1 with UNION1 task.";
  }
  VLOG(5) << "k_dim->x: " << k_dim->x;
  VLOG(5) << "k_type: " << *k_type;
  return MLUOP_STATUS_SUCCESS;
}

mluOpStatus_t MLUOP_WIN_API mluOpPolyNms(mluOpHandle_t handle,
                                        const mluOpTensorDescriptor_t boxes_desc,
                                        const void *boxes, float iou_threshold,
                                        void *workspace, size_t workspace_size,
                                        const mluOpTensorDescriptor_t output_desc,
                                        void *output, void *result_num) {
  const std::string API = "[cnnlPolyNms]";
  // check inputs/outputs
  PARAM_CHECK(API, handle != NULL);
  PARAM_CHECK(API, boxes_desc != NULL);
  PARAM_CHECK(API, output_desc != NULL);
  PARAM_CHECK(API, result_num != NULL);

  PARAM_CHECK(API, boxes_desc->dtype == MLUOP_DTYPE_FLOAT);
  PARAM_CHECK(API, boxes_desc->layout == MLUOP_LAYOUT_ARRAY);

  PARAM_CHECK_EQ(API, boxes_desc->dim, 2);
  PARAM_CHECK_EQ(API, boxes_desc->dims[1], 9);
  PARAM_CHECK(API, boxes_desc->dims[0] >= output_desc->dims[0]);

  uint32_t input_boxes_num = boxes_desc->dims[0];
  uint32_t input_stride = boxes_desc->dims[1];

  if (input_boxes_num == 0) {
    VLOG(5) << API << " skip zero element tensor.";
    return MLUOP_STATUS_SUCCESS;
  }

  PARAM_CHECK(API, boxes != NULL);
  if(workspace_size>0){
    PARAM_CHECK(API, workspace != NULL);
  }

  mluOpDataType_t data_type_input = boxes_desc->dtype;
  cnrtDim3_t k_dim;
  cnrtJobType_t k_type;
  PARAM_CHECK(API, MLUOP_STATUS_SUCCESS ==
                       policyFunc(handle, &k_dim, &k_type, input_boxes_num));

  VLOG(5) << "Launch Kernel MLUUnion1OrBlockPNMS<<<k_dim: " << k_type << ", "
          << k_dim.x << ", " << k_dim.y << ", " << k_dim.z << ">>>";
  KERNEL_CHECK((MLUUnion1OrBlockPNMS<<<k_dim, k_type, handle->queue>>>(
      (void *)boxes, input_boxes_num, input_stride, iou_threshold, result_num,
      output, data_type_input, workspace)));
  return MLUOP_STATUS_SUCCESS;
}

mluOpStatus_t MLUOP_WIN_API mluOpGetPolyNmsWorkspaceSize(
    mluOpHandle_t handle, const mluOpTensorDescriptor_t boxes_desc,
    size_t *size) {
  const std::string API = "[mluOpGetPolyNmsWorkspaceSize]";
  PARAM_CHECK(API, handle != NULL);
  PARAM_CHECK(API, boxes_desc != NULL);
  PARAM_CHECK("[mluOpGetPolyNmsWorkspaceSize]", size != NULL);

  PARAM_CHECK(API, boxes_desc->dim == 2);
  PARAM_CHECK(API, boxes_desc->dims[1] == 9);

  PARAM_CHECK(API, boxes_desc->dtype == MLUOP_DTYPE_FLOAT);
  PARAM_CHECK(API, boxes_desc->layout == MLUOP_LAYOUT_ARRAY);
// workspace stores the transposed input data[9, N].
  uint32_t input_boxes_num = boxes_desc->dims[0];  // N
  uint32_t input_stride = boxes_desc->dims[1];     // 9
#if __BANG_ARCH__ >= 300
  *size = input_boxes_num * input_stride * sizeof(float);
#else
  int align_num = 128 / sizeof(float);
  int align_box_num = CEIL_ALIGN(input_boxes_num, align_num);
  int align_stride = CEIL_ALIGN(input_stride, align_num);
  *size = align_box_num * align_stride * sizeof(float);
#endif

  VLOG(5) << "[mluOpGetPolyNmsWorkspaceSize] size = :" << *size;
  return MLUOP_STATUS_SUCCESS;
}
