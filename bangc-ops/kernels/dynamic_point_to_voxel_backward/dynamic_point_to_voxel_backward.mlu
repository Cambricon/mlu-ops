/*************************************************************************
 * Copyright (C) [2023] by Cambricon, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#include "dynamic_point_to_voxel_backward.h"

#include "kernels/kernel.h"
#include "kernels/debug.h"
#include "kernels/utils/common.h"

__nram__ char nram_buffer[MAX_NRAM_SIZE];

template <typename T>
__mlu_func__ void loadAsync(T *feats_nram, T *voxel_feats_nram,
                            int *index_mask_nram, int *voxel_from_nram,
                            int *point2voxel_map_real_nram,
                            const int *point2voxel_map_nram,
                            const int *index_col_nram, const T *feats,
                            const T *voxel_feats, const int *voxel_from, int &x,
                            int &n_real, const int n_limit, const int N,
                            const int C) {
  int invalid_index = -1;
  int size_feats = C * sizeof(T);
  int size_feats_idx = C * sizeof(int);
  n_real = 0;
  for (; x < N && n_real < n_limit; x++) {
    int point_to = point2voxel_map_nram[x];
    int input_offset = x * C;
    int input_real_offset = n_real * C;
    if (taskId == point_to % taskDim) {
      if (point_to == invalid_index) {
        continue;
      }
      int reduced_offset = point_to * C;
      // load valid data to feats_nram
      __memcpy_async(feats_nram + input_real_offset, feats + input_offset,
                     size_feats, GDRAM2NRAM);
      // boradcast voxel_feats data to voxel_feats_nram via the same "point_to"
      __memcpy_async(voxel_feats_nram + input_real_offset,
                     voxel_feats + reduced_offset, size_feats, GDRAM2NRAM);
      // boradcast voxel_from data to voxel_from_nram via the same "point_to"
      __memcpy_async(voxel_from_nram + input_real_offset,
                     voxel_from + reduced_offset, size_feats_idx, GDRAM2NRAM);
      // record valid index of x in index_mask_nram
      __bang_write_value(index_mask_nram + input_real_offset, C, x * C);
      // point2voxel_map removed invalid data
      point2voxel_map_real_nram[n_real] = point_to;
      ++n_real;
    }
  }
  if (n_real > 0) {
    __bang_cycle_add(index_mask_nram, index_mask_nram, index_col_nram,
                     n_real * C, C);
  }
}

template <typename T>
__mlu_func__ void compute(T *feats_nram, T *voxel_feats_nram,
                          int *index_mask_nram, int *voxel_from_nram,
                          const int n_real, const int N, const int C) {
  if (n_real > 0) {
    // view [n_real, C] as [n_real * C]
    int deal_num = n_real * C;
    // if (feats[i] == voxel_feats[i]) {mask[i] = 1} else {mask[i] = 0}
    __bang_eq(feats_nram, voxel_feats_nram, feats_nram, deal_num);
    // change mask1's dtype to int32
    __bang_float2int32_tz((int *)feats_nram, feats_nram, deal_num, 0);
    // mask2 = NOT mask1
    __bang_not((int *)voxel_feats_nram, (int *)feats_nram, deal_num);
    // choose index of "feats[i] == voxel_feats[i]"
    __bang_mul((int *)feats_nram, (int *)feats_nram, index_mask_nram, deal_num);
    // mask2 *= N * C
    __bang_mul_scalar((int *)voxel_feats_nram, (int *)voxel_feats_nram, N * C,
                      deal_num);
    // mix choosed index and 'N * C'
    __bang_add(index_mask_nram, (int *)voxel_feats_nram, (int *)feats_nram,
               deal_num);
    // choose the min index
    __bang_minequal(voxel_from_nram, voxel_from_nram, index_mask_nram,
                    deal_num);
  }
}

__mlu_func__ void storeAsync(int *voxel_from, const int *voxel_from_nram,
                             const int *point2voxel_map_real_nram,
                             bool *voxel_from_flag_nram, int *index_mask_nram,
                             const int n_real, const int N, const int C) {
  int size_feats_idx = C * sizeof(int);
  for (int i = 0; i < n_real; i++) {
    int offset_real = point2voxel_map_real_nram[i];
    // 1) use atomicmin, too slow
    // __bang_atomic_reduce_min(voxel_from + offset_real * C,
    //                          voxel_from_nram + i * C, C);
    // 2) compare one by one, use voxel_from_flag_nram as flags to record
    // whether dst idx has appeard
    if (voxel_from_flag_nram[offset_real] == false) {
      // if number of grad idx on offset_real == 1, use the idx value directly
      __memcpy_async(voxel_from + offset_real * C, voxel_from_nram + i * C,
                     size_feats_idx, NRAM2GDRAM);
      // set voxel_from_flag to true
      voxel_from_flag_nram[offset_real] = true;
    } else {
      __sync_io();
      // load the idx appeard
      __memcpy(index_mask_nram, voxel_from + offset_real * C, size_feats_idx,
               GDRAM2NRAM);
      // if number of grad idx on offset_real > 1, pick the min idx value
      __bang_minequal(index_mask_nram, index_mask_nram, voxel_from_nram + i * C,
                      C);
      // store the new idx
      __memcpy(voxel_from + offset_real * C, index_mask_nram, size_feats_idx,
               NRAM2GDRAM);
    }
  }
}

template <typename T>
__mlu_global__ void MLUKernelMaxReduceTracebackScatterIdx(
    const T *feats, const T *voxel_feats, T *grad_feats, int *voxel_from,
    const int *point2voxel_map, const int *voxel_num, const int N,
    const int C) {
  const int M = *voxel_num;
  int size_input = N * sizeof(int);
  int size_reduced_flag = M * sizeof(bool);
  int size_feats = C * sizeof(T);
  int size_feats_idx = C * sizeof(int);
  // init workspace and output
  int n_per_core = N / taskDim;
  int rem = N % taskDim;
  n_per_core += (int)(taskId < rem);
  int offset_per_core = (taskId * n_per_core + ((taskId < rem) ? 0 : rem)) * C;
  if (n_per_core > 0) {
    // voxel_from [M,C] fill 'N * C' ([M:N,C] is redundent)
    __gdramset(voxel_from + offset_per_core, n_per_core * C, N * C);
    // grad_feats [N,C] fill '0'
    __gdramset(grad_feats + offset_per_core, n_per_core * C, T(0));
  }

  int nram_size = MAX_NRAM_SIZE;
  int n_limit = (nram_size - size_input - size_reduced_flag - size_feats_idx) /
                (2 * size_feats + 2 * size_feats_idx + sizeof(int));
  int feats_limit = n_limit * C;

  T *feats_nram = (T *)nram_buffer;                // [n_limit, C]
  T *voxel_feats_nram = feats_nram + feats_limit;  // [n_limit, C]
  int *index_mask_nram =
      (int *)(voxel_feats_nram + feats_limit);                // [n_limit, C]
  int *voxel_from_nram = index_mask_nram + feats_limit;       // [n_limit, C]
  int *point2voxel_map_nram = voxel_from_nram + feats_limit;  // [N]
  int *point2voxel_map_real_nram = point2voxel_map_nram + N;  // [n_limit]
  bool *voxel_from_flag_nram =
      (bool *)(point2voxel_map_real_nram + n_limit);        // [M]
  int *index_col_nram = (int *)(voxel_from_flag_nram + M);  // [C]

  __sync_all();

  // broadcast point2voxel_map to nram
  __memcpy(point2voxel_map_nram, point2voxel_map, size_input, GDRAM2NRAM);
  // initialze voxel_from_flag to false
  __memset_nram(voxel_from_flag_nram, M, false);
  for (int i = 0; i < C; i++) {
    index_col_nram[i] = i;
  }
  for (int x = 0, n_real = 0; x < N;) {
    // load data, get x and n_real
    loadAsync(feats_nram, voxel_feats_nram, index_mask_nram, voxel_from_nram,
              point2voxel_map_real_nram, point2voxel_map_nram, index_col_nram,
              feats, voxel_feats, voxel_from, x, n_real, n_limit, N, C);
    __sync();
    // compute
    compute(feats_nram, voxel_feats_nram, index_mask_nram, voxel_from_nram,
            n_real, N, C);
    // store
    storeAsync(voxel_from, voxel_from_nram, point2voxel_map_real_nram,
               voxel_from_flag_nram, index_mask_nram, n_real, N, C);
    __sync();
  }
}

void MLUOP_WIN_API KernelDynamicPointToVoxelBackward(
    cnrtDim3_t k_dim, cnrtFunctionType_t k_type, cnrtQueue_t queue,
    const void *feats, const void *voxel_feats, void *grad_feats,
    void *voxel_from, const void *point2voxel_map, const void *voxel_num,
    const int N, const int C) {
  MLUKernelMaxReduceTracebackScatterIdx<<<k_dim, k_type, queue>>>(
      (const float *)feats, (const float *)voxel_feats, (float *)grad_feats,
      (int *)voxel_from, (const int *)point2voxel_map, (const int *)voxel_num,
      N, C);
}
