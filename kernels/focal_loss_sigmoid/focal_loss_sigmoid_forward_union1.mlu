/*************************************************************************
 * Copyright (C) [2022] by Cambricon, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#include "focal_loss_sigmoid.h"

#include <float.h>

#include "core/logging.h"
#include "kernels/kernel.h"
#include "kernels/utils/common.h"

#define PING 0
#define PONG 1

__nram__ char nram_buffer[MAX_NRAM_SIZE];

namespace forward {

/*
 * Functions Table
 * |----------|------------------------------------|
 * |  Convert | inplaceConvert                     |
 * |----------|------------------------------------|
 * |  I0      | loadInput, loadWeight, storeOutput |
 * |----------|------------------------------------|
 */
template <typename SrcType, typename DstType>
__mlu_func__ void inplaceConvert(char *nram_ptr, const int32_t count) {
  // no need to convert when SrcType and DstType are the same
}

template <>
__mlu_func__ void inplaceConvert<float, half>(char *nram_ptr,
                                              const int32_t count) {
  __bang_float2half_rd((half *)nram_ptr, (float *)nram_ptr,
                       PAD_UP(count, COMPUTE_COUNT_ALIGN));
}

template <>
__mlu_func__ void inplaceConvert<half, float>(char *nram_ptr,
                                              const int32_t count) {
  __bang_half2float((float *)nram_ptr, (half *)nram_ptr + count,
                    PAD_UP(count, COMPUTE_COUNT_ALIGN));
}

template <typename T>
__mlu_func__ void loadInput(char *nram_input, T *dram_input, const int32_t size,
                            const int32_t dst_stride = 0,
                            const int32_t src_stride = 0,
                            const int32_t count = 1) {
  if (dst_stride == src_stride) {
    __memcpy_async(nram_input, dram_input, size * count, GDRAM2NRAM);
  } else {
    __memcpy_async(nram_input, dram_input, size, GDRAM2NRAM, dst_stride,
                   src_stride, count - 1);
  }
}

template <>
__mlu_func__ void loadInput<half>(char *nram_input, half *dram_input,
                                  const int32_t size, const int32_t dst_stride,
                                  const int32_t src_stride,
                                  const int32_t count) {
  // load into the second half of input space for inplace convertion half2float
  const int32_t deal_num =
      PAD_UP(dst_stride * count / sizeof(half), NFU_ALIGN_SIZE / sizeof(float));
  if (dst_stride == src_stride) {
    __memcpy_async((half *)nram_input + deal_num, (half *)dram_input,
                   size * count, GDRAM2NRAM);
  } else {
    __memcpy_async((half *)nram_input + deal_num, (half *)dram_input, size,
                   GDRAM2NRAM, dst_stride, src_stride, count - 1);
  }
}

template <typename T>
__mlu_func__ void loadWeight(char *nram_input, T *dram_input, const int32_t t,
                             const int32_t c, const int32_t has_weight,
                             const int32_t partition_nc) {
  if (has_weight && partition_nc && t >= 0 && t < c) {
    *((float *)nram_input) = *((T *)dram_input + t);
  }
}

template <typename T>
__mlu_func__ void storeOutput(T *dram_output, char *nram_output,
                              const int32_t size, const int32_t dst_stride = 0,
                              const int32_t src_stride = 0,
                              const int32_t count = 1) {
  if (dst_stride == src_stride) {
    __memcpy_async(dram_output, nram_output, size * count, NRAM2GDRAM);
  } else {
    __memcpy_async(dram_output, nram_output, size, NRAM2GDRAM, dst_stride,
                   src_stride, count - 1);
  }
}

__mlu_func__ void compute(const focalLossSigmoidPreference_t prefer,
                          float *input, const int32_t *target,
                          const float *weight, const int32_t has_weight,
                          const int32_t partition_nc, const int32_t deal_num,
                          const int32_t n_seg, const int32_t c,
                          const int32_t c_seg, const int32_t c_start_index,
                          const float alpha, const float gamma,
                          float *compute_a, float *compute_b, float *output) {
  // set params
  const int32_t c_end_index = c_start_index + c_seg;
  const int32_t c_num =
      has_weight ? PAD_UP(c_seg, NFU_ALIGN_SIZE / sizeof(float)) : c_seg;

  // 0. p = sigmoid(x)
  __mluop_sigmoid(compute_b, input, NULL, 0, deal_num);
  __bang_write_value(output, deal_num, float(alpha - 1));

  if (prefer == COMPUTATION_FAST) {
    /********* COMPUTATION_FAST *********/
    __bang_mul_scalar(input, compute_b, (float)-1, deal_num);
    __bang_add_scalar(input, input, (float)1, deal_num);

    for (int32_t i = 0; i < n_seg; ++i) {
      const int32_t t = *((uint32_t *)target + i);
      if (t >= c_start_index && t < c_end_index) {
        const uint32_t index = i * c_num + t - c_start_index;

        // pt = p      t(target)
        //    = 1 - p  otherwise
        *((float *)input + index) = *(compute_b + index);

        // 1 - pt = 1 - p  t
        //        = p      otherwise
        *((float *)compute_b + index) = 1.0 - (*(compute_b + index));

        // -alpha_t = -alpha      t
        //          = alpha - 1   otherwise
        *((float *)output + index) = -alpha;
      }
    }

    // 1. (1-pt) ^ gamma:
    //        log(1-pt) -> log(1-pt)*gamma -> exp[log(1-pt)*gamma]
    if (gamma == float(0.0)) {
      __bang_write_value(compute_a, deal_num, (float)1.0);
    } else {
      __bang_log(compute_a, compute_b, deal_num);
      __bang_mul_scalar(compute_a, compute_a, (float)gamma, deal_num);
      __bang_pow2(compute_a, compute_a, deal_num);
    }

    // 2. output = -alpha_t * (1 - pt) ^ gamma
    __bang_mul(output, compute_a, output, deal_num);

    // 3. output *= log[max(pt, FTL_MIN)]
    __bang_write_value(compute_a, deal_num, (float)FLT_MIN);
    __bang_maxequal(input, compute_a, input, deal_num);
    __mluop_log(compute_b, input, NULL, 0, deal_num);
    __bang_mul(output, compute_b, output, deal_num);

  } else {
    /********* COMPUTATION_HIGH_PRECISION *********/
    for (int32_t i = 0; i < n_seg; ++i) {
      const int32_t t = *((uint32_t *)target + i);
      if (t >= c_start_index && t < c_end_index) {
        const uint32_t index = i * c_num + t - c_start_index;

        // x = x   t
        //   = -x  otherwise
        *((float *)input + index) = -1.0 * (*(input + index));

        // 1 - pt = 1 - p  t
        //        = p      otherwise
        *((float *)compute_b + index) = 1.0 - (*(compute_b + index)) + FLT_MIN;

        // -alpha_t = -alpha      t
        //          = alpha - 1   otherwise
        *((float *)output + index) = -alpha;
      }
    }

    // 1. (1-pt) ^ gamma:
    //        log(1-pt) -> log(1-pt)*gamma -> exp[log(1-pt)*gamma]
    if (gamma == float(0.0)) {
      __bang_write_value(compute_a, deal_num, (float)1.0);
    } else {
      __bang_log(compute_a, compute_b, deal_num);
      __bang_mul_scalar(compute_a, compute_a, (float)gamma, deal_num);
      __bang_pow2(compute_a, compute_a, deal_num);
    }

    __bang_mul(output, compute_a, output, deal_num);

    // 2. log(pt)
    // max = max(0, -x)  t
    //     = max(0, x)   otherwise
    __bang_write_value(compute_b, deal_num, (float)0);
    __bang_maxequal(compute_b, compute_b, input, deal_num);

    // log(p_t) = ln{1 / [e^(-max)+ e^(-max-x)]} - max   t
    //          = ln{1 / [e^(-max)+ e^(-max+x)]} - max   otherwise
    __bang_mul_scalar(compute_a, compute_b, (float)-1, deal_num);
    __bang_add(input, compute_a, input, deal_num);
    __mluop_exp(compute_a, compute_a, NULL, 0, deal_num);
    __mluop_exp(input, input, NULL, 0, deal_num);

    __bang_add(compute_a, compute_a, input, deal_num);
    __bang_recip(compute_a, compute_a, deal_num);

    // filter NAN
    __bang_write_value(input, deal_num, (float)FLT_MIN);
    __bang_maxequal(compute_a, input, compute_a, deal_num);
    __mluop_log(compute_a, compute_a, NULL, 0, deal_num);

    // filter INF
    __bang_le_scalar(input, compute_b, (float)FLT_MAX, deal_num);
    __bang_float2int32((int32_t *)input, input, deal_num, 0);
    __nram__ int32_t table[COMPUTE_COUNT_ALIGN] = {0, (int32_t)0xffffffff};
    __bang_lut_s32((int32_t *)input, (int32_t *)input, table, deal_num, COMPUTE_COUNT_ALIGN);  // NOLINT
    __bang_band((char *)compute_b, (char *)compute_b, (char *)input, sizeof(float) * deal_num);  // NOLINT
    __bang_sub(compute_a, compute_a, compute_b, deal_num);

    // 3. output = alpha_t * p_t^r * [-log(p_t)]
    __bang_mul(output, output, compute_a, deal_num);
  }

  // with weight
  if (has_weight) {
    for (int32_t i = 0; i < n_seg; ++i) {
      int32_t t = *((int32_t *)target + i);
      if (t >= 0 && t < c) {
        t = partition_nc ? 0 : t;
        __bang_mul_scalar(output + i * c_num, output + i * c_num, *(weight + t),
                          c_num);
      }
    }
  }
}

template <typename T>
__mlu_func__ void startPipeline(
    const focalLossSigmoidPreference_t prefer, const T *input,
    const int32_t *target, const T *weight, char *nram_compute_a,
    char *nram_compute_b, char *nram_input, char *nram_target,
    char *nram_weight, char *nram_output, const int32_t has_weight,
    const int32_t partition_nc, const int32_t pingpong_offset,
    const int32_t pingpong_weight_offset, const int32_t c_offset_num,
    const int32_t n, const int32_t n_seg, const int32_t c, const int32_t c_seg,
    const float alpha, const float gamma, T *output) {
  // with offset
  input = input + c_offset_num;
  output = output + c_offset_num;

  const int32_t c_seg_align_num = PAD_UP(c_seg, NFU_ALIGN_SIZE / sizeof(float));
  const int32_t c_num = has_weight ? c_seg_align_num : c_seg;
  const int32_t deal_num =
      PAD_UP(n_seg * c_num, NFU_ALIGN_SIZE / sizeof(float));

  const int32_t load_size = c_seg * sizeof(T);
  const int32_t dram_stride = c * sizeof(T);
  const int32_t nram_stride = c_num * sizeof(T);
  const int32_t repeat = n / n_seg;
  const int32_t remain = n % n_seg;

  if (has_weight && !partition_nc) {
    loadInput<T>(nram_weight, (T *)weight, load_size, nram_stride, dram_stride,
                 1);
    __sync();
    inplaceConvert<T, float>(nram_weight, c_seg_align_num);
  }

  /*
   * Pipeline: The pipeline is processed in three stages: Load, Compute, Store.
   *           The allocated memory space of NRAM is divided into two parts:
   *           PING and Pong. In a single time slice, PING is used to process
   *           IO stream and PONG is used for computation. Both of them are
   *           processed synchronously until finished.
   *
   * diagram of PINGPONG:
   * |------|-----------------------------------------------------------------|
   * |      |                              space                              |
   * |------|-----------------------------------------------------------------|
   * | time |   Ping   |   Pong   |   Ping   |   Pong   |   Ping   |   Pong   |
   * |------|-----------------------------------------------------------------|
   * |  0   |    L0    |          |          |          |          |          |
   * |  1   |    C0    |    L1    |          |          |          |          |
   * |  2   |    S0    |    C1    |    L2    |          |          |          |
   * |  3   |          |    S1    |    C2    |    L3    |          |          |
   * |  4   |          |          |    S2    |    C3    |    L4    |          |
   * |  5   |          |          |          |    S3    |    C4    |    L5    |
   * |  6   |          |          |          |          |    S4    |    C5    |
   * |  7   |          |          |          |          |          |    S5    |
   * |------|-----------------------------------------------------------------|
   */

  // diagram of PINGPONG: L0
  if (repeat > 0) {
    loadInput<T>(nram_input, (T *)input, load_size, nram_stride, dram_stride,
                 n_seg);
    loadInput<int32_t>(nram_target, (int32_t *)target, n_seg * sizeof(int32_t));
    loadWeight<T>(nram_weight, (T *)weight, *((int32_t *)target), c, has_weight,
                  partition_nc);
    __sync();
  }

  if (repeat > 1) {
    // diagram of PINGPONG: C0
    inplaceConvert<T, float>(nram_input, deal_num);
    compute(prefer, (float *)nram_input, (int32_t *)nram_target,
            (float *)nram_weight, has_weight, partition_nc, deal_num, n_seg, c,
            c_seg, c_offset_num, alpha, gamma, (float *)nram_compute_a,
            (float *)nram_compute_b, (float *)nram_output);
    inplaceConvert<float, T>(nram_output, deal_num);

    // diagram of PINGPONG: L1
    loadInput<T>(nram_input + pingpong_offset, (T *)input + c * n_seg,
                 load_size, nram_stride, dram_stride, n_seg);
    loadInput<int32_t>(nram_target + pingpong_offset, (int32_t *)target + n_seg,
                       n_seg * sizeof(int32_t));
    loadWeight<T>(nram_weight + pingpong_weight_offset, (T *)weight,
                  *((int32_t *)target + n_seg), c, has_weight, partition_nc);
    __sync();
  }

  for (int32_t i = 0; i < repeat - 2; ++i) {
    storeOutput<T>((T *)output + i * c * n_seg,
                   nram_output + (i % 2) * pingpong_offset, load_size,
                   dram_stride, nram_stride, n_seg);
    loadInput<T>(nram_input + (i % 2) * pingpong_offset,
                 (T *)(input) + (i + 2) * c * n_seg, load_size, nram_stride,
                 dram_stride, n_seg);
    loadInput<int32_t>(nram_target + (i % 2) * pingpong_offset,
                       (int32_t *)target + (i + 2) * n_seg,
                       n_seg * sizeof(int32_t));
    loadWeight<T>(nram_weight + (i % 2) * pingpong_weight_offset, (T *)weight,
                  *((int32_t *)target + (i + 2) * n_seg), c, has_weight,
                  partition_nc);

    inplaceConvert<T, float>(nram_input + ((i + 1) % 2) * pingpong_offset,
                             deal_num);
    compute(prefer, (float *)(nram_input + ((i + 1) % 2) * pingpong_offset),
            (int32_t *)(nram_target + ((i + 1) % 2) * pingpong_offset),
            (float *)(nram_weight +
                      partition_nc * ((i + 1) % 2) * pingpong_weight_offset),
            has_weight, partition_nc, deal_num, n_seg, c, c_seg, c_offset_num,
            alpha, gamma, (float *)nram_compute_a, (float *)nram_compute_b,
            (float *)(nram_output + ((i + 1) % 2) * pingpong_offset));
    inplaceConvert<float, T>(nram_output + ((i + 1) % 2) * pingpong_offset,
                             deal_num);
    __sync();
  }

  if (repeat > 1) {
    storeOutput<T>((T *)output + (repeat - 2) * c * n_seg,
                   nram_output + (repeat % 2) * pingpong_offset, load_size,
                   dram_stride, nram_stride, n_seg);
  }

  if (remain > 0) {
    loadInput<T>(nram_input + (repeat % 2) * pingpong_offset,
                 (T *)input + repeat * c * n_seg, load_size, nram_stride,
                 dram_stride, remain);
    loadInput<int32_t>(nram_target + (repeat % 2) * pingpong_offset,
                       (int32_t *)target + repeat * n_seg,
                       remain * sizeof(int32_t));
    loadWeight<T>(nram_weight + (repeat % 2) * pingpong_weight_offset,
                  (T *)weight, *((int32_t *)target + repeat * n_seg), c,
                  has_weight, partition_nc);
  }

  if (repeat > 0) {
    inplaceConvert<T, float>(nram_input + ((repeat - 1) % 2) * pingpong_offset,
                             deal_num);
    compute(prefer,
            (float *)(nram_input + ((repeat - 1) % 2) * pingpong_offset),
            (int32_t *)(nram_target + ((repeat - 1) % 2) * pingpong_offset),
            (float *)(nram_weight + partition_nc * ((repeat - 1) % 2) *
                                        pingpong_weight_offset),
            has_weight, partition_nc, deal_num, n_seg, c, c_seg, c_offset_num,
            alpha, gamma, (float *)nram_compute_a, (float *)nram_compute_b,
            (float *)(nram_output + ((repeat - 1) % 2) * pingpong_offset));
    inplaceConvert<float, T>(nram_output + ((repeat - 1) % 2) * pingpong_offset,
                             deal_num);
  }
  __sync();

  if (repeat > 0) {
    storeOutput<T>((T *)output + (repeat - 1) * c * n_seg,
                   nram_output + ((repeat - 1) % 2) * pingpong_offset,
                   load_size, dram_stride, nram_stride, n_seg);
  }

  if (remain > 0) {
    const int32_t rem_num =
        PAD_UP(remain * c_num, NFU_ALIGN_SIZE / sizeof(float));
    inplaceConvert<T, float>(nram_input + (repeat % 2) * pingpong_offset,
                             rem_num);
    compute(prefer, (float *)(nram_input + (repeat % 2) * pingpong_offset),
            (int32_t *)(nram_target + (repeat % 2) * pingpong_offset),
            (float *)(nram_weight +
                      partition_nc * (repeat % 2) * pingpong_weight_offset),
            has_weight, partition_nc, rem_num, remain, c, c_seg, c_offset_num,
            alpha, gamma, (float *)nram_compute_a, (float *)nram_compute_b,
            (float *)(nram_output + (repeat % 2) * pingpong_offset));
    inplaceConvert<float, T>(nram_output + (repeat % 2) * pingpong_offset,
                             deal_num);
    __sync();

    storeOutput<T>((T *)output + repeat * c * n_seg,
                   nram_output + (repeat % 2) * pingpong_offset, load_size,
                   dram_stride, nram_stride, remain);
  }
  __sync();
}

/*
 * NRAM partition
 *  |-----------------------------------------------------------------------|
 *  |                                 weight                                |
 *  |-----------------------------------------------------------------------|
 *  |              computeA             |           computeB                |
 *  |-----------------------------------|-----------------------------------|
 *  |              PING input           |           PING output             |
 *  |-----------------------------------|-----------------------------------|
 *  |              PONG input           |           PONG output             |
 *  |-----------------------------------|-----------------------------------|
 *  |              target               |           target                  |
 *  |-----------------------------------------------------------------------|
 *
 * split_pipeline_num = 6:
 *    compute(computeA, computeB)
 *    PING(input, output)
 *    PONG(input, output)
 *
 * split_target_num = 2:
 *    PING(target), PONG(target).
 *
 * weight is not NULL:
 *    The nram-size of weight is equal to c_align_size when partition input-N.
 *    The nram-size of weight is equal to NFU_ALIGN_SIZE when partition
 *    input-N.
 */
template <typename T>
__mlu_func__ void partitionInput(const focalLossSigmoidPreference_t prefer,
                                 const T *input, const int32_t *target,
                                 const T *weight, const int32_t n,
                                 const int32_t c, const float alpha,
                                 const float gamma, T *output) {
  // calculate threshold of c
  const int32_t has_weight = weight != NULL;
  const int32_t split_pipeline_num = 6;
  const int32_t split_target_num = 2;
  const int32_t threshold_c =
      PAD_DOWN((MAX_NRAM_SIZE - split_target_num * NFU_ALIGN_SIZE) /
                   (split_pipeline_num + has_weight),
               NFU_ALIGN_SIZE) /
      sizeof(float);
  const int32_t c_align = PAD_UP(c, NFU_ALIGN_SIZE / sizeof(float));
  const int32_t c_align_size = c_align * sizeof(float);

  if (c <= threshold_c) {
    // partition N
    int32_t c_num = c;
    int32_t reservered_align_size =
        (split_target_num + split_pipeline_num) * NFU_ALIGN_SIZE;
    int32_t weight_size = 0;
    if (has_weight) {
      c_num = c_align;
      reservered_align_size = split_target_num * NFU_ALIGN_SIZE;
      weight_size = c_align_size;
    }
    const int32_t remain_size =
        MAX_NRAM_SIZE - weight_size - reservered_align_size;
    const int32_t n_seg =
        remain_size / (split_pipeline_num * c_num * sizeof(float) +
                       split_target_num * sizeof(int32_t));
    const int32_t split_pipeline_size =
        PAD_UP(c_num * n_seg * sizeof(float), NFU_ALIGN_SIZE);
    const int32_t compute_size = 2 * split_pipeline_size;
    const int32_t pingpong_offset =
        (MAX_NRAM_SIZE - weight_size - compute_size) / 2;

    char *nram_weight = (char *)nram_buffer;
    char *nram_compute_a = nram_weight + has_weight * c_align_size;
    char *nram_compute_b = nram_compute_a + split_pipeline_size;
    char *nram_input = nram_compute_b + split_pipeline_size;
    char *nram_output = nram_input + split_pipeline_size;
    char *nram_target = nram_output + split_pipeline_size;
    startPipeline<T>(prefer, input, target, weight, nram_compute_a,
                     nram_compute_b, nram_input, nram_target, nram_weight,
                     nram_output, has_weight, 0, pingpong_offset, 0, 0, n,
                     n_seg, c, c, alpha, gamma, output);
  } else {
    // partition NC
    const int32_t weight_size = has_weight * NFU_ALIGN_SIZE;
    const int32_t remain_size =
        MAX_NRAM_SIZE - weight_size - split_target_num * NFU_ALIGN_SIZE;
    const int32_t split_pipeline_size =
        PAD_DOWN(remain_size / split_pipeline_num, NFU_ALIGN_SIZE);
    const int32_t c_seg = split_pipeline_size / sizeof(float);
    const int32_t n_seg = 1;
    const int32_t compute_size = 2 * split_pipeline_size;
    const int32_t pingpong_offset =
        (MAX_NRAM_SIZE - weight_size - compute_size) / 2;
    const int32_t pingpong_weight_offset = weight_size / 2;

    char *nram_weight = (char *)nram_buffer;
    char *nram_compute_a = nram_weight + weight_size;
    char *nram_compute_b = nram_compute_a + split_pipeline_size;
    char *nram_input = nram_compute_b + split_pipeline_size;
    char *nram_output = nram_input + split_pipeline_size;
    char *nram_target = nram_output + split_pipeline_size;

    const int32_t loop_num = (c + c_seg - 1) / c_seg;
    const int32_t partition_nc = 1;
    for (int32_t i = 0; i < loop_num; ++i) {
      const int32_t c_index = i * c_seg;
      const int32_t c_seg_curr = i == (loop_num - 1) ? c - c_index : c_seg;
      startPipeline<T>(prefer, input, target, weight, nram_compute_a,
                       nram_compute_b, nram_input, nram_target, nram_weight,
                       nram_output, has_weight, partition_nc, pingpong_offset,
                       pingpong_weight_offset, c_index, n, n_seg, c, c_seg_curr,
                       alpha, gamma, output);
    }
  }
}

template <typename T>
__mlu_global__ void MLUUnion1KernelFocalLossSigmoidForward(
    const focalLossSigmoidPreference_t prefer, const void *input,
    const void *target, const void *weight, const int32_t N, const int32_t C,
    const float alpha, const float gamma, void *output) {
  const int32_t n_seg = N / taskDim + (int32_t)((N % taskDim) > taskId);
  const int32_t n_offset = (N % taskDim) > taskId
                               ? (N / taskDim + 1) * taskId
                               : N / taskDim * taskId + N % taskDim;
  const T *base_input = (T *)input + n_offset * C;
  const int32_t *base_target = (int32_t *)target + n_offset;
  T *base_output = (T *)output + n_offset * C;

  partitionInput(prefer, (T *)base_input, (int32_t *)base_target, (T *)weight,
                 n_seg, C, alpha, gamma, (T *)base_output);
}

}  // namespace forward

mluOpStatus_t MLUOP_WIN_API mluOpBlockKernelFocalLossSigmoidForwardHalf(
    cnrtDim3_t k_dim, cnrtFunctionType_t k_type, cnrtQueue_t queue,
    const focalLossSigmoidPreference_t prefer, const void *input,
    const void *target, const void *weight, const int32_t N, const int32_t C,
    const float alpha, const float gamma, void *output) {
  KERNEL_CHECK(forward::MLUUnion1KernelFocalLossSigmoidForward<half>
               <<<k_dim, k_type, queue>>>(prefer, input, target, weight, N, C,
                                          alpha, gamma, output));
  return MLUOP_STATUS_SUCCESS;
}

mluOpStatus_t MLUOP_WIN_API mluOpBlockKernelFocalLossSigmoidForwardFloat(
    cnrtDim3_t k_dim, cnrtFunctionType_t k_type, cnrtQueue_t queue,
    const focalLossSigmoidPreference_t prefer, const void *input,
    const void *target, const void *weight, const int32_t N, const int32_t C,
    const float alpha, const float gamma, void *output) {
  KERNEL_CHECK(forward::MLUUnion1KernelFocalLossSigmoidForward<float>
               <<<k_dim, k_type, queue>>>(prefer, input, target, weight, N, C,
                                          alpha, gamma, output));
  return MLUOP_STATUS_SUCCESS;
}
