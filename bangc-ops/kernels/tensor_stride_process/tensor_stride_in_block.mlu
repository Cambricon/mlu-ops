/*************************************************************************
 * Copyright (C) [2022] by Cambricon, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#include "mlu.h"
#include "kernels/tensor_stride_process/tensor_stride_process_common.h"

#define SIZE_NRAM_BUF (MAX_NRAM_SIZE + REM_FOR_STACK - 12 * 1024)
__nram__ char ram[SIZE_NRAM_BUF];

template <typename T>
__mlu_func__ void blockTensorStridedIn(T *input, TensorShape &input_shape,
                                       T *output) {
  int total_num = input_shape.total_num;
  int data_per_core = total_num / taskDim;
  int data_last_core = total_num / taskDim + total_num % taskDim;
  // currently SIZE_NRAM_BUF does not exceed 2GB, keep int32 for performance
  // reasons
  int load_once = SIZE_NRAM_BUF / sizeof(T);
  int load_repeat;
  int load_remain;
  if (taskId < taskDim - 1) {
    load_once = load_once > data_per_core ? data_per_core : load_once;
    load_repeat = data_per_core / load_once;
    load_remain = data_per_core % load_once;
  } else {
    load_once = load_once > data_last_core ? data_last_core : load_once;
    load_repeat = data_last_core / load_once;
    load_remain = data_last_core % load_once;
  }
  for (int i = 0; i < load_repeat; i++) {
    tensorStrideLoad<T>((T *)ram, input, i * load_once + taskId * data_per_core,
                        load_once, sizeof(T), input_shape);
    __memcpy(output + i * load_once + taskId * data_per_core, (T *)ram,
             load_once * sizeof(T), NRAM2GDRAM);
  }
  if (load_remain > 0) {
    tensorStrideLoad<T>((T *)ram, input,
                        load_repeat * load_once + taskId * data_per_core,
                        load_remain, sizeof(T), input_shape);
    __memcpy(output + load_repeat * load_once + taskId * data_per_core,
             (T *)ram, load_remain * sizeof(T), NRAM2GDRAM);
  }
}

template <typename T>
__mlu_global__ void MLUUnionKernelTensorStrideIn(const void *input,
                                                 TensorShape input_shape,
                                                 void *output) {
  blockTensorStridedIn((T *)input, input_shape, (T *)output);
}

template void MLUUnionKernelTensorStrideIn<int8>(const void *input,
                                                 TensorShape input_shape,
                                                 void *output);
template void MLUUnionKernelTensorStrideIn<half>(const void *input,
                                                 TensorShape input_shape,
                                                 void *output);
template void MLUUnionKernelTensorStrideIn<float>(const void *input,
                                                  TensorShape input_shape,
                                                  void *output);
