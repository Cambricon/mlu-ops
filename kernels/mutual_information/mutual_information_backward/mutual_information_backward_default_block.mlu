/*************************************************************************
 * Copyright (C) [2023] by Cambricon, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#include "mutual_information_backward.h"

#include "core/logging.h"
#include "kernels/kernel.h"
#include "kernels/utils/common.h"
#include "kernels/mutual_information/mutual_information_backward/mutual_information_backward_utils.h"

__mlu_func__ bool calPartitionJobScope(
    bool has_boundary, const int64_t *opt_boundary, const int B, const int S,
    const int T, const int step_i, const int job_num_on_step,
    const int s_block_num, const int t_block_num, const int s_block_size,
    const int t_block_size, int &batch_idx, int &batch_s_begin,
    int &batch_t_begin, int &batch_s_end, int &batch_t_end, int &cur_s_begin,
    int &cur_t_begin, int &cur_s_end, int &cur_t_end, int &cur_s_size,
    int &cur_t_size, bool &need_compute_ans_grad, bool overwrite_ans_grad,
    float *px_grad, float *py_grad) {
  int job_num_on_batch = job_num_on_step / B;  // Each batch job num
  batch_idx = taskId / job_num_on_batch;       // Current job on which batch
  int block_id_in_batch =
      taskId - batch_idx * job_num_on_batch;  // Current job id in batch

  // taskDim is not always job num, because of TASK_DIM_X limit
  if (batch_idx >= B) {
    return true;
  }

  // Compute s and t block id in batch
  int s_block_id, t_block_id;
  s_block_id = __mluop_max(0, s_block_num - 1 - step_i) + block_id_in_batch;
  t_block_id =
      __mluop_min(t_block_num - 1, s_block_num + t_block_num - 2 - step_i) -
      block_id_in_batch;

  // Compute current job id scope
  cur_s_begin = s_block_id * s_block_size;
  cur_t_begin = t_block_id * t_block_size;
  cur_s_end = (s_block_id + 1) * s_block_size - 1;
  cur_t_end = (t_block_id + 1) * t_block_size - 1;

  // Deal with boundary and decide current job if need to compute
  if (has_boundary) {
    int64_t *boundary = (int64_t *)nram_buffer;
    __memcpy(boundary, opt_boundary + 4 * batch_idx, 4 * sizeof(int64_t),
             GDRAM2NRAM);
    batch_s_begin = boundary[0];
    batch_t_begin = boundary[1];
    batch_s_end = boundary[2];
    batch_t_end = boundary[3];
    // invalid boundary, already use cnnlFill to set px_grad and py_grad to 0
    if (batch_s_begin > batch_s_end || batch_t_begin > batch_t_end) {
      return true;
    }
  }

  // Compare current job scope with batch scope, if empty job, return
  if (cur_s_begin > batch_s_end || cur_t_begin > batch_t_end ||
      cur_s_end < batch_s_begin || cur_t_end < batch_t_begin) {
    return true;
  }

  // Reset s and t begin and end to valid boundary
  if (cur_s_begin < batch_s_begin) {
    cur_s_begin = batch_s_begin;
  }
  if (cur_t_begin < batch_t_begin) {
    cur_t_begin = batch_t_begin;
  }
  if (cur_s_end > batch_s_end) {
    cur_s_end = batch_s_end;
  }
  if (cur_t_end > batch_t_end) {
    cur_t_end = batch_t_end;
  }

  cur_s_size = cur_s_end - cur_s_begin + 1;
  cur_t_size = cur_t_end - cur_t_begin + 1;

  // At last compute step and overwrite, need to memcpy back to ans_grad
  if (overwrite_ans_grad && cur_s_begin == batch_s_begin &&
      cur_t_begin == batch_t_begin) {
    need_compute_ans_grad = true;
  } else {
    need_compute_ans_grad = false;
  }

  return false;
}

__mlu_func__ void loadInit(const float *gdram_px, const float *gdram_py,
                           const float *gdram_p, float *gdram_p_grad,
                           float *nram_px, float *nram_py, float *nram_p,
                           float *nram_p_grad, const int S, const int T,
                           const int batch_s_end, const int batch_t_end,
                           const int cur_s_begin, const int cur_t_begin,
                           const int cur_s_end, const int cur_t_end,
                           const int cur_s_size, const int cur_t_size) {
  // Load p(s, t)
  __memcpy_async(nram_p, gdram_p + cur_s_begin * (T + 1) + cur_t_begin,
                 cur_t_size * sizeof(float), GDRAM2NRAM,
                 (cur_t_size + 1) * sizeof(float), (T + 1) * sizeof(float),
                 cur_s_size - 1);

  // Compare current s_end and batch_s_end to decide:
  // load px or write -inf, load p or write large_neg, load p_grad or write 0
  if (cur_s_end < batch_s_end) {
    // Load px(s, t)
    __memcpy_async(nram_px, gdram_px + cur_s_begin * (T + 1) + cur_t_begin,
                   cur_t_size * sizeof(float), GDRAM2NRAM,
                   cur_t_size * sizeof(float), (T + 1) * sizeof(float),
                   cur_s_size - 1);
    // Load p(s+1, t), one row
    __memcpy_async(nram_p + cur_s_size * (cur_t_size + 1),
                   gdram_p + (cur_s_end + 1) * (T + 1) + cur_t_begin,
                   cur_t_size * sizeof(float), GDRAM2NRAM, 0, 0, 0);
    // load p_grad(s+1, t), one row
    __memcpy_async(nram_p_grad + cur_s_size * (cur_t_size + 1),
                   gdram_p_grad + (cur_s_end + 1) * (T + 1) + cur_t_begin,
                   cur_t_size * sizeof(float), GDRAM2NRAM, 0, 0, 0);
  } else {  // cur_s_end == batch_s_end, skip last row, write value
    if (cur_s_size > 1) {
      __memcpy_async(nram_px, gdram_px + cur_s_begin * (T + 1) + cur_t_begin,
                     cur_t_size * sizeof(float), GDRAM2NRAM,
                     cur_t_size * sizeof(float), (T + 1) * sizeof(float),
                     cur_s_size - 2);
    }
    // write -inf at px last row
    __nramset_async(nram_px + (cur_s_size - 1) * cur_t_size, cur_t_size,
                    (float)(-INFINITY), 0, 0);
    // write large_neg at p last row
    __nramset_async(nram_p + cur_s_size * (cur_t_size + 1), cur_t_size,
                    (float)-1.0e+30, 0, 0);
    // write 0 at p_grad last row
    __nramset_async(nram_p_grad + cur_s_size * (cur_t_size + 1), cur_t_size,
                    (float)0.0, 0, 0);
  }

  // Compare current t_end and batch_t_end to decide:
  // load py or write -inf, load p or write large_neg, load p_grad or write 0
  if (cur_t_end < batch_t_end) {
    // Load py(s, t)
    __memcpy_async(nram_py, gdram_py + cur_s_begin * T + cur_t_begin,
                   cur_t_size * sizeof(float), GDRAM2NRAM,
                   cur_t_size * sizeof(float), T * sizeof(float),
                   cur_s_size - 1);
    // Load p(s, t+1), one column
    __memcpy_async(nram_p + cur_t_size,
                   gdram_p + cur_s_begin * (T + 1) + cur_t_end + 1,
                   sizeof(float), GDRAM2NRAM, (cur_t_size + 1) * sizeof(float),
                   (T + 1) * sizeof(float), cur_s_size - 1);
    // Load p_grad(s, t+1), one column
    __memcpy_async(nram_p_grad + cur_t_size,
                   gdram_p_grad + cur_s_begin * (T + 1) + cur_t_end + 1,
                   sizeof(float), GDRAM2NRAM, (cur_t_size + 1) * sizeof(float),
                   (T + 1) * sizeof(float), cur_s_size - 1);
  } else {  // cur_t_end == batch_t_end, skip last column, write value
    // Load py(s, t)
    if (cur_t_size > 1) {
      __memcpy_async(nram_py, gdram_py + cur_s_begin * T + cur_t_begin,
                     (cur_t_size - 1) * sizeof(float), GDRAM2NRAM,
                     cur_t_size * sizeof(float), T * sizeof(float),
                     cur_s_size - 1);
    }
    // write -inf at py last column
    __nramset_async(nram_py + cur_t_size - 1, 1, (float)(-INFINITY),
                    cur_t_size * sizeof(float), cur_s_size - 1);
    // write large_neg at p last column
    __nramset_async(nram_p + cur_t_size, 1, (float)-1.0e+30,
                    (cur_t_size + 1) * sizeof(float), cur_s_size - 1);
    // write 0 at p_grad last column
    __nramset_async(nram_p_grad + cur_t_size, 1, (float)0.0,
                    (cur_t_size + 1) * sizeof(float), cur_s_size - 1);
  }
}

__mlu_func__ void computeByDiagonal(
    float *nram_px, float *nram_py, float *nram_p, float *nram_p_grad,
    float *nram_cur_px, float *nram_cur_py, float *nram_cur_p,
    float *nram_next_p, float *nram_large_neg, float *nram_mask,
    float *gdram_ans_grad, const int batch_s_end, const int batch_t_end,
    const int cur_s_end, const int cur_t_end, const int cur_s_size,
    const int cur_t_size) {
  const int repeat = cur_s_size + cur_t_size - 1;
  const int max_s_t = __mluop_max(cur_s_size, cur_t_size);
  const int min_s_t = __mluop_min(cur_s_size, cur_t_size);

  for (int i = 0; i < repeat; ++i) {
    int data_num = i < max_s_t ? __mluop_min(i + 1, min_s_t)
                               : cur_s_size + cur_t_size - i - 1;

    // px, py use same s, t index on nram,
    int first_s = __mluop_max(0, cur_s_size - 1 - i);
    int first_t = __mluop_min(cur_t_size - 1, cur_s_size + cur_t_size - 2 - i);

    // memcpy_async cur_px, cur_py,
    // memcpy cur_p(same index, data_num), next_p(next index, data_num+1)
    __memcpy(nram_cur_p, nram_p + first_s * (cur_t_size + 1) + first_t,
             sizeof(float), NRAM2NRAM, sizeof(float),
             cur_t_size * sizeof(float), data_num - 1);
    __memcpy(nram_next_p, nram_p + first_s * (cur_t_size + 1) + first_t + 1,
             sizeof(float), NRAM2NRAM, sizeof(float),
             cur_t_size * sizeof(float), data_num);
    __memcpy_async(nram_cur_px, nram_px + first_s * cur_t_size + first_t,
                   sizeof(float), NRAM2NRAM, sizeof(float),
                   (cur_t_size - 1) * sizeof(float), data_num - 1);
    __memcpy_async(nram_cur_py, nram_py + first_s * cur_t_size + first_t,
                   sizeof(float), NRAM2NRAM, sizeof(float),
                   (cur_t_size - 1) * sizeof(float), data_num - 1);

    // make cur_p and next_p number < -1.0e+30 to -1.0e+30
    __bang_nan_maximum(nram_cur_p, nram_cur_p, nram_large_neg, data_num);
    __bang_nan_maximum(nram_next_p, nram_next_p, nram_large_neg, data_num + 1);

    // sync for cur_px and cur_py
    __sync();

    // Compute term1 and term2, reuse cur_px, cur_py RAM
    // cur_term1(s, t) = exp(cur_p(s, t) + cur_px(s, t) - next_p(s + 1, t));
    __bang_fusion(FUSION_FAS, nram_cur_px, nram_cur_px, nram_cur_p,
                  nram_next_p + 1, data_num, data_num);
    // cur_term2(s, t) = exp(cur_p(s, t) + cur_py(s, t) - next_p(s, t + 1));
    __bang_fusion(FUSION_FAS, nram_cur_py, nram_cur_py, nram_cur_p, nram_next_p,
                  data_num, data_num);

    // sync for next_p
    __sync();
    // memcpy_async next_p_grad to nram_next_p
    __memcpy_async(nram_next_p,
                   nram_p_grad + first_s * (cur_t_size + 1) + first_t + 1,
                   sizeof(float), NRAM2NRAM, sizeof(float),
                   cur_t_size * sizeof(float), data_num);

    // safeExp for term1 and term2
    safeExp(nram_cur_px, nram_cur_px, nram_mask, data_num);
    safeExp(nram_cur_py, nram_cur_py, nram_mask, data_num);

    // sync for next_p_grad
    __sync();

    // Compute px_grad and py_grad
    // cur_px_grad = cur_term1 * next_p_grad(s + 1, t)
    __bang_mul(nram_cur_px, nram_cur_px, nram_next_p + 1, data_num);
    // cur_py_grad = cur_term2 * next_p_grad(s, t + 1)
    __bang_mul(nram_cur_py, nram_cur_py, nram_next_p, data_num);

    // sync for cur_px_grad and cur_py_grad
    __sync();

    // memcpy_async back to px_grad, py_grad
    __memcpy_async(nram_px + first_s * cur_t_size + first_t, nram_cur_px,
                   sizeof(float), NRAM2NRAM, (cur_t_size - 1) * sizeof(float),
                   sizeof(float), data_num - 1);
    __memcpy_async(nram_py + first_s * cur_t_size + first_t, nram_cur_py,
                   sizeof(float), NRAM2NRAM, (cur_t_size - 1) * sizeof(float),
                   sizeof(float), data_num - 1);

    // Compute p_grad
    if (cur_s_end == batch_s_end && cur_t_end == batch_t_end && i == 0) {
      // step 0, Initialize p_grad[s_end][t_end] = ans_grad[b]
      __memcpy(nram_p_grad + first_s * (cur_t_size + 1) + first_t,
               gdram_ans_grad, sizeof(float), GDRAM2NRAM);
    } else {
      // otherwise, need to compute cur_p_grad:
      // cur_p_grad(cur_p) = cur_px_grad + cur_py_grad
      __bang_add(nram_cur_p, nram_cur_px, nram_cur_py, data_num);
      // memcpy back to p_grad
      __memcpy(nram_p_grad + first_s * (cur_t_size + 1) + first_t, nram_cur_p,
               sizeof(float), NRAM2NRAM, cur_t_size * sizeof(float),
               sizeof(float), data_num - 1);
    }
  }
}

__mlu_global__ void mluBlockDefaultMutualInformationBackward(
    const int B, const int S, const int T, const int step_i,
    const int job_num_on_step, const int s_block_num, const int t_block_num,
    const int s_block_size, const int t_block_size, const float *px,
    const float *py, const bool has_boundary, const int64_t *opt_boundary,
    const float *p, const bool overwrite_ans_grad, float *ans_grad,
    float *px_grad, float *py_grad, float *p_grad) {
  /******************************** NRAM SPACE ******************************/
  /* Load Init */
  /*|---------------------------------------------------------------------|*/
  /*| px,py |  p, p_grad  |large_neg    |         |         |             |*/
  /*| 2*S*T |2*(S+1)*(T+1)| 2*min_len+1 | min_len | min_len | 2*min_len+1 |*/
  /*|---------------------------------------------------------------------|*/
  /* Compute term1 and term2 */
  /*|------------------------------------------------------------------|*/
  /*| px,py |  p          |large_neg,mask|cur_term1,2| cur_p | next_p  |*/
  /*| 2*S*T |2*(S+1)*(T+1)| 2*min_len+1  | 2*min_len |min_len|min_len+1|*/
  /*|------------------------------------------------------------------|*/
  /* Compute px_grad, py_grad, p_grad */
  /*|------------------------------------------------------------------------|*/
  /*|px/y_grad|     p_grad  |           | cur_term1,2 |cur_p_grad|next_p_grad|*/
  /*|         |             |           |cur_px/y_grad|          |           |*/
  /*|  2*S*T  |2*(S+1)*(T+1)|2*min_len+1|  2*min_len  | min_len  | min_len+1 |*/
  /*|------------------------------------------------------------------------|*/

  // NOTE: s and t block size has already + 1 on S and T
  int min_s_t_block_size = __mluop_min(s_block_size, t_block_size);

  // px, term1, px_grad
  float *nram_px_buf = (float *)nram_buffer;
  // py, term2, py_grad
  float *nram_py_buf = nram_px_buf + s_block_size * t_block_size;
  // p block
  float *nram_p = nram_py_buf + s_block_size * t_block_size;
  // p_grad block
  float *nram_p_grad = nram_p + (s_block_size + 1) * (t_block_size + 1);
  // Initialize with float(1.0e+30) value, to maximum with p
  float *nram_large_neg = nram_p_grad + (s_block_size + 1) * (t_block_size + 1);
  // mask
  float *nram_mask = nram_large_neg + min_s_t_block_size + 1;
  // cur_px, cur_term1, cur_px_grad
  float *nram_cur_px_buf = nram_mask + min_s_t_block_size;
  // cur_py, cur_term2, cur_py_grad
  float *nram_cur_py_buf = nram_cur_px_buf + min_s_t_block_size;
  // cur_p, cur_p_grad
  float *nram_cur_p = nram_cur_py_buf + min_s_t_block_size;
  // next_p, next_p_grad
  float *nram_next_p = nram_cur_p + min_s_t_block_size;

  int batch_idx;
  int batch_s_begin = 0;
  int batch_t_begin = 0;
  int batch_s_end = S;
  int batch_t_end = T;
  int cur_s_begin, cur_t_begin, cur_s_end, cur_t_end, cur_s_size, cur_t_size;
  bool need_compute_ans_grad;

  // According to has_boundary, calculate current job scope
  bool need_return = calPartitionJobScope(
      has_boundary, opt_boundary, B, S, T, step_i, job_num_on_step, s_block_num,
      t_block_num, s_block_size, t_block_size, batch_idx, batch_s_begin,
      batch_t_begin, batch_s_end, batch_t_end, cur_s_begin, cur_t_begin,
      cur_s_end, cur_t_end, cur_s_size, cur_t_size, need_compute_ans_grad,
      overwrite_ans_grad, px_grad, py_grad);
  // Because taskDimX could change to taskDimY, so not all jobs need to compute
  if (need_return) {
    return;
  }

  // sync for boundary loadGE on NRAM
  __sync();
  // px_grad and px, py_grad and py, p_grad and p, have the same shape
  const int px_one_batch_num = S * (T + 1);
  const int py_one_batch_num = (S + 1) * T;
  const int p_one_batch_num = (S + 1) * (T + 1);

  const float *gdram_px = px + batch_idx * px_one_batch_num;
  const float *gdram_py = py + batch_idx * py_one_batch_num;
  const float *gdram_p = p + batch_idx * p_one_batch_num;

  float *gdram_px_grad = px_grad + batch_idx * px_one_batch_num;
  float *gdram_py_grad = py_grad + batch_idx * py_one_batch_num;
  float *gdram_p_grad = p_grad + batch_idx * p_one_batch_num;
  float *gdram_ans_grad = ans_grad + batch_idx;

  const int min_s_t = __mluop_min(cur_s_size, cur_t_size);
  // loadInit: load px, py, other block p,
  //           or write -inf at last row of px, last column of py,
  //              write large_neg at last row and column of p,
  //           load other block p_grad,
  //           or write 0 at last row/column of p_grad
  loadInit(gdram_px, gdram_py, gdram_p, gdram_p_grad, nram_px_buf, nram_py_buf,
           nram_p, nram_p_grad, S, T, batch_s_end, batch_t_end, cur_s_begin,
           cur_t_begin, cur_s_end, cur_t_end, cur_s_size, cur_t_size);

  // Initialize large_neg with value -1e+30
  __nramset_async(nram_large_neg, min_s_t + 1, (float)-1.0e+30, 0, 0);
  // sync for initialization async instructions
  __sync();

  // Compute term1, term2, p_grad, px_grad, py_grad
  computeByDiagonal(nram_px_buf, nram_py_buf, nram_p, nram_p_grad,
                    nram_cur_px_buf, nram_cur_py_buf, nram_cur_p, nram_next_p,
                    nram_large_neg, nram_mask, gdram_ans_grad, batch_s_end,
                    batch_t_end, cur_s_end, cur_t_end, cur_s_size, cur_t_size);

  // Store:
  // memcpy back p_grad(workspace)
  __memcpy(gdram_p_grad + cur_s_begin * (T + 1) + cur_t_begin, nram_p_grad,
           cur_t_size * sizeof(float), NRAM2GDRAM, (T + 1) * sizeof(float),
           (cur_t_size + 1) * sizeof(float), cur_s_size - 1);
  // memcpy back px_grad
  if (cur_s_end < batch_s_end) {
    // memcpy all px_grad data back
    __memcpy(gdram_px_grad + cur_s_begin * (T + 1) + cur_t_begin, nram_px_buf,
             cur_t_size * sizeof(float), NRAM2GDRAM, (T + 1) * sizeof(float),
             cur_t_size * sizeof(float), cur_s_size - 1);
  } else {
    // memcpy px_grad data except last row
    if (cur_s_size > 1) {
      __memcpy(gdram_px_grad + cur_s_begin * (T + 1) + cur_t_begin, nram_px_buf,
               cur_t_size * sizeof(float), NRAM2GDRAM, (T + 1) * sizeof(float),
               cur_t_size * sizeof(float), cur_s_size - 2);
    }
  }
  // memcpy back py_grad
  if (cur_t_end < batch_t_end) {
    // memcpy all py_grad data back
    __memcpy(gdram_py_grad + cur_s_begin * T + cur_t_begin, nram_py_buf,
             cur_t_size * sizeof(float), NRAM2GDRAM, T * sizeof(float),
             cur_t_size * sizeof(float), cur_s_size - 1);
  } else {
    // memcpy py_grad data except last column
    if (cur_t_size > 1) {
      __memcpy(gdram_py_grad + cur_s_begin * T + cur_t_begin, nram_py_buf,
               (cur_t_size - 1) * sizeof(float), NRAM2GDRAM, T * sizeof(float),
               cur_t_size * sizeof(float), cur_s_size - 1);
    }
  }

  // If last compute step, need store p_grad[s_begin, t_begin] to ans_grad
  if (need_compute_ans_grad) {
    ans_grad[batch_idx] = nram_p_grad[0];
  }
}

mluOpStatus_t MLUOP_WIN_API kernelDefaultMutualInformationBackward(
    cnrtDim3_t k_dim, cnrtFunctionType_t k_type, cnrtQueue_t queue, const int B,
    const int S, const int T, const int step_i, const int job_num_on_step,
    const int s_block_num, const int t_block_num, const int s_block_size,
    const int t_block_size, const void *px, const void *py,
    const bool has_boundary, const void *opt_boundary, const void *p,
    const bool overwrite_ans_grad, void *ans_grad, void *px_grad, void *py_grad,
    void *p_grad) {
  KERNEL_CHECK(
      mluBlockDefaultMutualInformationBackward<<<k_dim, k_type, queue>>>(
          B, S, T, step_i, job_num_on_step, s_block_num, t_block_num,
          s_block_size, t_block_size, (float *)px, (float *)py, has_boundary,
          (int64_t *)opt_boundary, (float *)p, overwrite_ans_grad,
          (float *)ans_grad, (float *)px_grad, (float *)py_grad,
          (float *)p_grad));
  return MLUOP_STATUS_SUCCESS;
}
