/*************************************************************************
 * Copyright (C) [2022] by Cambricon, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#include "ms_deform_attn_backward.h"

#include "kernels/kernel.h"
#include "kernels/utils/common.h"

__nram__ char nram_buffer[MAX_NRAM_SIZE];

#define ALIGN_NUM 32

void __mlu_func__ computeGridMaskAndOffset(
    float *nram_grad_output_tl, float *nram_grad_output_tr, float *nram_loc_w,
    float *nram_loc_h, float *nram_h_stride, int32_t *nram_spatial_shapes,
    float *nram_w_low_temp, float *nram_h_high_temp, float *nram_w_low,
    float *nram_h_low, float *nram_h_high, float *nram_w_high, float *nram_lh,
    float *nram_lw, float *nram_hh, float *nram_hw,
    float *nram_h_low_ptr_offset, float *nram_h_high_ptr_offset,
    float *nram_w_low_ptr_offset, float *nram_w_high_ptr_offset, float *nram_w1,
    float *nram_w2, float *nram_w3, float *nram_w4, float *nram_offset_temp,
    float *nram_offset1, float *nram_offset2, float *nram_offset3,
    float *nram_offset4, float *nram_base_ptr, float *nram_h_low_temp,
    const int32_t &num_deal_grid, const int32_t &num_per_time_real,
    const int32_t &num_heads, const int32_t &num_levels,
    const int32_t &num_points, const int32_t &w_stride,
    const int32_t &qid_stride, float *grad_temp1) {
  // [num_levels, 2] --> [2, num_levels]
#if __BANG_ARCH__ >= 372
  __bang_transpose(nram_grad_output_tl, nram_loc_w, num_deal_grid,
                   2);  // 2 * xhlp
  __bang_transpose(nram_loc_w, nram_grad_output_tl,
                   num_per_time_real * num_heads * num_levels, num_points);
  __bang_transpose(nram_loc_h, nram_grad_output_tl + num_deal_grid,
                   num_per_time_real * num_heads * num_levels, num_points);
  __bang_int322float((float *)nram_spatial_shapes,
                     (int32_t *)nram_spatial_shapes, num_levels * 2, 0);

  __bang_transpose(nram_grad_output_tr, (float *)nram_spatial_shapes,
                   num_levels, 2);
  __bang_mul_scalar(nram_h_stride, nram_grad_output_tr + num_levels, w_stride,
                    num_levels);
  __memcpy_async(nram_spatial_shapes, nram_grad_output_tr,
                 num_levels * 2 * sizeof(float), NRAM2NRAM);

  __bang_cycle_mul(nram_loc_w, nram_loc_w,
                   (float *)nram_spatial_shapes + num_levels, num_deal_grid,
                   num_levels);
  __bang_cycle_mul(nram_loc_h, nram_loc_h, (float *)(nram_spatial_shapes),
                   num_deal_grid, num_levels);
  __bang_sub_scalar(nram_loc_w, nram_loc_w, 0.5, num_deal_grid);
  __bang_sub_scalar(nram_loc_h, nram_loc_h, 0.5, num_deal_grid);
  // get mask. (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)
  __bang_cycle_lt(nram_w_low_temp, nram_loc_w,
                  (float *)(nram_spatial_shapes + num_levels), num_deal_grid,
                  num_levels);
  __bang_cycle_lt(nram_h_high_temp, nram_loc_h, (float *)(nram_spatial_shapes),
                  num_deal_grid, num_levels);

  __bang_and(nram_w_low_temp, nram_w_low_temp, nram_h_high_temp, num_deal_grid);
  __bang_gt_scalar(nram_h_high_temp, nram_loc_h, -1, num_deal_grid);
  __bang_and(nram_h_high_temp, nram_h_high_temp, nram_w_low_temp,
             num_deal_grid);
  __bang_gt_scalar(nram_w_low_temp, nram_loc_w, -1, num_deal_grid);
  __bang_and(nram_h_high_temp, nram_h_high_temp, nram_w_low_temp,
             num_deal_grid);

  __bang_transpose(nram_w_low_temp, nram_h_high_temp, num_points,
                   num_per_time_real * num_heads * num_levels);
  __memcpy_async(nram_h_high_temp, nram_w_low_temp,
                 num_deal_grid * sizeof(float), NRAM2NRAM);

  __bang_transpose(nram_grad_output_tl, nram_loc_w, num_points,
                   num_per_time_real * num_heads * num_levels);
  __memcpy_async(nram_loc_w, nram_grad_output_tl, num_deal_grid * sizeof(float),
                 NRAM2NRAM);
  __bang_transpose(nram_grad_output_tl, nram_loc_h, num_points,
                   num_per_time_real * num_heads * num_levels);
  __memcpy_async(nram_loc_h, nram_grad_output_tl, num_deal_grid * sizeof(float),
                 NRAM2NRAM);

  __bang_floor(nram_w_low, nram_loc_w, num_deal_grid);
  __bang_floor(nram_h_low, nram_loc_h, num_deal_grid);

  __bang_add_scalar(nram_h_high, nram_h_low, 1, num_deal_grid);
  __bang_add_scalar(nram_w_high, nram_w_low, 1, num_deal_grid);

  __bang_transpose(nram_h_low_ptr_offset, nram_h_low,
                   num_per_time_real * num_heads * num_levels, num_points);
  __bang_cycle_mul(nram_h_low_ptr_offset, nram_h_low_ptr_offset, nram_h_stride,
                   num_deal_grid, num_levels);

  __bang_cycle_add(nram_h_high_ptr_offset, nram_h_low_ptr_offset, nram_h_stride,
                   num_deal_grid, num_levels);

  __bang_transpose(nram_w_low_ptr_offset, nram_h_low_ptr_offset, num_points,
                   num_per_time_real * num_heads * num_levels);

  __memcpy_async(nram_h_low_ptr_offset, nram_w_low_ptr_offset,
                 num_deal_grid * sizeof(float), NRAM2NRAM);
  __bang_transpose(nram_w_low_ptr_offset, nram_h_high_ptr_offset, num_points,
                   num_per_time_real * num_heads * num_levels);
  __memcpy_async(nram_h_high_ptr_offset, nram_w_low_ptr_offset,
                 num_deal_grid * sizeof(float), NRAM2NRAM);
  __bang_mul_scalar(nram_w_low_ptr_offset, nram_w_low, qid_stride,
                    num_deal_grid);
  __bang_add_scalar(nram_w_high_ptr_offset, nram_w_low_ptr_offset, qid_stride,
                    num_deal_grid);

  __bang_add(nram_offset1, nram_h_low_ptr_offset, nram_w_low_ptr_offset,
             num_deal_grid);

  __bang_transpose(nram_offset_temp, nram_offset1,
                   num_per_time_real * num_heads, num_levels * num_points);
  __bang_cycle_add(nram_offset_temp, nram_offset_temp, nram_base_ptr,
                   num_deal_grid, num_heads);

  __bang_transpose(nram_offset1, nram_offset_temp, num_levels * num_points,
                   num_per_time_real * num_heads);

  __bang_add(nram_offset2, nram_h_low_ptr_offset, nram_w_high_ptr_offset,
             num_deal_grid);
  __bang_transpose(nram_offset_temp, nram_offset2,
                   num_per_time_real * num_heads, num_levels * num_points);
  __bang_cycle_add(nram_offset_temp, nram_offset_temp, nram_base_ptr,
                   num_deal_grid, num_heads);
  __bang_transpose(nram_offset2, nram_offset_temp, num_levels * num_points,
                   num_per_time_real * num_heads);

  __bang_add(nram_offset3, nram_h_high_ptr_offset, nram_w_low_ptr_offset,
             num_deal_grid);
  __bang_transpose(nram_offset_temp, nram_offset3,
                   num_per_time_real * num_heads, num_levels * num_points);
  __bang_cycle_add(nram_offset_temp, nram_offset_temp, nram_base_ptr,
                   num_deal_grid, num_heads);
  __bang_transpose(nram_offset3, nram_offset_temp, num_levels * num_points,
                   num_per_time_real * num_heads);
  __bang_add(nram_offset4, nram_h_high_ptr_offset, nram_w_high_ptr_offset,
             num_deal_grid);
  __bang_transpose(nram_offset_temp, nram_offset4,
                   num_per_time_real * num_heads, num_levels * num_points);
  __bang_cycle_add(nram_offset_temp, nram_offset_temp, nram_base_ptr,
                   num_deal_grid, num_heads);
  __bang_transpose(nram_offset4, nram_offset_temp, num_levels * num_points,
                   num_per_time_real * num_heads);

  // h_low >= 0 && w_low >= 0  mask2
  float *mask1 = nram_h_low_ptr_offset;
  float *mask2 = nram_h_high_ptr_offset;
  float *mask3 = nram_w_low_ptr_offset;
  float *mask4 = nram_w_high_ptr_offset;
  __bang_ge_scalar(mask1, nram_h_low, 0, num_deal_grid);
  __bang_ge_scalar(mask2, nram_w_low, 0, num_deal_grid);
  __bang_and(mask2, mask1, mask2, num_deal_grid);
  __bang_and(mask2, nram_h_high_temp, mask2, num_deal_grid);

  // h_low >= 0 && w_high <= width - 1 mask1
  __bang_transpose(mask3, nram_w_high,
                   num_per_time_real * num_heads * num_levels, num_points);
  __bang_sub_scalar(nram_spatial_shapes, nram_spatial_shapes, 1,
                    num_levels * 2);
  __bang_cycle_le(mask3, mask3, (float *)(nram_spatial_shapes + num_levels),
                  num_deal_grid, num_levels);
  __bang_transpose(mask4, mask3, num_points,
                   num_per_time_real * num_heads * num_levels);
  __bang_and(mask1, mask1, mask4, num_deal_grid);
  __bang_and(mask1, nram_h_high_temp, mask1, num_deal_grid);

  // h_high <= height - 1 && w_high <= width - 1 mask3
  __bang_transpose(mask3, nram_h_high,
                   num_per_time_real * num_heads * num_levels, num_points);
  __bang_cycle_le(mask3, mask3, (float *)(nram_spatial_shapes), num_deal_grid,
                  num_levels);
  __bang_transpose(nram_h_low_temp, mask3, num_points,
                   num_per_time_real * num_heads * num_levels);
  __bang_and(mask4, mask4, nram_h_low_temp, num_deal_grid);
  __bang_and(mask3, mask4, nram_h_high_temp, num_deal_grid);

  // h_high <= height - 1 && w_low >= 0 mask4
  __bang_ge_scalar(nram_w_low_temp, nram_w_low, 0, num_deal_grid);
  __bang_and(mask4, nram_h_low_temp, nram_w_low_temp, num_deal_grid);
  __bang_and(mask4, mask4, nram_h_high_temp, num_deal_grid);
  __bang_gt_scalar(grad_temp1, nram_offset1, 0, num_deal_grid);
  __bang_mul(nram_offset1, nram_offset1, grad_temp1, num_deal_grid);
  __bang_mul(nram_offset1, nram_offset1, mask2, num_deal_grid);

  __bang_gt_scalar(grad_temp1, nram_offset2, 0, num_deal_grid);
  __bang_mul(nram_offset2, nram_offset2, grad_temp1, num_deal_grid);
  __bang_mul(nram_offset2, nram_offset2, mask1, num_deal_grid);

  __bang_gt_scalar(grad_temp1, nram_offset3, 0, num_deal_grid);
  __bang_mul(nram_offset3, nram_offset3, grad_temp1, num_deal_grid);
  __bang_mul(nram_offset3, nram_offset3, mask4, num_deal_grid);

  __bang_gt_scalar(grad_temp1, nram_offset4, 0, num_deal_grid);
  __bang_mul(nram_offset4, nram_offset4, grad_temp1, num_deal_grid);
  __bang_mul(nram_offset4, nram_offset4, mask3, num_deal_grid);
  __sync_io_move_compute();
  __bang_sub(nram_lh, nram_loc_h, nram_h_low, num_deal_grid);
  __bang_sub(nram_lw, nram_loc_w, nram_w_low, num_deal_grid);

  __bang_fusion(FUSION_FMA, nram_hh, nram_lh, (float)(-1), 1, num_deal_grid);
  __bang_fusion(FUSION_FMA, nram_hw, nram_lw, (float)(-1), 1, num_deal_grid);

  __bang_mul(nram_w1, nram_hh, nram_hw, num_deal_grid);
  __bang_mul(nram_w2, nram_hh, nram_lw, num_deal_grid);
  __bang_mul(nram_w3, nram_lh, nram_hw, num_deal_grid);
  __bang_mul(nram_w4, nram_lh, nram_lw, num_deal_grid);
#endif
}

void __mlu_func__ loadValue(
    float *nram_grad_output_tl, float *nram_grad_output_tr,
    float *nram_grad_output_bl, float *nram_grad_output_br,
    const float *data_value, float *grad_temp1, float *grad_temp3, float *mask1,
    float *mask2, float *mask3, float *mask4, float *nram_offset1,
    float *nram_offset2, float *nram_offset3, float *nram_offset4,
    float *nram_grad_weight, int32_t *nram_level_start_index,
    const int32_t &offset_nram, const int32_t &num_heads,
    const int32_t &deal_num_real, const int32_t &num_deal_grid,
    const int32_t &num_query, const int32_t &num_levels,
    const int32_t &num_points, const int32_t &grid_offset,
    const int32_t &spatial_size, const int32_t &qid_stride) {
#if __BANG_ARCH__ >= 372
  int32_t value_offset_temp = 0;

#if __BANG_ARCH__ >= 592
  for (int i = 0; i < num_deal_grid; ++i) {
    int32_t b_col =
        (grid_offset + i) / num_query / num_heads / num_levels / num_points;
    int32_t l_col = (grid_offset + i) / num_points % num_levels;
    int32_t level_start_id = nram_level_start_index[l_col];
    value_offset_temp =
        b_col * spatial_size * qid_stride + level_start_id * qid_stride;
    ((float *)grad_temp1)[i] = value_offset_temp;
  }

  __bang_add(grad_temp3, grad_temp1, nram_offset1, num_deal_grid);
  __bang_add(grad_temp3 + num_deal_grid, grad_temp1, nram_offset2,
             num_deal_grid);
  __bang_add(grad_temp3 + 2 * num_deal_grid, grad_temp1, nram_offset3,
             num_deal_grid);
  __bang_add(grad_temp3 + 3 * num_deal_grid, grad_temp1, nram_offset4,
             num_deal_grid);
  __bang_mul_scalar(grad_temp3, grad_temp3, sizeof(float), 4 * num_deal_grid);
  __bang_float2int32((int32_t *)(grad_temp3), (grad_temp3), 4 * num_deal_grid,
                     0);
  __sync_io_move_compute();

  __gather_async((void *)nram_grad_output_tl, (void *)data_value,
                 (unsigned int *)grad_temp3, deal_num_real * sizeof(float),
                 GDRAM2NRAM, deal_num_real * sizeof(float), num_deal_grid);

  __gather_async((void *)nram_grad_output_tr, (void *)data_value,
                 (unsigned int *)(grad_temp3 + num_deal_grid),
                 deal_num_real * sizeof(float), GDRAM2NRAM,
                 deal_num_real * sizeof(float), num_deal_grid);

  __gather_async((void *)nram_grad_output_bl, (void *)data_value,
                 (unsigned int *)(grad_temp3 + 2 * num_deal_grid),
                 deal_num_real * sizeof(float), GDRAM2NRAM,
                 deal_num_real * sizeof(float), num_deal_grid);

  __gather_async((void *)nram_grad_output_br, (void *)data_value,
                 (unsigned int *)(grad_temp3 + 3 * num_deal_grid),
                 deal_num_real * sizeof(float), GDRAM2NRAM,
                 deal_num_real * sizeof(float), num_deal_grid);
  __sync_io_move_compute();

#else
  int32_t b_col =
      (grid_offset) / num_query / num_heads / num_levels / num_points;
  int32_t l_col = (grid_offset) / num_points % num_levels;
  int32_t level_start_id = nram_level_start_index[l_col];
  value_offset_temp =
      b_col * spatial_size * qid_stride + level_start_id * qid_stride;
  for (int32_t loop = 0; loop < num_deal_grid; ++loop) {
    __memcpy_async(
        (void *)(nram_grad_output_tl + loop * deal_num_real),
        (void *)(data_value + value_offset_temp + int32_t(nram_offset1[loop])),
        deal_num_real * sizeof(float), GDRAM2NRAM, offset_nram * sizeof(float),
        (int32_t(nram_offset2[loop]) - int32_t(nram_offset1[loop])) *
            sizeof(float),
        mask1[loop]);
    b_col = (grid_offset + loop + 1) / num_query / num_heads / num_levels /
            num_points;
    l_col = (grid_offset + loop + 1) / num_points % num_levels;
    level_start_id = nram_level_start_index[l_col];

    __memcpy_async(
        (void *)(nram_grad_output_bl + loop * deal_num_real),
        (void *)(data_value + value_offset_temp + int32_t(nram_offset3[loop])),
        deal_num_real * sizeof(float), GDRAM2NRAM, offset_nram * sizeof(float),
        (int32_t(nram_offset4[loop]) - int32_t(nram_offset3[loop])) *
            sizeof(float),
        mask3[loop]);
    value_offset_temp =
        b_col * spatial_size * qid_stride + level_start_id * qid_stride;
  }

#endif
  __bang_write_zero(grad_temp1, deal_num_real * num_deal_grid);
  __bang_cycle_add(grad_temp1, grad_temp1, mask2, deal_num_real * num_deal_grid,
                   num_deal_grid);
  __bang_transpose(grad_temp3, grad_temp1, deal_num_real, num_deal_grid);
  __nram__ int32_t table[2] = {0, (int32_t)0xffffffff};
  __bang_float2int32((int32_t *)grad_temp3, grad_temp3,
                     num_deal_grid * deal_num_real, 0);
  __bang_lut_s32((int32_t *)grad_temp3, (int32_t *)grad_temp3, (int32_t *)table,
                 num_deal_grid * deal_num_real, 64);
  __bang_write_zero(grad_temp1, deal_num_real * num_deal_grid);
  __bang_cycle_add(grad_temp1, grad_temp1, mask1, deal_num_real * num_deal_grid,
                   num_deal_grid);
  __sync_io_move_compute();

  __bang_band((char *)nram_grad_output_tl, (char *)nram_grad_output_tl,
              (char *)grad_temp3,
              num_deal_grid * deal_num_real * sizeof(float));
  __bang_transpose(grad_temp3, grad_temp1, deal_num_real, num_deal_grid);

  __bang_float2int32((int32_t *)grad_temp3, grad_temp3,
                     num_deal_grid * deal_num_real, 0);
  __bang_lut_s32((int32_t *)grad_temp3, (int32_t *)grad_temp3, (int32_t *)table,
                 num_deal_grid * deal_num_real, 64);
  __bang_band((char *)nram_grad_output_tr, (char *)nram_grad_output_tr,
              (char *)grad_temp3,
              num_deal_grid * deal_num_real * sizeof(float));

  __bang_write_zero(grad_temp1, deal_num_real * num_deal_grid);
  __bang_cycle_add(grad_temp1, grad_temp1, mask4, deal_num_real * num_deal_grid,
                   num_deal_grid);
  __bang_transpose(grad_temp3, grad_temp1, deal_num_real, num_deal_grid);

  __bang_float2int32((int32_t *)grad_temp3, grad_temp3,
                     num_deal_grid * deal_num_real, 0);
  __bang_lut_s32((int32_t *)grad_temp3, (int32_t *)grad_temp3, (int32_t *)table,
                 num_deal_grid * deal_num_real, 64);
  __bang_band((char *)nram_grad_output_bl, (char *)nram_grad_output_bl,
              (char *)grad_temp3,
              num_deal_grid * deal_num_real * sizeof(float));

  __bang_write_zero(grad_temp1, deal_num_real * num_deal_grid);
  __bang_cycle_add(grad_temp1, grad_temp1, mask3, deal_num_real * num_deal_grid,
                   num_deal_grid);
  __bang_transpose(grad_temp3, grad_temp1, deal_num_real, num_deal_grid);

  __bang_float2int32((int32_t *)grad_temp3, grad_temp3,
                     num_deal_grid * deal_num_real, 0);
  __bang_lut_s32((int32_t *)grad_temp3, (int32_t *)grad_temp3, (int32_t *)table,
                 num_deal_grid * deal_num_real, 64);
  __bang_band((char *)nram_grad_output_br, (char *)nram_grad_output_br,
              (char *)grad_temp3,
              num_deal_grid * deal_num_real * sizeof(float));
#endif
}

void __mlu_func__ computeGradValue(
    float *grad_temp1, float *grad_temp2, float *grad_temp3, float *grad_temp4,
    float *mask1, float *mask2, float *mask3, float *mask4, float *nram_offset1,
    float *nram_offset2, float *nram_offset3, float *nram_offset4,
    int32_t *nram_level_start_index, int32_t deal_num_real,
    const float *grad_value, float *nram_w1, float *nram_w2, float *nram_w3,
    float *nram_w4, const int32_t &num_per_time_real, const int32_t &num_heads,
    const int32_t &num_levels, const int32_t &num_points,
    const int32_t &num_query, const int32_t &num_deal_grid,
    const int32_t &grid_offset, const int32_t &spatial_size,
    const int32_t &qid_stride, float *nram_grid_offset1,
    float *nram_grid_offset2, const int32_t &batch, float *nram_grad_output_tl,
    float *nram_grad_output_tr, float *nram_grad_output_bl,
    float *nram_grad_output_br, float *nram_grad_weight) {
#if __BANG_ARCH__ >= 372
  __bang_write_zero(grad_temp1, deal_num_real * num_deal_grid);
  __bang_cycle_add(grad_temp1, grad_temp1, nram_grad_weight,
                   deal_num_real * num_deal_grid, num_deal_grid);
  __bang_transpose(grad_temp3, grad_temp1,
                   deal_num_real * num_per_time_real * num_heads,
                   num_levels * num_points);
  __bang_transpose(grad_temp1, grad_temp2, num_per_time_real * num_heads,
                   deal_num_real);
  __bang_cycle_mul(grad_temp3, grad_temp3, grad_temp1,
                   num_deal_grid * deal_num_real,
                   deal_num_real * num_per_time_real * num_heads);
  __bang_transpose(grad_temp4, grad_temp3, num_levels * num_points,
                   deal_num_real * num_per_time_real * num_heads);

  int32_t temp_res = num_query * num_heads * num_levels * num_points;
  for (int32_t loop = 0; loop < num_deal_grid; ++loop) {
    nram_grid_offset1[loop] = ((loop + grid_offset) / temp_res);
  }
  __bang_mul_scalar((float *)nram_grid_offset1, (float *)nram_grid_offset1,
                    spatial_size * qid_stride, num_deal_grid);
  __bang_transpose(nram_grid_offset2, nram_grid_offset1,
                   num_per_time_real * num_heads * num_levels, num_points);
  __bang_int322float((float *)nram_level_start_index, nram_level_start_index,
                     num_levels, 0);
  __bang_mul_scalar(nram_grid_offset1, (float *)nram_level_start_index,
                    qid_stride, num_levels);
  __bang_cycle_add(nram_grid_offset2, nram_grid_offset2, nram_grid_offset1,
                   num_deal_grid, num_levels);
  __bang_transpose(nram_grid_offset1, nram_grid_offset2, num_points,
                   num_per_time_real * num_heads * num_levels);
  __bang_add(nram_offset1, nram_offset1, nram_grid_offset1, num_deal_grid);
  __bang_add(nram_offset2, nram_offset2, nram_grid_offset1, num_deal_grid);
  __bang_add(nram_offset3, nram_offset3, nram_grid_offset1, num_deal_grid);
  __bang_add(nram_offset4, nram_offset4, nram_grid_offset1, num_deal_grid);

#if __BANG_ARCH__ >= 592
  // make sure offset not great than (batch * spatial_size * num_heads *
  // channels)
  __bang_lt_scalar(grad_temp1, nram_offset1,
                   batch * spatial_size * num_heads * deal_num_real,
                   num_deal_grid);
  __bang_mul(nram_offset1, nram_offset1, grad_temp1, num_deal_grid);
  __bang_lt_scalar(grad_temp1, nram_offset2,
                   batch * spatial_size * num_heads * deal_num_real,
                   num_deal_grid);
  __bang_mul(nram_offset2, nram_offset2, grad_temp1, num_deal_grid);
  __bang_lt_scalar(grad_temp1, nram_offset3,
                   batch * spatial_size * num_heads * deal_num_real,
                   num_deal_grid);
  __bang_mul(nram_offset3, nram_offset3, grad_temp1, num_deal_grid);
  __bang_lt_scalar(grad_temp1, nram_offset4,
                   batch * spatial_size * num_heads * deal_num_real,
                   num_deal_grid);
  __bang_mul(nram_offset4, nram_offset4, grad_temp1, num_deal_grid);
  __bang_mul(grad_temp3, nram_w1, mask2, num_deal_grid);
  __bang_cycle_mul(grad_temp1, grad_temp4, grad_temp3,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_transpose(grad_temp3, grad_temp1, deal_num_real, num_deal_grid);

  for (int32_t loop = 0; loop < num_deal_grid; ++loop) {
    __bang_atomic_reduce_add(
        (float *)(grad_value + int32_t(nram_offset1[loop])),
        (float *)(grad_temp3 + loop * deal_num_real), deal_num_real);
  }

  __bang_mul(grad_temp3, nram_w2, mask1, num_deal_grid);
  __bang_cycle_mul(grad_temp1, grad_temp4, grad_temp3,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_transpose(grad_temp3, grad_temp1, deal_num_real, num_deal_grid);

  for (int32_t loop = 0; loop < num_deal_grid; ++loop) {
    __bang_atomic_reduce_add(
        (float *)(grad_value + int32_t(nram_offset2[loop])),
        (float *)(grad_temp3 + loop * deal_num_real), deal_num_real);
  }
  __bang_mul(grad_temp3, nram_w3, mask4, num_deal_grid);
  __bang_cycle_mul(grad_temp1, grad_temp4, grad_temp3,
                   num_deal_grid * deal_num_real, num_deal_grid);

  __bang_transpose(grad_temp3, grad_temp1, deal_num_real, num_deal_grid);

  for (int32_t loop = 0; loop < num_deal_grid; ++loop) {
    __bang_atomic_reduce_add(
        (float *)(grad_value + int32_t(nram_offset3[loop])),
        (float *)(grad_temp3 + loop * deal_num_real), deal_num_real);
  }
  __bang_mul(grad_temp3, nram_w4, mask3, num_deal_grid);
  __bang_cycle_mul(grad_temp1, grad_temp4, grad_temp3,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_transpose(grad_temp3, grad_temp1, deal_num_real, num_deal_grid);

  for (int32_t loop = 0; loop < num_deal_grid; ++loop) {
    __bang_atomic_reduce_add(
        (float *)(grad_value + int32_t(nram_offset4[loop])),
        (float *)(grad_temp3 + loop * deal_num_real), deal_num_real);
  }
#else
  __bang_cycle_mul(grad_temp1, grad_temp4, nram_w1,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_transpose(nram_grad_output_br, grad_temp1, deal_num_real,
                   num_deal_grid);

  __bang_cycle_mul(grad_temp1, grad_temp4, nram_w2,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_transpose(nram_grad_output_tl, grad_temp1, deal_num_real,
                   num_deal_grid);

  __bang_cycle_mul(grad_temp1, grad_temp4, nram_w3,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_transpose(nram_grad_output_tr, grad_temp1, deal_num_real,
                   num_deal_grid);

  __bang_cycle_mul(grad_temp1, grad_temp4, nram_w4,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_transpose(nram_grad_output_bl, grad_temp1, deal_num_real,
                   num_deal_grid);
  for (int32_t loop = 0; loop < num_deal_grid; ++loop) {
    if (mask2[loop]) {
      __bang_atomic_reduce_add(
          (float *)(grad_value + int32_t(nram_offset1[loop])),
          (float *)(nram_grad_output_br + loop * deal_num_real), deal_num_real);
    }
    if (mask1[loop]) {
      __bang_atomic_reduce_add(
          (float *)(grad_value + int32_t(nram_offset2[loop])),
          (float *)(nram_grad_output_tl + loop * deal_num_real), deal_num_real);
    }
    if (mask4[loop]) {
      __bang_atomic_reduce_add(
          (float *)(grad_value + int32_t(nram_offset3[loop])),
          (float *)(nram_grad_output_tr + loop * deal_num_real), deal_num_real);
    }

    if (mask3[loop]) {
      __bang_atomic_reduce_add(
          (float *)(grad_value + int32_t(nram_offset4[loop])),
          (float *)(nram_grad_output_bl + loop * deal_num_real), deal_num_real);
    }
  }
#endif
#endif
}

void __mlu_func__ computeGradAttnWeight(
    float *grad_w_weight, float *grad_weight, float *nram_grad_output_tl,
    float *nram_grad_output_tr, float *nram_grad_output_bl,
    float *nram_grad_output_br, float *grad_temp2,
    const float *grad_attn_weight, float *nram_hw, float *nram_hh,
    float *nram_lw, float *nram_lh, float *grad_h_weight, float *nram_w1,
    float *nram_w2, float *nram_w3, float *nram_w4, const int32_t &offset_nram,
    const int32_t &num_deal_grid, const int32_t &deal_num_real,
    const int32_t &num_per_time_real, const int32_t &num_heads,
    const int32_t &num_levels, const int32_t &num_points,
    const int32_t &grid_offset, float *nram_h_high_temp) {
  __bang_write_zero(grad_w_weight, 2 * offset_nram);
  // grad_output_nram_tl
#if __BANG_ARCH__ >= 372
  __bang_transpose(grad_weight, nram_grad_output_tl, num_deal_grid,
                   deal_num_real);
  __bang_cycle_mul(nram_grad_output_tl, grad_weight, nram_hw,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_sub(grad_h_weight, grad_h_weight, nram_grad_output_tl,
             num_deal_grid * deal_num_real);
  __bang_cycle_mul(nram_grad_output_tl, grad_weight, nram_hh,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_sub(grad_w_weight, grad_w_weight, nram_grad_output_tl,
             num_deal_grid * deal_num_real);
  __bang_cycle_mul(nram_grad_output_tl, grad_weight, nram_w1,
                   num_deal_grid * deal_num_real, num_deal_grid);
  // nram_grad_output_tr
  __bang_transpose(grad_weight, nram_grad_output_tr, num_deal_grid,
                   deal_num_real);
  __bang_cycle_mul(nram_grad_output_tr, grad_weight, nram_lw,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_sub(grad_h_weight, grad_h_weight, nram_grad_output_tr,
             num_deal_grid * deal_num_real);
  __bang_cycle_mul(nram_grad_output_tr, grad_weight, nram_hh,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_add(grad_w_weight, grad_w_weight, nram_grad_output_tr,
             num_deal_grid * deal_num_real);
  __bang_cycle_mul(nram_grad_output_tr, grad_weight, nram_w2,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_add(nram_grad_output_tl, nram_grad_output_tl, nram_grad_output_tr,
             num_deal_grid * deal_num_real);

  // nram_grad_output_tl
  __bang_transpose(grad_weight, nram_grad_output_bl, num_deal_grid,
                   deal_num_real);
  __bang_cycle_mul(nram_grad_output_bl, grad_weight, nram_hw,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_add(grad_h_weight, grad_h_weight, nram_grad_output_bl,
             num_deal_grid * deal_num_real);
  __bang_cycle_mul(nram_grad_output_bl, grad_weight, nram_lh,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_sub(grad_w_weight, grad_w_weight, nram_grad_output_bl,
             num_deal_grid * deal_num_real);
  __bang_cycle_mul(nram_grad_output_bl, grad_weight, nram_w3,
                   num_deal_grid * deal_num_real, num_deal_grid);

  __bang_add(nram_grad_output_tl, nram_grad_output_tl, nram_grad_output_bl,
             num_deal_grid * deal_num_real);

  // nram_grad_output_br
  __bang_transpose(grad_weight, nram_grad_output_br, num_deal_grid,
                   deal_num_real);
  __bang_cycle_mul(nram_grad_output_br, grad_weight, nram_lw,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_add(grad_h_weight, grad_h_weight, nram_grad_output_br,
             num_deal_grid * deal_num_real);
  __bang_cycle_mul(nram_grad_output_br, grad_weight, nram_lh,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_add(grad_w_weight, grad_w_weight, nram_grad_output_br,
             num_deal_grid * deal_num_real);
  __bang_cycle_mul(nram_grad_output_br, grad_weight, nram_w4,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_add(nram_grad_output_tl, nram_grad_output_tl, nram_grad_output_br,
             num_deal_grid * deal_num_real);

  __bang_transpose(nram_grad_output_br, nram_grad_output_tl, deal_num_real,
                   num_deal_grid);
  __bang_transpose(nram_grad_output_tr, nram_grad_output_br,
                   num_per_time_real * num_heads,
                   num_points * num_levels * deal_num_real);
  __bang_transpose(grad_weight, grad_temp2, num_per_time_real * num_heads,
                   deal_num_real);
  __bang_cycle_mul(nram_grad_output_tr, nram_grad_output_tr, grad_weight,
                   num_deal_grid * deal_num_real,
                   num_per_time_real * num_heads * deal_num_real);
  __bang_transpose(nram_grad_output_br, nram_grad_output_tr,
                   num_points * num_levels * deal_num_real,
                   num_per_time_real * num_heads);
  __bang_transpose((float *)nram_grad_output_tr, (float *)nram_grad_output_br,
                   num_deal_grid, deal_num_real);

  recursiveSumPool(nram_grad_output_tr, num_deal_grid, deal_num_real,
                   ALIGN_NUM);

  __bang_float2int32((int32_t *)nram_h_high_temp, nram_h_high_temp,
                     num_deal_grid, 0);
  __nram__ int32_t table[2] = {0, (int32_t)0xffffffff};
  __bang_lut_s32((int32_t *)nram_h_high_temp, (int32_t *)nram_h_high_temp,
                 (int32_t *)table, num_deal_grid, 64);
  __bang_band((char *)nram_grad_output_tr, (char *)nram_grad_output_tr,
              (char *)nram_h_high_temp, num_deal_grid * sizeof(float));
  __bang_atomic_reduce_add((float *)grad_attn_weight + grid_offset,
                           (float *)nram_grad_output_tr, num_deal_grid);
#endif
}

void __mlu_func__ computeGradSampingLoc(
    const float *grad_sampling_loc, float *nram_grad_output_tl,
    float *nram_grad_output_tr, float *grad_h_weight, float *grad_w_weight,
    int32_t *nram_spatial_shapes, float *grad_temp1, float *grad_temp2,
    float *nram_grad_weight, const int32_t &num_deal_grid,
    const int32_t &deal_num_real, const int32_t &num_per_time_real,
    const int32_t &num_heads, const int32_t &num_levels,
    const int32_t &num_points, const int32_t &grid_offset,
    float *nram_h_high_temp) {
#if __BANG_ARCH__ >= 372
  __bang_transpose(nram_grad_output_tl, grad_h_weight,
                   num_per_time_real * num_heads * num_levels * deal_num_real,
                   num_points);  // pcxhl
  __bang_cycle_mul(nram_grad_output_tl, nram_grad_output_tl,
                   (float *)nram_spatial_shapes, num_deal_grid * deal_num_real,
                   num_levels);
  __bang_transpose(grad_h_weight, nram_grad_output_tl,
                   num_points * deal_num_real,
                   num_per_time_real * num_heads * num_levels);

  __bang_write_zero(grad_temp1, num_deal_grid * deal_num_real);
  __bang_cycle_add(grad_temp1, grad_temp1, nram_grad_weight,
                   num_deal_grid * deal_num_real, num_deal_grid);
  __bang_transpose(nram_grad_output_tr, grad_temp1,
                   deal_num_real * num_per_time_real * num_heads,
                   num_levels * num_points);
  __bang_transpose(grad_temp1, grad_temp2, num_per_time_real * num_heads,
                   deal_num_real);
  __bang_cycle_mul(nram_grad_output_tr, nram_grad_output_tr, grad_temp1,
                   num_deal_grid * deal_num_real,
                   deal_num_real * num_per_time_real * num_heads);
  __bang_transpose(grad_temp1, nram_grad_output_tr,
                   num_levels * num_points * deal_num_real,
                   num_per_time_real * num_heads);

  __bang_mul(grad_h_weight, grad_h_weight, grad_temp1,
             num_deal_grid * deal_num_real);
  __bang_transpose(nram_grad_output_tl, grad_h_weight, num_deal_grid,
                   deal_num_real);
  __memcpy_async(grad_h_weight, nram_grad_output_tl,
                 num_deal_grid * deal_num_real * sizeof(float), NRAM2NRAM);
  recursiveSumPool(grad_h_weight, num_deal_grid, deal_num_real, ALIGN_NUM);

  __nram__ int32_t table[2] = {0, (int32_t)0xffffffff};
  __bang_lut_s32((int32_t *)nram_h_high_temp, (int32_t *)nram_h_high_temp,
                 (int32_t *)table, num_deal_grid, 64);
  __bang_band((char *)grad_h_weight, (char *)grad_h_weight,
              (char *)nram_h_high_temp, num_deal_grid * sizeof(float));

  __bang_transpose(nram_grad_output_tl, grad_w_weight,
                   num_per_time_real * num_heads * num_levels * deal_num_real,
                   num_points);  // pcxhl
  __bang_cycle_mul(nram_grad_output_tl, nram_grad_output_tl,
                   (float *)(nram_spatial_shapes + num_levels),
                   num_deal_grid * deal_num_real, num_levels);
  __bang_transpose(grad_w_weight, nram_grad_output_tl,
                   num_points * deal_num_real,
                   num_per_time_real * num_heads * num_levels);

  __bang_mul(grad_w_weight, grad_w_weight, grad_temp1,
             num_deal_grid * deal_num_real);
  __bang_transpose(nram_grad_output_tl, grad_w_weight, num_deal_grid,
                   deal_num_real);
  __memcpy_async(grad_w_weight, nram_grad_output_tl,
                 num_deal_grid * deal_num_real * sizeof(float), NRAM2NRAM);
  recursiveSumPool(grad_w_weight, num_deal_grid, deal_num_real, ALIGN_NUM);
  __bang_lut_s32((int32_t *)nram_h_high_temp, (int32_t *)nram_h_high_temp,
                 (int32_t *)table, num_deal_grid, 64);
  __bang_band((char *)grad_w_weight, (char *)grad_w_weight,
              (char *)nram_h_high_temp, num_deal_grid * sizeof(float));

  __memcpy_async(grad_w_weight + num_deal_grid, grad_h_weight,
                 num_deal_grid * sizeof(float), NRAM2NRAM);
  __bang_transpose(nram_grad_output_tl, grad_w_weight, 2, num_deal_grid);
  __bang_atomic_reduce_add((float *)grad_sampling_loc + grid_offset * 2,
                           (float *)nram_grad_output_tl, 2 * num_deal_grid);
#endif
}

__mlu_global__ void MLUUnion1KernelMsDeformAttnBackwardSmallChannelsKernel(
    const float *data_value, const int32_t *spatial_shapes,
    const int32_t *data_level_start_index, const float *data_sampling_loc,
    const float *data_attn_weight, const float *grad_output,
    const int32_t batch, const int32_t spatial_size, const int32_t num_heads,
    const int32_t channels, const int32_t num_levels, const int32_t num_query,
    const int32_t num_points, float *grad_value, float *grad_sampling_loc,
    float *grad_attn_weight) {
  const int32_t split_grid_num = 28;
  const int32_t split_num_c = 8;
  const int32_t C_align = PAD_UP(channels, ALIGN_NUM);

  const int32_t num_hlp = num_heads * num_levels * num_points;
  int32_t num_per_time_theory =
      (MAX_NRAM_SIZE - num_levels * sizeof(float) -
       3 * PAD_UP(num_levels, 32) * sizeof(int32_t)) /
      sizeof(float) / (split_num_c * C_align + split_grid_num) / (num_hlp);

  int32_t deal_grid_num_theory = num_per_time_theory * num_hlp;

  const int32_t offset_nram = num_per_time_theory * C_align * num_hlp;
  const int32_t offset_nram_calc = PAD_UP(deal_grid_num_theory, ALIGN_NUM);
  float *nram_grad_output_tl = (float *)nram_buffer;
  float *nram_grad_output_tr = (float *)nram_buffer + offset_nram;
  float *nram_grad_output_bl = (float *)nram_buffer + 2 * offset_nram;
  float *nram_grad_output_br = (float *)nram_buffer + 3 * offset_nram;

  float *grad_temp1 = (float *)nram_buffer + 4 * offset_nram;
  float *grad_temp2 = (float *)nram_buffer + 5 * offset_nram;
  float *grad_temp3 = (float *)nram_buffer + 6 * offset_nram;
  float *grad_temp4 = (float *)nram_buffer + 7 * offset_nram;

  float *nram_loc_w = (float *)nram_buffer + split_num_c * offset_nram;
  float *nram_loc_h =
      (float *)nram_buffer + split_num_c * offset_nram + offset_nram_calc;
  float *nram_h_low =
      (float *)nram_buffer + split_num_c * offset_nram + 2 * offset_nram_calc;
  float *nram_w_low =
      (float *)nram_buffer + split_num_c * offset_nram + 3 * offset_nram_calc;
  float *nram_h_high =
      (float *)nram_buffer + split_num_c * offset_nram + 4 * offset_nram_calc;
  float *nram_w_high =
      (float *)nram_buffer + split_num_c * offset_nram + 5 * offset_nram_calc;
  float *nram_h_low_temp =
      (float *)nram_buffer + split_num_c * offset_nram + 6 * offset_nram_calc;
  float *nram_h_high_temp =
      (float *)nram_buffer + split_num_c * offset_nram + 7 * offset_nram_calc;

  float *nram_hw =
      (float *)nram_buffer + split_num_c * offset_nram + 8 * offset_nram_calc;
  float *nram_hh =
      (float *)nram_buffer + split_num_c * offset_nram + 9 * offset_nram_calc;
  float *nram_lw =
      (float *)nram_buffer + split_num_c * offset_nram + 10 * offset_nram_calc;
  float *nram_lh =
      (float *)nram_buffer + split_num_c * offset_nram + 11 * offset_nram_calc;

  float *nram_h_low_ptr_offset =
      (float *)nram_buffer + split_num_c * offset_nram + 12 * offset_nram_calc;
  float *nram_h_high_ptr_offset =
      (float *)nram_buffer + split_num_c * offset_nram + 13 * offset_nram_calc;
  float *nram_w_low_ptr_offset =
      (float *)nram_buffer + split_num_c * offset_nram + 14 * offset_nram_calc;
  float *nram_w_high_ptr_offset =
      (float *)nram_buffer + split_num_c * offset_nram + 15 * offset_nram_calc;

  float *nram_w1 =
      (float *)nram_buffer + split_num_c * offset_nram + 16 * offset_nram_calc;
  float *nram_w2 =
      (float *)nram_buffer + split_num_c * offset_nram + 17 * offset_nram_calc;
  float *nram_w3 =
      (float *)nram_buffer + split_num_c * offset_nram + 18 * offset_nram_calc;
  float *nram_w4 =
      (float *)nram_buffer + split_num_c * offset_nram + 19 * offset_nram_calc;

  float *nram_grad_weight =
      (float *)nram_buffer + split_num_c * offset_nram + 20 * offset_nram_calc;
  float *nram_base_ptr =
      (float *)nram_buffer + split_num_c * offset_nram + 21 * offset_nram_calc;
  float *nram_offset_temp =
      (float *)nram_buffer + split_num_c * offset_nram + 22 * offset_nram_calc;

  float *nram_offset1 =
      (float *)nram_buffer + split_num_c * offset_nram + 23 * offset_nram_calc;
  float *nram_offset2 =
      (float *)nram_buffer + split_num_c * offset_nram + 24 * offset_nram_calc;
  float *nram_offset3 =
      (float *)nram_buffer + split_num_c * offset_nram + 25 * offset_nram_calc;
  float *nram_offset4 =
      (float *)nram_buffer + split_num_c * offset_nram + 26 * offset_nram_calc;

  float *nram_w_low_temp =
      (float *)nram_buffer + split_num_c * offset_nram + 27 * offset_nram_calc;
  int32_t *nram_spatial_shapes =
      (int32_t *)((float *)nram_buffer + split_num_c * offset_nram +
                  28 * offset_nram_calc);
  int32_t *nram_level_start_index =
      (int32_t *)(nram_spatial_shapes + 2 * PAD_UP(num_levels, 32));
  float *nram_h_stride =
      (float *)(nram_level_start_index + 3 * PAD_UP(num_levels, 32));

  const int32_t total_num = batch * num_query;
  int32_t num_per_core = total_num / taskDim;
  int32_t num_rem = total_num % taskDim;
  num_per_core = num_per_core + int32_t(taskId < num_rem);
  num_per_time_theory =
      num_per_core > num_per_time_theory ? num_per_time_theory : num_per_core;
  int32_t num_deal_grid = num_per_time_theory * num_hlp;

  if (num_per_core == 0) return;
  int32_t start_per_core = num_rem > taskId ? (taskId * num_per_core)
                                            : (num_rem + taskId * num_per_core);

  const int32_t qid_stride = num_heads * channels;
  int32_t deal_num_real = channels;

  const int32_t repeat_times = num_per_core / num_per_time_theory;
  const int32_t tail_num = num_per_core % num_per_time_theory;

  int32_t num_per_time_real = num_per_time_theory;

  for (int32_t loop = 0; loop < num_heads; ++loop) {
    nram_base_ptr[loop] = loop * channels;
  }
  const int32_t w_stride = num_heads * channels;

  for (int32_t grid_loop = 0; grid_loop < repeat_times + 1; ++grid_loop) {
    int32_t grid_offset =
        (start_per_core + grid_loop * num_per_time_theory) * num_hlp;
    if (grid_loop == repeat_times) {
      if (tail_num == 0) {
        continue;
      } else {
        grid_offset =
            (start_per_core + repeat_times * num_per_time_theory) * num_hlp;
        num_per_time_real = tail_num;
        num_deal_grid = tail_num * num_hlp;
      }
    }
    __memcpy_async(nram_spatial_shapes, spatial_shapes,
                   num_levels * 2 * sizeof(int32_t), GDRAM2NRAM);

    __memcpy_async(nram_loc_w, data_sampling_loc + grid_offset * 2,
                   num_deal_grid * 2 * sizeof(float), GDRAM2NRAM);

    __sync_io_move_compute();
    __memcpy_async(nram_grad_weight, data_attn_weight + grid_offset,
                   num_deal_grid * sizeof(float), GDRAM2NRAM);
    __memcpy_async(nram_level_start_index, data_level_start_index,
                   num_levels * sizeof(int32_t), GDRAM2NRAM);
    computeGridMaskAndOffset(
        nram_grad_output_tl, nram_grad_output_tr, nram_loc_w, nram_loc_h,
        nram_h_stride, nram_spatial_shapes, nram_w_low_temp, nram_h_high_temp,
        nram_w_low, nram_h_low, nram_h_high, nram_w_high, nram_lh, nram_lw,
        nram_hh, nram_hw, nram_h_low_ptr_offset, nram_h_high_ptr_offset,
        nram_w_low_ptr_offset, nram_w_high_ptr_offset, nram_w1, nram_w2,
        nram_w3, nram_w4, nram_offset_temp, nram_offset1, nram_offset2,
        nram_offset3, nram_offset4, nram_base_ptr, nram_h_low_temp,
        num_deal_grid, num_per_time_real, num_heads, num_levels, num_points,
        w_stride, qid_stride, grad_temp1);
    float *mask1 = nram_h_low_ptr_offset;
    float *mask2 = nram_h_high_ptr_offset;
    float *mask3 = nram_w_low_ptr_offset;
    float *mask4 = nram_w_high_ptr_offset;
    __memcpy_async(
        grad_temp2,
        grad_output + (start_per_core + grid_loop * num_per_time_theory) *
                          num_heads * deal_num_real,
        num_per_time_real * num_heads * deal_num_real * sizeof(float),
        GDRAM2NRAM);
    loadValue(nram_grad_output_tl, nram_grad_output_tr, nram_grad_output_bl,
              nram_grad_output_br, data_value, grad_temp1, grad_temp3, mask1,
              mask2, mask3, mask4, nram_offset1, nram_offset2, nram_offset3,
              nram_offset4, nram_grad_weight, nram_level_start_index,
              offset_nram, num_heads, deal_num_real, num_deal_grid, num_query,
              num_levels, num_points, grid_offset, spatial_size, qid_stride);

    // compute grad_weight
    float *grad_weight = grad_temp1;
    float *grad_h_weight = grad_temp4;
    float *grad_w_weight = grad_temp3;
    computeGradAttnWeight(
        grad_w_weight, grad_weight, nram_grad_output_tl, nram_grad_output_tr,
        nram_grad_output_bl, nram_grad_output_br, grad_temp2, grad_attn_weight,
        nram_hw, nram_hh, nram_lw, nram_lh, grad_h_weight, nram_w1, nram_w2,
        nram_w3, nram_w4, offset_nram, num_deal_grid, deal_num_real,
        num_per_time_real, num_heads, num_levels, num_points, grid_offset,
        nram_h_high_temp);

    // compute grad_sampling_loc
    computeGradSampingLoc(grad_sampling_loc, nram_grad_output_tl,
                          nram_grad_output_tr, grad_h_weight, grad_w_weight,
                          nram_spatial_shapes, grad_temp1, grad_temp2,
                          nram_grad_weight, num_deal_grid, deal_num_real,
                          num_per_time_real, num_heads, num_levels, num_points,
                          grid_offset, nram_h_high_temp);

    float *nram_grid_offset1 = nram_loc_h;
    float *nram_grid_offset2 = nram_loc_w;
    computeGradValue(
        grad_temp1, grad_temp2, grad_temp3, grad_temp4, mask1, mask2, mask3,
        mask4, nram_offset1, nram_offset2, nram_offset3, nram_offset4,
        nram_level_start_index, deal_num_real, grad_value, nram_w1, nram_w2,
        nram_w3, nram_w4, num_per_time_real, num_heads, num_levels, num_points,
        num_query, num_deal_grid, grid_offset, spatial_size, qid_stride,
        nram_grid_offset1, nram_grid_offset2, batch, nram_grad_output_tl,
        nram_grad_output_tr, nram_grad_output_bl, nram_grad_output_br,
        nram_grad_weight);
  }
}

void MLUOP_WIN_API KernelMsDeformAttnBackwardSmallChannels(
    cnrtDim3_t k_dim, cnrtFunctionType_t k_type, cnrtQueue_t queue,
    const float *data_value, const int32_t *spatial_shapes,
    const int32_t *data_level_start_index, const float *data_sampling_loc,
    const float *data_attn_weight, const float *grad_output,
    const int32_t batch, const int32_t spatial_size, const int32_t num_heads,
    const int32_t channels, const int32_t num_levels, const int32_t num_query,
    const int32_t num_points, float *grad_value, float *grad_sampling_loc,
    float *grad_attn_weight) {
  MLUUnion1KernelMsDeformAttnBackwardSmallChannelsKernel<<<k_dim, k_type,
                                                           queue>>>(
      data_value, spatial_shapes, data_level_start_index, data_sampling_loc,
      data_attn_weight, grad_output, batch, spatial_size, num_heads, channels,
      num_levels, num_query, num_points, grad_value, grad_sampling_loc,
      grad_attn_weight);
}
