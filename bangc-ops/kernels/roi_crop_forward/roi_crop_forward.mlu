/*************************************************************************
 * Copyright (C) 2022 by Cambricon, Inc. All rights reserved.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#include "roi_crop_forward.h"
#include <string>
#include <iostream>
#include "core/context.h"
#include "core/logging.h"
#include "core/runtime/device.h"
#include "core/tensor.h"
#include "core/type.h"
#include "kernels/binary_op/binary_op_host.h"
#include "kernels/kernel.h"
#include "mlu_op.h"

static void policyFunc(const mluOpHandle_t &handle, int bin_num,
                       cnrtDim3_t *k_dim, cnrtFunctionType_t *k_type) {
  unsigned int cluster_num = mluop::runtime::getClusterLimitCapability(handle);
  unsigned int core_in_cluster = handle->core_num_per_cluster;
  *k_type = CNRT_FUNC_TYPE_UNION1;
  k_dim->x = core_in_cluster;
  unsigned int use_cluster = (bin_num + core_in_cluster - 1) / core_in_cluster;
  k_dim->y = use_cluster > cluster_num ? cluster_num : use_cluster;
  k_dim->z = 1;
}

/* user param check
 * step1:check desc and data ptr is not nullptr_t
 * step2:check shape and data type
 * step3:check zero element
 * */
mluOpStatus_t RoiCropForwardParamCheck(
    const std::string &op_name, const mluOpHandle_t &handle,
    const mluOpTensorDescriptor_t &input_desc, const void *input,
    const mluOpTensorDescriptor_t &grid_desc, const void *grid,
    const mluOpTensorDescriptor_t &output_desc, const void *output) {
  // check descriptor and data
  PARAM_CHECK(op_name, handle != NULL);
  PARAM_CHECK(op_name, input_desc != NULL);
  PARAM_CHECK(op_name, grid_desc != NULL);
  PARAM_CHECK(op_name, output_desc != NULL);
  // check data type
  PARAM_CHECK(op_name, input_desc->dtype == MLUOP_DTYPE_FLOAT);
  PARAM_CHECK(op_name, grid_desc->dtype == MLUOP_DTYPE_FLOAT);
  PARAM_CHECK(op_name, output_desc->dtype == MLUOP_DTYPE_FLOAT);
  // check shape and layout
  PARAM_CHECK(op_name, input_desc->layout == MLUOP_LAYOUT_NHWC);
  PARAM_CHECK(op_name, grid_desc->layout == MLUOP_LAYOUT_NHWC);
  PARAM_CHECK(op_name, output_desc->layout == MLUOP_LAYOUT_NHWC);
  if (output_desc->dims[0] != grid_desc->dims[0] ||
      output_desc->dims[1] != grid_desc->dims[1] ||
      output_desc->dims[2] != grid_desc->dims[2]) {
    LOG(ERROR) << op_name << ":Check failed: output_desc->dims[0]/[1]/[2] "
                             "should be equal to grid_desc->dims[0]/[1]/[2].";
    return MLUOP_STATUS_BAD_PARAM;
  }
  if (output_desc->dims[3] != input_desc->dims[3]) {
    LOG(ERROR) << op_name << ":Check failed: output_desc->dims[3] should be "
                             "equal to input_desc->dims[3].";
    return MLUOP_STATUS_BAD_PARAM;
  }
  if (grid_desc->dims[3] != 2) {
    LOG(ERROR) << op_name
               << ":Check failed: grid_desc->dims[3] should be equal to 2.";
    return MLUOP_STATUS_BAD_PARAM;
  }
  // check zero element
  if ((mluOpGetTensorElementNum(input_desc) == 0) ||
      (mluOpGetTensorElementNum(grid_desc) == 0) ||
      (mluOpGetTensorElementNum(output_desc) == 0)) {
    VLOG(5) << op_name << " skip zero element tensor.";
    zero_element = true;
    return MLUOP_STATUS_BAD_PARAM;
  }
  PARAM_CHECK(op_name, input != NULL);
  PARAM_CHECK(op_name, grid != NULL);
  PARAM_CHECK(op_name, output != NULL);
  return MLUOP_STATUS_SUCCESS;
}

mluOpStatus_t MLUOP_WIN_API mluOpRoiCropForward(
    mluOpHandle_t handle, const mluOpTensorDescriptor_t input_desc,
    const void *input, const mluOpTensorDescriptor_t grid_desc,
    const void *grid, const mluOpTensorDescriptor_t output_desc, void *output) {
  // check params
  bool zero_element = false;
  mluOpStatus_t param_check = RoiCropForwardParamCheck(
      "[mluOpRoiCropForward]", handle, input_desc, input, grid_desc, grid,
      output_desc, output);
  if (zero_element == true) {
    return MLUOP_STATUS_SUCCESS;
  }
  if (param_check != MLUOP_STATUS_SUCCESS) {
    return param_check;
  }

  uint32_t batch = input_desc->dims[0];
  uint32_t height = input_desc->dims[1];
  uint32_t width = input_desc->dims[2];
  uint32_t channels = input_desc->dims[3];
  uint32_t grid_n = grid_desc->dims[0];
  uint32_t output_h = output_desc->dims[1];
  uint32_t output_w = output_desc->dims[2];
  uint32_t bin_num = grid_n * output_h * output_w;
  cnrtDim3_t k_dim;
  cnrtFunctionType_t k_type;

  policyFunc(handle, bin_num, &k_dim, &k_type);

  mluOpDataType_t data_type = input_desc->dtype;
  VLOG(5) << "[mluOpRoiCropForward] launch kernel policyFUnc[" << k_dim.x
          << ", " << k_dim.y << ", " << k_dim.z << "].";
  KERNEL_CHECK((MLUKernelRoiCropForward<<<k_dim, k_type, handle->queue>>>(
      input, batch, height, width, channels, grid, grid_n, output, output_h,
      output_w, data_type)));
  return MLUOP_STATUS_SUCCESS;
}

