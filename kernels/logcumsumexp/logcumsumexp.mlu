/*******************************************************************************
 * Copyright (C) [2023] by Cambricon, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modif y, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS for A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS self.tcp LIABLE for ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *******************************************************************************/
#include "logcumsumexp.h"
#include <algorithm>
#include "core/logging.h"
#include "kernels/debug.h"
#include "kernels/kernel.h"
#include "kernels/utils/common.h"
#include "kernels/log/log.h"
#include "kernels/logcumsumexp/compute_kernels/dim_one.h"
#include "kernels/logcumsumexp/compute_kernels/highest_dim.h"
#include "kernels/logcumsumexp/compute_kernels/lowest_dim.h"
#include "kernels/logcumsumexp/compute_kernels/mid_dim.h"

#define SEG_L 128
#define CoreCapacity 327680
#define DimOneDealLength 147456 // memory length of one NRAM in dim-one
#define ClusterCapacity 1310720

__nram__ char nram_buffer[MAX_NRAM_SIZE];
__mlu_shared__ char sram_buffer[MAX_SRAM_SIZE];

#if 0
// removing the dependence between columns
template <typename T>
__mlu_func__ void removeDataDependence(T *dst,
                                       T *src,
                                       int deal_num,
                                       int unroll_num) {
  if (deal_num < unroll_num) {
    __bang_add(dst, dst, src, deal_num);
    return;
  }
  int deal_per_loop = deal_num / unroll_num;
  int rem_num       = deal_num % unroll_num;
  for (int i = 0; i < unroll_num; i++) {
    int deal_this_loop = (i == unroll_num - 1) ?
      deal_per_loop + rem_num : deal_per_loop;
    __bang_add(dst + deal_per_loop * i, dst + deal_per_loop * i,
               src + deal_per_loop * i, deal_this_loop);
  }
}

// cumsum execution
template <typename T>
__mlu_func__ void dimOneCumsum(T *src0_nram, T *src1_nram, int deal_length) {
  int seg_num = (deal_length + SEG_L - 1) / SEG_L;
  T pre_sum = 0;
  __bang_transpose(src1_nram, src0_nram, SEG_L, seg_num);

  for (int j = 1; j < seg_num; j++) {
    removeDataDependence(src1_nram + j * SEG_L,
                         src1_nram + (j - 1) * SEG_L,
                         SEG_L, 4);
  }
  src0_nram[0]  = pre_sum;
  int index_offset = (seg_num - 1) * SEG_L - 1;
  for (int k = 1; k < SEG_L; k++) {
    pre_sum      = pre_sum + src1_nram[index_offset + k];
    src0_nram[k] = pre_sum;
  }
  __bang_cycle_add(src1_nram, src1_nram, src0_nram, seg_num * SEG_L, SEG_L);
  __bang_transpose(src0_nram, src1_nram, seg_num, SEG_L);
}

// inclusive execution
template <typename T>
__mlu_func__ void inclusiveScan(T* nram_src,
                                const int32_t data_size,
                                bool offset_type,
                                T* cluster_offsets,
                                T cum_offset) {
  // parameter preparing
  int32_t IdInCluster = taskId % 4;
  __mlu_shared__ T core_offsets[4];
  T *nram_src0 = nram_src;
  T *nram_src1 = nram_src0 + DimOneDealLength / sizeof(T);

  __mluop_exp(nram_src0, nram_src0, nullptr, 0, data_size);
  dimOneCumsum(nram_src0, nram_src1, data_size);
  core_offsets[IdInCluster] = nram_src0[data_size-1];
  __sync_cluster();


  // offset between coresx
  if (IdInCluster == 0) {
    core_offsets[1] = core_offsets[0] + core_offsets[1];
    core_offsets[3] = core_offsets[1] + core_offsets[2];
    core_offsets[2] = core_offsets[1];
    core_offsets[1] = core_offsets[0];
    core_offsets[0] = 0;
  }
  __sync_cluster();
  __bang_add_scalar(nram_src0, nram_src0,
                    core_offsets[IdInCluster], data_size);

  // offset between clusters
  // "offset_type 0" means calculate the offset with the array
  // that pointed by "cluster_offsets"
  // "offset_type 1" means calculate the offset with the scalar "cum_offset"
  if (offset_type == 0) {
    if (IdInCluster == 3)
      cluster_offsets[clusterId] = nram_src0[data_size - 1];
    __sync_all();
    if (taskId == 0) {
      cluster_offsets[8] = cluster_offsets[7];
      cluster_offsets[7] = cluster_offsets[6];
      cluster_offsets[6] = cluster_offsets[5];
      cluster_offsets[5] = cluster_offsets[4];
      cluster_offsets[4] = cluster_offsets[3];
      cluster_offsets[3] = cluster_offsets[2];
      cluster_offsets[2] = cluster_offsets[1];
      cluster_offsets[1] = cluster_offsets[0];
      cluster_offsets[0] = cluster_offsets[9];
      cluster_offsets[1] = cluster_offsets[0] + cluster_offsets[1];
      cluster_offsets[2] = cluster_offsets[1] + cluster_offsets[2];
      cluster_offsets[3] = cluster_offsets[2] + cluster_offsets[3];
      cluster_offsets[4] = cluster_offsets[3] + cluster_offsets[4];
      cluster_offsets[5] = cluster_offsets[4] + cluster_offsets[5];
      cluster_offsets[6] = cluster_offsets[5] + cluster_offsets[6];
      cluster_offsets[7] = cluster_offsets[6] + cluster_offsets[7];
      cluster_offsets[9] = cluster_offsets[7] + cluster_offsets[8];
    }
    __sync_all();
    __bang_add_scalar(nram_src0, nram_src0,
                      cluster_offsets[clusterId], data_size);
  }
  if (offset_type == 1)__bang_add_scalar(nram_src0, nram_src0,
                                         cum_offset, data_size);
  // log computing
  __mluop_log(nram_src0, nram_src0, nullptr, 0, data_size);
}

// one dimension executing kernel==========================================
template <typename T>
__mlu_func__ void
dimOneKernel_unino8(const T *input,
                    T *result,
                    int32_t data_size) {
    // parameters preparing
    int32_t dataCore = DimOneDealLength / sizeof(T);
    int32_t dataCluster = dataCore * 4;
    int32_t data_per_round = dataCluster * 8;
    int32_t rounds = (data_size + data_per_round - 1) / data_per_round;
    int32_t last_round_length = data_size - (rounds - 1) * data_per_round;

    T *cluster_offsets = result + data_size - 10;
    cluster_offsets[9] = 0;
    // [0],[1],[2]...[7] are 8 offsets for 8 clusters;
    // the [9] records and saves the cumulative offset for next round;
    // the [8] is to support the offset calculate;
    T basenum = 0;
    int32_t round = 0;
    T *sram_src0 = (T *)sram_buffer;
    T *sram_src1 = (T *)(sram_buffer + ClusterCapacity);
    T *nram_src0 = (T *)nram_buffer;
    T *nram_src1 = nram_src0 + ((MAX_NRAM_SIZE/sizeof(T)) >> 1);
    int32_t totalId = clusterId;
    int32_t IdInCluster = taskId % 4;
    int32_t last_round_clusters
      = (last_round_length + dataCluster - 1) / dataCluster;
    int32_t last_cluster_length
      = last_round_length - (last_round_clusters - 1) * dataCluster;
    int32_t LastCoresLength = last_cluster_length >> 2;
    int32_t length_offset = last_cluster_length % 4;
    int32_t copy_offset = IdInCluster * LastCoresLength
      + __mluop_min(IdInCluster, length_offset);
    if (IdInCluster < length_offset)LastCoresLength += 1;
    int32_t padding_length = (LastCoresLength + SEG_L - 1) / SEG_L * SEG_L;

    // first memory copy GDRAM2SRAM
    if (rounds > 1)__memcpy(nram_src0,
      input + totalId * dataCluster + IdInCluster * dataCore,
      dataCore * sizeof(T), GDRAM2NRAM);

    T *thisSram = sram_src0;
    T *nextSram = sram_src1;
    T *thisNram = nram_src0;
    T *nextNram = nram_src1;

    // pipeline execute
    while (round < rounds - 1) {
      totalId = round * 8 + clusterId;
      thisSram = (round % 2 == 0) ? sram_src0 : sram_src1;
      nextSram = (round % 2 == 0) ? sram_src1 : sram_src0;
      thisNram = (round % 2 == 0) ? nram_src0 : nram_src1;
      nextNram = (round % 2 == 0) ? nram_src1 : nram_src0;
      // data copy for next round
      if (round < rounds - 2) {
        __memcpy_async(nextNram,
          input + (totalId + 8) * dataCluster + dataCore * IdInCluster,
          dataCore * sizeof(T), GDRAM2NRAM);
      } else {
        if (clusterId < last_round_clusters - 1) {
          __memcpy_async(nextNram,
            input + (totalId + 8) * dataCluster + IdInCluster * dataCore,
            dataCore * sizeof(T), GDRAM2NRAM);
        } else if (clusterId == last_round_clusters - 1) {
          __bang_write_zero(nextNram, padding_length);
          __memcpy_async(nextNram,
            input + (totalId + 8) * dataCluster + copy_offset,
            LastCoresLength * sizeof(T), GDRAM2NRAM);
        }
      }
      // compute
      inclusiveScan(thisNram, dataCore, 0, cluster_offsets, basenum);
      __memcpy(thisSram + dataCore * IdInCluster, thisNram,
               dataCore * sizeof(T), NRAM2SRAM);
      __sync_cluster();
      __memcpy_async(result + totalId * dataCluster + dataCore * IdInCluster,
                     thisSram + dataCore * IdInCluster,
                     dataCluster * sizeof(T), SRAM2GDRAM);
      round++;
    }

    thisSram = nextSram;
    thisNram = nextNram;
    totalId = round * 8 + clusterId;
    // the last round

    if (last_round_clusters == 1) {
      if (clusterId == 0) {
        if (rounds == 1) {
          __bang_write_zero(thisNram, padding_length);
          __memcpy(thisNram, input + totalId * dataCluster + copy_offset,
                   LastCoresLength * sizeof(T), GDRAM2NRAM);
        }
        inclusiveScan(thisNram, LastCoresLength, 1,
                      result, cluster_offsets[9]);
        __memcpy(result + totalId * dataCluster + copy_offset, thisNram,
                 LastCoresLength * sizeof(T), NRAM2GDRAM);
      }
    } else {
      if (clusterId < last_round_clusters - 1) {
        if (rounds == 1)__memcpy(thisNram,
          input + totalId * dataCluster + IdInCluster * dataCore,
          dataCore * sizeof(T), GDRAM2NRAM);
        inclusiveScan(thisNram, dataCore, 0, cluster_offsets, basenum);
        __memcpy(result + totalId * dataCluster + IdInCluster * dataCore,
                 thisNram, dataCore * sizeof(T), NRAM2GDRAM);
      } else if (clusterId == last_round_clusters - 1) {
        if (rounds == 1) {
          __bang_write_zero(thisNram, padding_length);
          __memcpy(thisNram, input + totalId * dataCluster + copy_offset,
                   LastCoresLength * sizeof(T), GDRAM2NRAM);
        }
        inclusiveScan(thisNram, LastCoresLength, 0,
                      cluster_offsets, basenum);
        __memcpy(result + totalId * dataCluster + copy_offset, thisNram,
                 LastCoresLength * sizeof(T), NRAM2GDRAM);
      } else {
        __sync_all();
        __sync_all();
      }
    }
}

// LCSE execution for small part
template <typename T>
__mlu_func__ void smallPartLCSE(T *result,
                                const T *source,
                                int32_t data_size,
                                int32_t axis_size,
                                int32_t parts) {
    T *nram_src0 = (T *)nram_buffer;
    T *nram_src1 = nram_src0 + ((MAX_NRAM_SIZE/sizeof(T)) >> 1);
    int32_t part_width = axis_size;
    int32_t part_height = SEG_L / sizeof(T);
    int32_t part_size = part_width * part_height;

    __memcpy(nram_src0, source, data_size * sizeof(T), GDRAM2NRAM);
    __mluop_exp(nram_src0, nram_src0, nullptr, 0, data_size);
    for (int i = 0; i < parts; i++) {
        __bang_transpose(nram_src1 + part_size * i,
                         nram_src0 + part_size * i,
                         part_height, part_width);
    }
    for (int i = 0; i < parts; i++) {
        for (int j = 1; j < part_width; j++) {
            __bang_add(nram_src1 + part_size * i + part_height * j,
                       nram_src1 + part_size * i + part_height * j,
                       nram_src1 + part_size * i + part_height * (j - 1),
                       part_height);
        }
    }
    for (int i = 0; i < parts; i++) {
        __bang_transpose(nram_src0 + part_size * i,
                         nram_src1 + part_size * i,
                         part_width, part_height);
    }
    __mluop_log(nram_src0, nram_src0, nullptr, 0, data_size);
    __memcpy(result, nram_src0, data_size * sizeof(T), NRAM2GDRAM);
}

// highest dimension executing kernel====================================
template <typename T>
__mlu_func__ void
highestDimKernel_unino8(const T *input,
                        T *result,
                        int32_t axis_size,
                        int32_t lower_size) {
    // if nram_size > part_size,
    // there will be several parts on one nram every round;
    // if nram_size < part_size, call dimOneKernel for batches.
    int32_t data_size = axis_size * lower_size;
    int32_t nram_size = CoreCapacity / sizeof(T);
    int32_t nram_height = SEG_L / sizeof(T);
    int32_t nram_width = nram_size / nram_height;
    int32_t part_height = nram_height;
    int32_t part_width = axis_size;
    int32_t parts_per_core = nram_width / part_width;

    if (parts_per_core == 0) {
      for (int batch = 0; batch < lower_size; batch++)
        dimOneKernel_unino8(input + axis_size * batch,
                            result + axis_size * batch,
                            axis_size);
    } else {
      int32_t deal_size = parts_per_core * part_width * part_height;
      int32_t round_size = deal_size * taskDim;
      int32_t round = 0;
      int32_t deal_rounds = (data_size + round_size - 1) / round_size;
      while (round < deal_rounds - 1) {
          smallPartLCSE(result + round * round_size + taskId * deal_size,
                        input + round * round_size + taskId * deal_size,
                        deal_size, axis_size, parts_per_core);
          round++;
      }
      // last round
      int32_t last_round_size = data_size - round_size * (deal_rounds - 1);
      int32_t last_round_height = last_round_size / part_width;
      int32_t last_round_parts
        = (last_round_height + part_height - 1) / part_height;
      int32_t lastRoundCores
        = (last_round_parts + parts_per_core - 1)/ parts_per_core;
      int32_t last_core_size
        = data_size - (deal_rounds - 1) * round_size
        - (lastRoundCores - 1) * deal_size;

      if (taskId < lastRoundCores - 1) {
        smallPartLCSE(result + round * round_size + taskId * deal_size,
                      input + round * round_size + taskId * deal_size,
                      deal_size, axis_size, parts_per_core);
      } else if (taskId == lastRoundCores - 1) {
          T *nram_src = (T *)nram_buffer;
          __bang_write_zero(nram_src, deal_size);
          smallPartLCSE(result + round * round_size + taskId * deal_size,
                        input + round * round_size + taskId * deal_size,
                        last_core_size, axis_size, parts_per_core);
      }
    }
}

// LCSE execution for rows
template <typename T>
__mlu_func__ void
rowsKernel(T *result, const T *source, int32_t deal_length,
           int32_t rounds, int32_t address_offset) {
  T *nram_src0 = (T *)nram_buffer;
  T *nram_src1 = nram_src0 + deal_length;
  T *nram_out = nram_src0 + ((MAX_NRAM_SIZE/sizeof(T)) >> 1);
  int32_t part_length = deal_length / 4;
  int32_t length_offset = deal_length % 4;
  int32_t copy_offset = taskId * part_length + __mluop_min(taskId, length_offset);
  if (taskId < length_offset)part_length += 1;

  T *thisNram = nram_src1;
  T *lastNram = nram_src0;
  __memcpy(lastNram, source + copy_offset,
           part_length * sizeof(T), GDRAM2NRAM);
  __memcpy(result + copy_offset, lastNram,
           part_length * sizeof(T), NRAM2GDRAM);
  __mluop_exp(lastNram, lastNram, nullptr, 0, part_length);

  for (int i = 1; i < rounds; i++) {
    thisNram = (i % 2 == 0) ? nram_src0 : nram_src1;
    lastNram = (i % 2 == 0) ? nram_src1 : nram_src0;
    __memcpy(thisNram,
             source + i * address_offset + copy_offset,
             part_length * sizeof(T),
             GDRAM2NRAM);
    __mluop_exp(thisNram, thisNram, nullptr, 0, part_length);
    __bang_add(thisNram, thisNram, lastNram, part_length);
    __mluop_log(nram_out, thisNram, nullptr, 0, part_length);
    __memcpy(result + i * address_offset + copy_offset,
             nram_out,
             part_length * sizeof(T),
             NRAM2GDRAM);
    __sync_cluster();
  }
}

// LCSE execution for one part
template <typename T>
__mlu_func__ void
onePartKernel(const T *source, T *result, int32_t width,
              int32_t height, int32_t part_size) {
    T *offset_cores = (T *)sram_buffer;
    T *nram_src = (T *)nram_buffer;
    T *offset_area = nram_src + ((MAX_NRAM_SIZE/sizeof(T)) >> 1);
    int32_t data_size = width * height;
    __memcpy(nram_src, source + part_size * taskId,
             data_size * sizeof(T), GDRAM2NRAM);
    __mluop_exp(nram_src, nram_src, nullptr, 0, data_size);
    for (int i = 1; i < height; i++) {
        __bang_add(nram_src + width * i,
                   nram_src + width * i,
                   nram_src + width * (i - 1),
                   width);
    }
    // offset between cores
    __memcpy(offset_cores + width * taskId,
             nram_src + (height - 1) * width,
             width * sizeof(T),
             NRAM2SRAM);
    __sync_cluster();
    __memcpy(offset_area,
             offset_cores,
             width * sizeof(T) * 5,
             SRAM2NRAM);
    __bang_write_zero(offset_area + width * 5, width);
    // [0],[1],[2],[3] are 4 offsets for 4 cores;
    // the [4] records and saves the cumulative offset for next round;
    // the [5] is to support the offset calculate;
    for (int i = 5; i > 0; i--) {
        __bang_move(offset_area + width * i,
                    offset_area + width * (i - 1),
                    width * sizeof(T));
    }
    __bang_move(offset_area,
                offset_area + width * 5,
                width * sizeof(T));
    for (int i = 1; i < 5; i++) {
        __bang_add(offset_area + width * i,
                   offset_area + width * i,
                   offset_area + width * (i - 1),
                   width);
    }
    __bang_cycle_add(nram_src,
                     nram_src,
                     offset_area + width * taskId,
                     data_size,
                     width);
    if (taskId == 0) {
        __memcpy(offset_cores + width * 4,
                 offset_area + width * 4,
                 width * sizeof(T),
                 NRAM2SRAM);
    }

    __mluop_log(nram_src, nram_src, nullptr, 0, data_size);
    __memcpy(result + part_size * taskId, nram_src,
             data_size * sizeof(T), NRAM2GDRAM);
}

// lowest dimension executing kernel=========================================
template <typename T>
__mlu_func__ void
lowestDimKernel_unino1(const T *input,
                        T *result,
                        int32_t axis_size,
                        int32_t higher_size) {
    int32_t data_size = axis_size * higher_size;
    int32_t nram_size = CoreCapacity / sizeof(T);
    int32_t part_width = higher_size;
    int32_t part_height = nram_size / part_width;
    int32_t part_size = part_height * part_width;
    int32_t cluster_size = part_size << 2;

    // data has a too large width to be deal as parts
    if (part_height < 6) {
        int32_t cluster_capacity = nram_size >> 1;
        int32_t batches_num
          = (higher_size + cluster_capacity - 1) / cluster_capacity;
        for (int i = 0; i < batches_num; i++) {
          int32_t deal_length;
          if (i < batches_num - 1)
            deal_length = cluster_capacity;
          else
            deal_length = higher_size - (batches_num - 1) * cluster_capacity;
          rowsKernel(result + i * cluster_capacity,
                     input + i * cluster_capacity,
                     deal_length, axis_size, higher_size);
        }
    } else {  // deal as parts
        T *offset_cores = (T *)sram_buffer;
        for (int i = 0; i < part_width; i++)
          offset_cores[part_width * 4 + i] = 0;
        int32_t parts_num = (axis_size + part_height - 1) / part_height;
        int32_t rounds_num = (parts_num + 3) / 4;
        int32_t round = 0;
        __sync_cluster();
        while (round < rounds_num - 1) {
            onePartKernel(input + cluster_size * round,
                          result + cluster_size * round,
                          part_width, part_height, part_size);
            round++;
        }
        int32_t last_round_parts = parts_num - (rounds_num - 1) * 4;
        int32_t lastRoundCores = last_round_parts;
        int32_t last_part_size
          = data_size - part_size * ((rounds_num - 1)
          * 4 + last_round_parts - 1);
        int32_t last_part_height = last_part_size / part_width;

        if (taskId < lastRoundCores - 1) {
          onePartKernel(input + cluster_size * round,
                        result + cluster_size * round,
                        part_width, part_height, part_size);
        } else if (taskId == lastRoundCores - 1) {
          onePartKernel(input + cluster_size * round,
                        result + cluster_size * round,
                        part_width, last_part_height, part_size);
        } else {
          __sync_cluster();
        }
    }
}

template <typename T>
__mlu_func__ void
batchKernel(const T *source,
            T *result,
            int32_t batches_num,
            int32_t width,
            int32_t height) {
    T *nram_src = (T *)nram_buffer;
    int32_t batch_size = width * height;
    int32_t data_size = batch_size * batches_num;
    __memcpy(nram_src, source, data_size * sizeof(T), GDRAM2NRAM);
    __mluop_exp(nram_src, nram_src, nullptr, 0, data_size);

    for (int i = 0; i < batches_num; i++) {
        for (int j = 1; j < height; j++) {
            __bang_add(nram_src + i * batch_size + j * width,
                       nram_src + i * batch_size + j * width,
                       nram_src + i * batch_size + (j - 1) * width,
                       width);
        }
    }
    __mluop_log(nram_src, nram_src, nullptr, 0, data_size);
    __memcpy(result, nram_src, data_size * sizeof(T), NRAM2GDRAM);
}

// mid dimension executing kernel============================================
template <typename T>
__mlu_func__ void
midDimKernel_unino8(const T *input,
                    T *result,
                    int32_t axis_size,
                    int32_t higher_size,
                    int32_t lower_size) {
  int32_t nram_size = CoreCapacity / sizeof(T);
  int32_t batches_num = lower_size;
  int32_t batch_size = axis_size * higher_size;
  int32_t batches_per_core = nram_size / batch_size;
  int32_t core_size = batch_size * batches_per_core;
  int32_t rounds_num
    = (batches_num + (batches_per_core * taskDim) - 1)
    / (batches_per_core * taskDim);
  if (batches_per_core > 0) {
    for (int i = 0; i < rounds_num - 1; i++) {
        batchKernel(input + i * core_size * taskDim + core_size * taskId,
                    result + i * core_size * taskDim + core_size * taskId,
                    batches_per_core, higher_size, axis_size);
    }
    int32_t last_round_batches
      = batches_num - batches_per_core * taskDim * (rounds_num - 1);
    int32_t lastRoundCores
      = (last_round_batches + batches_per_core - 1) / batches_per_core;
    int32_t last_core_batches
      = last_round_batches - batches_per_core * (lastRoundCores - 1);
    if (taskId < lastRoundCores - 1) {
      batchKernel(
        input + (rounds_num - 1) * core_size * taskDim + core_size * taskId,
        result + (rounds_num - 1) * core_size * taskDim + core_size * taskId,
        batches_per_core, higher_size, axis_size);
    }
    if (taskId == lastRoundCores - 1) {
      batchKernel(
        input + (rounds_num - 1) * core_size * taskDim + core_size * taskId,
        result + (rounds_num - 1) * core_size * taskDim + core_size * taskId,
        last_core_batches, higher_size, axis_size);
    }
  } else {
    if (clusterId == 0) {
      for (int i = 0; i < batches_num; i++) {
        lowestDimKernel_unino1(input + i * batch_size,
                                result + i * batch_size,
                                axis_size, higher_size);
        __sync_cluster();
      }
    }
  }
}

template <typename T>
__mlu_global__ void
logcumsumexpLaunch(const T *input,
                   T *result,
                   int32_t axis_size,
                   int32_t higher_size,
                   int32_t lower_size) {
  if (higher_size == 1 && lower_size == 1)
    dimOneKernel_unino8(input, result, axis_size);
  else if (higher_size == 1)
    highestDimKernel_unino8(input, result, axis_size, lower_size);
  else if (lower_size == 1)
    lowestDimKernel_unino1(input, result, axis_size, higher_size);
  else
    midDimKernel_unino8(input, result, axis_size, higher_size, lower_size);
}

mluOpStatus_t MLUOP_WIN_API
KernelLogcumsumexp(const cnrtDim3_t k_dim,
                   const cnrtFunctionType_t k_type,
                   const cnrtQueue_t queue,
                   mluOpDataType_t data_type,
                   const void *input,
                   void *result,
                   const int32_t axis_size,
                   const int32_t higher_size,
                   const int32_t lower_size) {
  if (data_type == mluOpDataType_t::MLUOP_DTYPE_FLOAT) {
    KERNEL_CHECK(logcumsumexpLaunch<<<k_dim, k_type, queue >>>(
    (float*)input, (float*)result, axis_size, higher_size, lower_size));
  } else {
    // half
    KERNEL_CHECK(logcumsumexpLaunch<<<k_dim, k_type, queue >>>(
    (half*)input, (half*)result, axis_size, higher_size, lower_size));
  }
  return MLUOP_STATUS_SUCCESS;
}
#endif

mluOpStatus_t MLUOP_WIN_API
LogcumsumexpDimOne(const cnrtDim3_t k_dim,
                   const cnrtFunctionType_t k_type,
                   const cnrtQueue_t queue,
                   mluOpDataType_t data_type,
                   const void *input,
                   void *result,
                   const int32_t axis_size) {
  if (data_type == mluOpDataType_t::MLUOP_DTYPE_FLOAT) {
    // float32
    KERNEL_CHECK();
  } else {
    // half
    KERNEL_CHECK();
  }
  return MLUOP_STATUS_SUCCESS;
}

mluOpStatus_t MLUOP_WIN_API
LogcumsumexpHighestDim(const cnrtDim3_t k_dim,
                   const cnrtFunctionType_t k_type,
                   const cnrtQueue_t queue,
                   mluOpDataType_t data_type,
                   const void *input,
                   void *result,
                   const int32_t axis_size,
                   const int32_t lower_size) {
  if (data_type == mluOpDataType_t::MLUOP_DTYPE_FLOAT) {
    // float32
    KERNEL_CHECK();
  } else {
    // half
    KERNEL_CHECK();
  }
  return MLUOP_STATUS_SUCCESS;
}

mluOpStatus_t MLUOP_WIN_API
LogcumsumexpLowestDim(const cnrtDim3_t k_dim,
                   const cnrtFunctionType_t k_type,
                   const cnrtQueue_t queue,
                   mluOpDataType_t data_type,
                   const void *input,
                   void *result,
                   const int32_t axis_size,
                   const int32_t higher_size) {
  if (data_type == mluOpDataType_t::MLUOP_DTYPE_FLOAT) {
    // float32
    KERNEL_CHECK();
  } else {
    // half
    KERNEL_CHECK();
  }
  return MLUOP_STATUS_SUCCESS;
}

mluOpStatus_t MLUOP_WIN_API
LogcumsumexpMidDim(const cnrtDim3_t k_dim,
                   const cnrtFunctionType_t k_type,
                   const cnrtQueue_t queue,
                   mluOpDataType_t data_type,
                   const void *input,
                   void *result,
                   const int32_t axis_size,
                   const int32_t higher_size,
                   const int32_t lower_size) {
  if (data_type == mluOpDataType_t::MLUOP_DTYPE_FLOAT) {
    // float32
    KERNEL_CHECK();
  } else {
    // half
    KERNEL_CHECK();
  }
  return MLUOP_STATUS_SUCCESS;
}
