/*************************************************************************
 * Copyright (C) [2022] by Cambricon, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#include "abs.h"

#include "core/logging.h"
#include "kernels/debug.h"
#include "kernels/kernel.h"

#include "kernels/unary_op/unary_op_3pipeline.h"
#include "kernels/tensor_stride_process/tensor_stride_process_common.h"
#include "kernels/unary_op/unary_op_stride_3pipeline.h"
#include "kernels/unary_op/complex_unary_op_3pipeline.h"
#include "kernels/unary_op/complex_unary_op_stride_3pipeline.h"

__nram__ int8_t nram_buffer[UNARY_NRAM_SIZE];

template <typename T1, typename T2>
__mlu_func__ void auxFunc3AbsFloat(size_t &output_input_gap,
                                   size_t &ping_pong_gap,
                                   size_t &auxiliary_a_gap,
                                   size_t &auxiliary_b_gap,
                                   size_t &span_num_deal, size_t &align_num) {
  align_num = NFU_ALIGN_SIZE / sizeof(T1);
  // need 2 pingpong sapce,
  span_num_deal = PAD_DOWN(UNARY_NRAM_SIZE / sizeof(T1) / 2, align_num);
  ping_pong_gap = span_num_deal * sizeof(T1);
  output_input_gap = 0;
  auxiliary_a_gap = 0;
  auxiliary_b_gap = 0;
}

template <typename T1, typename T2>
__mlu_func__ void auxFunc3AbsHalfBfloat16(
    size_t &output_input_gap, size_t &ping_pong_gap, size_t &auxiliary_a_gap,
    size_t &auxiliary_b_gap, size_t &span_num_deal, size_t &align_num) {
  align_num = NFU_ALIGN_SIZE / sizeof(T1);
  // need 2 pingpong sapce,
  span_num_deal = PAD_DOWN(UNARY_NRAM_SIZE / sizeof(T1) / 2, align_num);
  ping_pong_gap = span_num_deal * sizeof(T1) * 2;
  output_input_gap = 0;
  auxiliary_a_gap = 0;
  auxiliary_b_gap = 0;
}

template <typename T1, typename T2>
__mlu_func__ void auxComplexFunc3AbsComplexFloat(
    size_t &output_input_gap, size_t &ping_pong_gap, size_t &auxiliary_a_gap,
    size_t &auxiliary_b_gap, size_t &span_num_deal, size_t &align_num) {
  align_num = NFU_ALIGN_SIZE / sizeof(T2);
  span_num_deal = PAD_DOWN(UNARY_NRAM_SIZE / sizeof(T2) / 14, align_num);
  output_input_gap = 2 * span_num_deal * sizeof(T2);
  ping_pong_gap = 6 * span_num_deal * sizeof(T2);
  auxiliary_a_gap = 12 * span_num_deal * sizeof(T2);
  auxiliary_b_gap = 13 * span_num_deal * sizeof(T2);
}

template <typename T1, typename T2>
__mlu_func__ void computeAbsHalfBfloat16(int8_t *nram_output,
                                         int8_t *nram_input,
                                         int8_t *auxiliary_a,
                                         int8_t *auxiliary_b, size_t deal_num,
                                         size_t actual_num) {
  __bang_abs((T2 *)nram_output, (T1 *)nram_input, deal_num);
}

template <typename T1, typename T2>
__mlu_func__ void computeAbsFloat(int8_t *nram_output, int8_t *nram_input,
                                  int8_t *auxiliary_a, int8_t *auxiliary_b,
                                  size_t deal_num, size_t actual_num) {
  __bang_abs((T2 *)nram_output, (T1 *)nram_input, deal_num);
}

template <typename T1, typename T2>
__mlu_func__ void computeAbsComplexFloat(int8_t *nram_output,
                                         int8_t *nram_input,
                                         int8_t *auxiliary_a,
                                         int8_t *auxiliary_b, size_t deal_num,
                                         size_t actual_num) {
  T2 *aux_a = (T2 *)nram_input + 2 * deal_num;
  T2 *aux_b = (T2 *)nram_input + 3 * deal_num;
  __bang_write_value((T2 *)auxiliary_a, deal_num * sizeof(T2), (int8_t)0xAA);
  __bang_write_value((T2 *)auxiliary_b, deal_num * sizeof(T2), (int8_t)0x55);
  // select [real,real,real...] and [imaginary,imaginary,imaginary...]
  __bang_filter_bitindex((T2 *)aux_a, (T2 *)nram_input, auxiliary_a,
                         deal_num * 2);
  __bang_filter_bitindex((T2 *)aux_b, (T2 *)nram_input, auxiliary_b,
                         deal_num * 2);
  if (std::is_same<T2, float>::value) {
    // currently this op only support fp32
    __cn_vector_hypot_f32(deal_num, (T2 *)nram_output, (T2 *)aux_a,
                          (T2 *)aux_b);
  }
}

// function implementation
UNARY_OP_KERNEL_3PIPELINE_WITH_STRIDE_IMPLE(Abs, Float);
UNARY_OP_KERNEL_3PIPELINE_WITH_STRIDE_IMPLE(Abs, HalfBfloat16);
COMPLEX_UNARY_OP_KERNEL_3PIPELINE_WITH_STRIDE_IMPLE(Abs, ComplexFloat);

UNARY_OP_KERNEL_3PIPELINE_IMPLE(Abs, Float);
UNARY_OP_KERNEL_3PIPELINE_IMPLE(Abs, HalfBfloat16);
COMPLEX_UNARY_OP_KERNEL_3PIPELINE_IMPLE(Abs, ComplexFloat);

mluOpStatus_t MLUOP_WIN_API
Kernel3StagePipelineAbs(const cnrtDim3_t k_dim, const cnrtFunctionType_t k_type,
                        const cnrtQueue_t queue, const mluOpDataType_t d_type,
                        const void *x, void *y, size_t element_num) {
  if (d_type == MLUOP_DTYPE_FLOAT || d_type == MLUOP_DTYPE_INT32) {
    KERNEL_CHECK(
        MLUBlockKernel3StagePipelineAbsFloat<float, float>
        <<<k_dim, k_type, queue>>>((int8_t *)x, (int8_t *)y, element_num));
  } else if (d_type == MLUOP_DTYPE_HALF || d_type == MLUOP_DTYPE_BFLOAT16) {
    KERNEL_CHECK(
        MLUBlockKernel3StagePipelineAbsHalfBfloat16<half, half>
        <<<k_dim, k_type, queue>>>((int8_t *)x, (int8_t *)y, element_num));
  } else {
    KERNEL_CHECK(
        MLUBlockKernel3StagePipelineComplexAbsComplexFloat<double, float>
        <<<k_dim, k_type, queue>>>((int8_t *)x, (int8_t *)y, element_num))
  }
  return MLUOP_STATUS_SUCCESS;
}

mluOpStatus_t MLUOP_WIN_API Kernel3StagePipelineWithStrideAbs(
    const cnrtDim3_t k_dim, const cnrtFunctionType_t k_type,
    const cnrtQueue_t queue, const mluOpDataType_t d_type, const void *x,
    mluop::TensorShape x_shape, void *y, mluop::TensorShape y_shape,
    size_t element_num) {
  if (d_type == MLUOP_DTYPE_FLOAT || d_type == MLUOP_DTYPE_INT32) {
    KERNEL_CHECK(MLUBlockKernel3StagePipelineWithStrideAbsFloat<float, float>
                 <<<k_dim, k_type, queue>>>((int8_t *)x, x_shape, (int8_t *)y,
                                            y_shape, element_num));
  } else if (d_type == MLUOP_DTYPE_HALF || d_type == MLUOP_DTYPE_BFLOAT16) {
    KERNEL_CHECK(
        MLUBlockKernel3StagePipelineWithStrideAbsHalfBfloat16<half, half>
        <<<k_dim, k_type, queue>>>((int8_t *)x, x_shape, (int8_t *)y, y_shape,
                                   element_num));
  } else {
    KERNEL_CHECK(MLUBlockKernel3StagePipelineWithStrideComplexAbsComplexFloat<
                 double, float><<<k_dim, k_type, queue>>>(
        (int8_t *)x, x_shape, (int8_t *)y, y_shape, element_num))
  }
  return MLUOP_STATUS_SUCCESS;
}
