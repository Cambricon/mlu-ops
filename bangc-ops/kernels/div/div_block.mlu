/*************************************************************************
 * Copyright (C) 2021 by Cambricon, Inc. All rights reserved.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#include "kernels/binary_op/binary_op_3pipeline.h"
#include "kernels/kernel.h"
#include "mlu_op_kernel.h"

#define HIGH_BOUND 1e5
#define LOW_BOUND 1e-10
#define SCALE 1e-5
#define LOW_SCALE 1e10

#define DIV_NRAM_USED MAX_NRAM_SIZE
__nram__ char nram_buffer[DIV_NRAM_USED];

template <typename T>
__mlu_func__ void get3OffsetDivFast(int32_t &nram_limit, int32_t &pong_x,
                                    int32_t &pong_y, T *&nram_x, T *&nram_y,
                                    T *&nram_aux1, T *&nram_aux2, T *&nram_aux3,
                                    char *nram_buffer) {
  if (sizeof(T) == sizeof(half)) {
    // nram: x - x_pong - y - y_pong - nram_scaling - nram_zero
    nram_limit =
        ((DIV_NRAM_USED - 3 * BINARY_ALIGN_NUM * sizeof(float)) / sizeof(T)) /
        16;
    nram_limit = FLOOR_ALIGN(nram_limit, BINARY_ALIGN_NUM);
    pong_x = 2 * nram_limit;
    pong_y = 2 * nram_limit;
    nram_x =
        (T *)nram_buffer + nram_limit;  // nram_x_pong = nram_x + nram_limit * 2
    nram_y = nram_x + nram_limit * 4;
    nram_aux1 = nram_y + nram_limit * 3;     // scaling
    nram_aux2 = nram_aux1 + nram_limit * 4;  // nram_aux2
    nram_aux3 = nram_aux2 + nram_limit * 4;  // zero
    __bang_write_value((float *)nram_aux3 + BINARY_ALIGN_NUM, BINARY_ALIGN_NUM,
                       (float)HIGH_BOUND);
    __bang_write_value((float *)nram_aux3 + 2 * BINARY_ALIGN_NUM,
                       BINARY_ALIGN_NUM, (float)LOW_BOUND);
  } else {
    // nram: x - x_pong - y - y_pong - scaling - zoom - zero - factor
    nram_limit = (DIV_NRAM_USED / sizeof(T) - 3 * BINARY_ALIGN_NUM) / 8;
    nram_limit = FLOOR_ALIGN(nram_limit, BINARY_ALIGN_NUM);
    pong_x = nram_limit;
    pong_y = nram_limit;
    nram_x = (T *)nram_buffer;  // nram_x_pong = nram_x + nram_limit
    nram_y = nram_x + nram_limit * 2;
    nram_aux1 = nram_y + nram_limit * 2;
    nram_aux2 = nram_aux1 + nram_limit * 2;
    nram_aux3 = nram_aux2 + nram_limit * 2;
    __bang_write_value(nram_aux3 + BINARY_ALIGN_NUM, BINARY_ALIGN_NUM,
                       (float)HIGH_BOUND);
    __bang_write_value(nram_aux3 + 2 * BINARY_ALIGN_NUM, BINARY_ALIGN_NUM,
                       (float)LOW_BOUND);
  }
}

/* HighAcc mode will only be used when data type is half*/
template <typename T>
__mlu_func__ void get3OffsetDivHighAcc(int32_t &nram_limit, int32_t &pong_x,
                                       int32_t &pong_y, T *&nram_x, T *&nram_y,
                                       T *&nram_aux1, T *&nram_aux2,
                                       T *&nram_aux3, char *nram_buffer) {
  // nram: x - x_pong - y - y_pong - nram_scaling - nram_zero
  nram_limit =
      ((DIV_NRAM_USED - 3 * BINARY_ALIGN_NUM * sizeof(float)) / sizeof(T)) / 16;
  nram_limit = FLOOR_ALIGN(nram_limit, BINARY_ALIGN_NUM);
  pong_x = 2 * nram_limit;
  pong_y = 2 * nram_limit;
  nram_x =
      (T *)nram_buffer + nram_limit;  // nram_x_pong = nram_x + nram_limit * 2
  nram_y = nram_x + nram_limit * 4;
  nram_aux1 = nram_y + nram_limit * 3;     // scaling
  nram_aux2 = nram_aux1 + nram_limit * 4;  // nram_aux2
  nram_aux3 = nram_aux2 + nram_limit * 4;  // zero
  __bang_write_value((float *)nram_aux3 + BINARY_ALIGN_NUM, BINARY_ALIGN_NUM,
                     (float)HIGH_BOUND);
  __bang_write_value((float *)nram_aux3 + 2 * BINARY_ALIGN_NUM,
                     BINARY_ALIGN_NUM, (float)LOW_BOUND);
}

template <typename T>
__mlu_func__ void computeDivFast(T *nram_x, T *nram_y, T *nram_scaling,
                                 T *nram_aux2, T *nram_zero, int32_t actual_num,
                                 int32_t deal_num) {
  T *nram_zoom = nram_scaling + deal_num;
  T *nram_aux4 = nram_aux2 + deal_num;
  T *nram_factor = nram_zero + BINARY_ALIGN_NUM;
  T *nram_bound = nram_factor + BINARY_ALIGN_NUM;
  __bang_write_zero(nram_zero, BINARY_ALIGN_NUM);
  // ensure all the input are larger than 0
  __bang_cycle_gt(nram_scaling, nram_y, nram_zero, deal_num, BINARY_ALIGN_NUM);
  __bang_mul_scalar(nram_scaling, nram_scaling, (T)(2), deal_num);
  __bang_add_scalar(nram_scaling, nram_scaling, (T)(-1), deal_num);
  __bang_mul(nram_y, nram_y, nram_scaling, deal_num);
  if (sizeof(T) == sizeof(float)) {
    // ZOOM
    __bang_cycle_lt(nram_zoom, nram_y, nram_factor, deal_num, BINARY_ALIGN_NUM);
    __bang_mul_scalar((float *)nram_zoom, (float *)nram_zoom,
                      (float)(1 - SCALE), deal_num);
    __bang_add_scalar((float *)nram_zoom, (float *)nram_zoom, (float)SCALE,
                      deal_num);
    __bang_mul(nram_y, nram_y, nram_zoom, deal_num);

    __bang_cycle_lt(nram_aux2, nram_y, nram_bound, deal_num, BINARY_ALIGN_NUM);
    __bang_mul_scalar((float *)nram_aux4, (float *)nram_aux2, (float)LOW_SCALE,
                      deal_num);
    __bang_cycle_eq(nram_aux2, nram_aux2, nram_zero, deal_num,
                    BINARY_ALIGN_NUM);
    __bang_add(nram_aux2, nram_aux2, nram_aux4, deal_num);
    __bang_mul(nram_y, nram_y, nram_aux2, deal_num);
  }
  // execute active
  __bang_active_reciphp(nram_y, nram_y, deal_num);
  // recover all the sacled input data
  if (sizeof(T) == sizeof(float)) {
    __bang_mul(nram_y, nram_y, nram_zoom, deal_num);
    __bang_mul(nram_y, nram_y, nram_aux2, deal_num);
  }
  __bang_mul(nram_y, nram_y, nram_scaling, deal_num);
  // x * (1 / y)
  __bang_mul(nram_x, nram_y, nram_x, deal_num);
}

/* 200 active half with COMPUTATION_HIGH_PRECISION
 */
template <typename T>
__mlu_func__ void computeDivHighAcc(T *nram_x, T *nram_y, T *nram_scaling,
                                    T *nram_aux2, T *nram_zero,
                                    int32_t actual_num, int32_t deal_num) {
  float *nram_fp_x = (float *)(nram_x - deal_num);
  float *nram_fp_y = (float *)(nram_y - deal_num);
  // bit-up
  __bang_half2float(nram_fp_x, nram_x, deal_num);
  __bang_half2float(nram_fp_y, nram_y, deal_num);
  float *scale = (float *)nram_scaling;
  float *zoom = scale + deal_num;
  float *aux2 = (float *)nram_aux2;
  float *aux4 = aux2 + deal_num;
  float *zero = (float *)nram_zero;
  float *factor = zero + BINARY_ALIGN_NUM;
  float *bound = factor + BINARY_ALIGN_NUM;
  __bang_write_zero(zero, BINARY_ALIGN_NUM);
  // ensure all the input are larger than 0
  __bang_cycle_gt(scale, nram_fp_y, zero, deal_num, BINARY_ALIGN_NUM);
  __bang_mul_scalar(scale, scale, (float)(2), deal_num);
  __bang_add_scalar(scale, scale, (float)(-1), deal_num);
  __bang_mul(nram_fp_y, nram_fp_y, scale, deal_num);

  __bang_cycle_lt(zoom, nram_fp_y, factor, deal_num, BINARY_ALIGN_NUM);
  __bang_mul_scalar(zoom, zoom, (float)(1 - SCALE), deal_num);
  __bang_add_scalar(zoom, zoom, (float)SCALE, deal_num);
  __bang_mul(nram_fp_y, nram_fp_y, zoom, deal_num);

  __bang_cycle_lt(aux2, nram_fp_y, bound, deal_num, BINARY_ALIGN_NUM);
  __bang_mul_scalar(aux4, aux2, (float)LOW_SCALE, deal_num);
  __bang_cycle_eq(aux2, aux2, zero, deal_num, BINARY_ALIGN_NUM);
  __bang_add(aux2, aux2, aux4, deal_num);
  __bang_mul(nram_fp_y, nram_fp_y, aux2, deal_num);

  // execute active
  __bang_active_reciphp(nram_fp_y, nram_fp_y, deal_num);
  // recover all the sacled input data
  __bang_mul(nram_fp_y, nram_fp_y, zoom, deal_num);
  __bang_mul(nram_fp_y, nram_fp_y, aux2, deal_num);
  __bang_mul(nram_fp_y, nram_fp_y, scale, deal_num);
  // x * (1 / y)
  __bang_mul(nram_fp_y, nram_fp_y, nram_fp_x, deal_num);
  __bang_float2half_rd((half *)nram_x, nram_fp_y, deal_num);
}

BINARY_OP_3PIPELINE_IMPLE(Div, float, Fast);
BINARY_OP_3PIPELINE_IMPLE(Div, half, Fast);
BINARY_OP_3PIPELINE_IMPLE(Div, half, HighAcc);

void MLUOP_WIN_API mluOpBlockKernel3StagePipelineDivHalfFast(
    cnrtDim3_t k_dim, cnrtFunctionType_t k_type, cnrtQueue_t queue,
    const void *x, const void *y, void *z, int num) {
  MLUBlockKernel3StagePipelineDivhalfFast<<<k_dim, k_type, queue>>>(
      (void *)x, (void *)y, (void *)z, num);
}

void MLUOP_WIN_API mluOpBlockKernel3StagePipelineDivHalfHighAcc(
    cnrtDim3_t k_dim, cnrtFunctionType_t k_type, cnrtQueue_t queue,
    const void *x, const void *y, void *z, int num) {
  MLUBlockKernel3StagePipelineDivhalfHighAcc<<<k_dim, k_type, queue>>>(
      (void *)x, (void *)y, (void *)z, num);
}

void MLUOP_WIN_API mluOpBlockKernel3StagePipelineDivFloatFast(
    cnrtDim3_t k_dim, cnrtFunctionType_t k_type, cnrtQueue_t queue,
    const void *x, const void *y, void *z, int num) {
  MLUBlockKernel3StagePipelineDivfloatFast<<<k_dim, k_type, queue>>>(
      (void *)x, (void *)y, (void *)z, num);
}
